\placeholder{ We need to make sure we mention somewhere that (1) predicates are conceptually sampled after the line
is executed and (2) we transform our counters into real predicates before running this algorithm.}

This section presents the algorithm that we have developed for
isolating bugs in programs where multiple bugs are present
simultaneously.  As discussed in \Autoref{sec:background}, our
approach is to count the number of times we observe pre-specified
predicates at each program program point to be true during program
execution.  Because our system has no \textit{a priori} knowledge of
what bugs may be in the program, or even any model of what the program
does, our strategy is to make the set of predicates large in the
expectation that if the predicate set covers enough facets of the
program, every bug will be correlated with some predicate in the set.

In fact, our instrumentation strategies generate very large sets
of predicates; in a typical application tens of thousands of distinct
predicates are randomly sampled during program execution.  Because the
number of distinct bugs in a program is (hopefully!) orders of
magnitude smaller than the number of instrumented predicates, the
algorithmic problem is at least as much about discarding irrelevant
predicates as it is about identifying relevant predicates.  This
observation is reflected in our algorithm, which consists of two phases:
\begin{enumerate}
\item Eliminate predicates that are not predictive of program failure.

\item Rank the predicates that remain.  The higher a predicate is ranked,
the more confident we are that is involved in a bug.
\end{enumerate}

Consider the following C code fragment, which we use to motivate and illustrate
our technique:
\begin{quote}
\begin{verbatim}
f = ...;          (a)  
if (f == NULL) {  (b)
        x = 0;    (c)
        *f;       (d)
}
\end{verbatim}
\end{quote}
Consider the predicate {\tt f == NULL} at line {\tt (b)}.  Clearly
this predicate is highly correlated with failure; in fact, whenever it
is true this program inevitably crashes.  An important observation,
however, is that even a ``smoking gun'' such as {\tt f == NULL} at
line {\tt (b)} cannot be a perfect predictor of failure when there are
multiple bugs in the program---since there are other bugs, the program can fail
even if the predicate is false.  Put another way, we assume
that bugs are independent, and there is no reason to believe that
a predicate that is a good predictor for one bug is at all correlated
with any other bug.

The bug in the code fragment above is \termdef{deterministic} with
respect to {\tt f == NULL}: if {\tt f == NULL} is true at line {\tt
(b)}, the program is guaranteed to eventually fail.  In many cases it
is simply impossible to observe the exact conditions that cause
failure; for example, buffer overrun bugs in a C program may or may
not cause the program to crash depending on runtime system decisions
about how data is laid out in memory.  Such bugs are
\termdef{non-deterministic} with respect to every predicate that we instrument:
even for the best predictor $P$, it is possible that $P$ is true and
still the program terminates normally.  In the example above, if we replace line
{\tt (d)} by
\begin{quote}
\begin{verbatim}
if (random) f = ... some valid pointer ...;
*f;
\end{verbatim}
\end{quote}
then the bug becomes non-deterministic.

To summarize, even for predicates that truly are the causes of bugs, we can neither assume that 
when the predicate is true that
the program fails nor that when the predicate is false that
the program succeeds. But we can express the probability that a predicate
being true implies failure.  Let $\fail$ be an atomic predicate that is
true for failing runs and false for successful runs.  We want to compute:
\[ \crash(P) = \prob(P \Rightarrow \fail) \]
for every predicate $P$ over the set of all runs.  Let $\#S(P)$ be the number
of successful runs in which $P$ is observed to be true, and let $\#F(P)$ be the number of
failing runs in which $P$ is observed to be true.  Then we have
\[ \crash(P) = \frac{\#F(P)}{\#S(P) + \#F(P)} \]

Notice that $\crash(P)$ is not affected by the set of runs in which
$P$ is observed to be false.  Thus, if $P$ is the cause of a bug, the
causes of other independent bugs do not affect $\crash(P)$.  
Also note that runs in which $P$ is not observed at all (either because
the line of code on which $P$ is checked is not reached, or the line is reached
but $P$ is not sampled) have no effect on $\crash(P)$.
Finally, obvserve that the definition of $\crash(P)$
generalizes the idea of deterministic and non-deterministic bugs.  A
bug is deterministic for $P$ if $\crash(P) = 1.0$ or, equivalently,
$P$ is never observed to be true in a successful run ($\#S(P) =
0$) and $P$ is observed to be true in at least one failing run ($\#F(P) > 0$). 
If $\crash(P) < 1.0$ then the bug is non-deterministic, with
lower scores showing weaker correlation between the predicate and
program failure.

As we shall show, $\crash(P)$ is a useful measure, but it is not good
enough for step (1) of our algorithm. To see this, consider again the
code fragment given above (in its original form, not with the
modification to make the bug non-deterministic).  At line {\tt (b)} we
have $\crash(\mbox{\tt f == NULL}) = 1.0$, so this predicate is a good
candidate for the cause of the bug.  
But on line {\tt (c)} we have the surprising fact that $\crash(\mbox{\tt x == 0}) = 1.0$ as well.
To understand why, observe that the predicate $\crash(\mbox{\tt x == 0})$ is always
true when we reach the point immediately after line {\tt (c)} and, in addition, 
only failing runs reach this line.
Thus $\#S(\mbox{\tt x == 0}) = 0$, and, so long as there is at least one run that
reaches line {\tt (c)} at all, the crash value of {\tt x == 0} at line {\tt (c)} is 1.0.

As the predicate {\tt x == 0} at line {\tt (c)} of the example
shows, just because a predicate has a high $\crash(\ldots)$ score does not
mean it is the cause of a bug.  In the case of {\tt x == 0}, the
decision that eventually causes the crash is made earlier, and the
high $\crash(\ldots)$ score of {\tt x == 0} merely reflects the fact that this
predicate is checked on a path where the program is already doomed.

One way to address this difficulty is to score a predicate not by the chance
that it implies failure, but by how much difference it makes that the predicate
is observed to be true versus simply reaching the line where the predicate is checked.
That is, on line {\tt (c)}, the probability of crashing is already 1.0 regardless
of the value of the predicate {\tt x == 0}, and thus the fact that {\tt x == 0} is
true does not increase the probability of failure at all; this coincides with
the intuition that this predicate is irrelevant to the bug.

This leads us to the following definition:
\[ \context(P) = \prob((P \vee \neg P) \Rightarrow \fail) \]
Now, $P \vee \neg P$ is not the set of all runs, because we are not working in a two-valued logic.
In any given run, neither of $P$ or $\neg P$ may be observed (because the site where this predicate is
sampled is not reached), or one may be observed, or both may be observed (because the statement is executed
multiple times and $P$ is sometimes true and sometimes false).  Thus, $\context(P)$ is the probability that
in the set of runs where the value of $P$ is observed at all, the program fails. We can compute $\context(P)$ as follows:
\[ \context(P) = \frac{\#F(P \vee \neg P)}{\#S(P \vee \neg P) + \#F(P \vee \neg P)} \]

The interesting quantity, then, is 
\[ \increase(P) = \crash(P) - \context(P) \]
which can be read as: How much does $P$ being true increase the probability of failure
over simply reaching the line where $P$ is sampled?  For example, for the predicate {\tt x == 0} on line {\tt (c)},
we have 
\[\crash(\mbox{\tt x == 0}) = \context(\mbox{\tt x == 0}) = 1.0 \]
and so $\increase(\mbox{\tt x == 0}) = 0$.
We can now state our algorithm:
\begin{enumerate}
\item Discard any predicate $P$ where $\increase(P) \leq 0$.

\item Sort the remaining predicates lexicographically first by $\crash(P)$ and then by $\context(P)$.
\end{enumerate}

A few comments on this algorithm are in order.  First, pruning
predicates using $\increase(P) \leq 0$ has many desirable
properties.  It is easy to prove that large classes of irrelevant
predicates always have scores $\leq 0$.  For example, any predicate
that is not reached, that is a program invariant, or that is just
control-dependent on a true cause is eliminated by this test.  It is
also worth pointing out that this algorithm tends to localize bugs at
a point where the condition that causes the bug becomes true, rather than at
the crash site.  For example, in the code fragment given above, the bug is
attributed to the success of the conditional branch test {\tt f ==
NULL} on line {\tt (b)} rather than the pointer dereference on line
{\tt (d)}.  Thus, the cause of the bug discovered by the algorithm
points directly to the conditions under which it occurs, rather than
the line on which it occurs (which is usually available anyway in the
stack trace). \Autoref{sec:experiments:results} gives several examples
of this phenomenon while hunting for real bugs.

The purpose of sorting the surviving predicates in step (2) by
$\crash(\ldots)$ is to ensure that the highest confidence predicates (those
most likely to actually cause crashes) are listed first in the final
result.  We need step (2) because while $\increase(\ldots)$ is very good at
eliminating unimportant predicates, it is rather poorer as a measure
of the most important predicates.  Consider line {\tt (a)} in the
example above.  If {\tt (a)} has, say, a 90\% probability of assigning
a {\tt NULL} pointer to {\tt f}, then the value of
$\increase(\mbox{\tt f == NULL})$ will be be high at line {\tt (a)}
and lower, but still positive, at line {\tt (b)}.  While both {\tt
(a)} and {\tt (b)} contribute to the bug, it is our experience that,
once irrelevant predicates are discarded, it is most natural to
associate a bug with the predicate that gives the highest absolute
chance of failure; these are most likely to be the true causes.  For
instance, in our example, something is certainly wrong by the time we
reach line {\tt (b)}, where taking the true branch leads to a certain
crash, while it is entirely possible that line {\tt (a)} is correct.
Most likely the bug is a typographical error: the programmer meant to
write {\tt f != NULL} as the predicate of the conditional instead of
{\tt f == NULL}.

Finally, it should be clear that the algorithm is efficient.
Step (1) requires only a single pass over the data, and step (2) is
simply sorting.


% LocalWords:  pre
