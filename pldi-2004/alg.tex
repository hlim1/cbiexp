This section presents our algorithm for automatically isolating
multiple bugs.  As discussed in \autoref{sec:background}, the input is
a set of feedback reports from individual program runs $R$, where
$R(P) = 1$ if predicate $P$ is observed to be true during the
execution of $R$.

The idea behind the algorithm is to simulate the iterative manner in which  human programmers
typically find and fix bugs:
\begin{enumerate}

\item Identify the cause of the most important bug ${\cal B}$.

\item Fix ${\cal B}$, and repeat.

\end{enumerate}

For our purposes, identifying the cause of a bug ${\cal B}$ means selecting a
predicate $P$ closely correlated with ${\cal B}$.  The difficulty is that we
know the set of runs that succeed and fail, but we do not know which
set of failing runs corresponds to ${\cal B}$, or even how many bugs there
are.  Thus, in the first step we must infer which predicates are most
likely to correspond to individual bugs and rank those predicates in
importance.

For the second step, while we cannot literally fix the bug
corresponding to the chosen predictor $P$, we can simulate what
happens if the bug does not occur.  We discard any run $R$ such that
$R(P) = 1$ and repeat.  Discarding all the runs where $R(P) = 1$
reduces the importance of other predictors of ${\cal B}$, allowing predicates
that predict different bugs (i.e., different sets of failing runs) to
rise to the top in subsequent iterations.

\subsection{Increase Scores}
\label{sec:increase}

We now discuss the first step, how to find the cause of the most important bug.
We break this step into two sub-steps.  First, we eliminate predicates that have no
predictive power at all; this typically reduces the number of predicates we need
to consider by two orders of magnitude (e.g., from hundreds of thousands to thousands).
Next, we rank the surviving predicates by importance (see \autoref{sec:ranking}).

Consider the following C code fragment:
\begin{quote}
\begin{verbatim}
f = ...;          (a)
if (f == NULL) {  (b)
    x = 0;        (c)
    *f;           (d)
}
\end{verbatim}
\end{quote}
Consider the predicate {\tt f == NULL} at line {\tt (b)}, which would
be captured by branches instrumentation.  Clearly
this predicate is highly correlated with failure; in fact, whenever it
is true this program inevitably crashes.\footnote{We also note that this bug could 
be detected by a simple static analysis; this example is meant to be concise rather than 
a significant application of our techniques.}   An important observation,
however, is that there is no one perfect predictor of failure in a
program with multiple bugs.  Even a ``smoking gun'' such as {\tt f ==
  NULL} at line {\tt (b)} has little or no predictive power for
failures due to unrelated bugs in the same program.

The bug in the code fragment above is \termdef{deterministic} with
respect to {\tt f == NULL}: if {\tt f == NULL} is true at line {\tt
(b)}, the program fails.  In many cases it is impossible to observe
the exact conditions causing failure; for example, buffer overrun bugs
in a C program may or may not cause the program to crash depending on
runtime system decisions about how data is laid out in memory.  Such
bugs are \termdef{non-deterministic} with respect to every predicate;
even for the best predictor $P$, it is possible
that $P$ is true and still the program terminates normally.  In the
example above, if we insert before line {\tt (d)} a valid pointer
assignment to {\tt f} controlled by a conditional that is true at
least occasionally (say via a call to read input)
\begin{quote}
\begin{verbatim}
if (...) f = ... some valid pointer ...;
*f;
\end{verbatim}
\end{quote}
the bug becomes non-deterministic with respect to {\tt f == NULL}.

To summarize, even for a predicate $P$ that is truly the cause of a bug, we can neither assume that
when $P$ is true that
the program fails nor that when $P$ is never observed to be true  that
the program succeeds.
%%
\issue[Mayur]{``To summarize, even for a predicate P ..., we can
  neither X nor Y.''

  I think we need to switch X and Y in the above sentence because Alex
  describes Y first and X next in the preceding 2 para's.}
%%
But we can express the probability that $P$
being true implies failure.  Let $\fail$ be an atomic predicate that is
true for failing runs and false for successful runs.  Let $\prob(A | B)$ denote
the conditional probability function of the event $A$ given event $B$ .  
We want to compute:
\[ \crash(P) \equiv \prob(\fail = 1 | P = 1) \]
for every instrumented predicate $P$ over the set of all runs.  Let $S(P)$ be the number
of successful runs in which $P$ is observed to be true, and let $F(P)$ be the number of
failing runs in which $P$ is observed to be true.  
We estimate $\crash(P)$ as:
\[ \crash(P) = \frac{F(P)}{S(P) + F(P)} \]

Notice that $\crash(P)$ is unaffected by the set of runs in which
$P$ is not observed to be true.  Thus, if $P$ is the cause of a bug, the
causes of other independent bugs do not affect $\crash(P)$.
Also note that runs in which $P$ is not observed at all (either because
the line of code on which $P$ is checked is not reached, or the line is reached
but $P$ is not sampled) have no effect on $\crash(P)$.
The definition of $\crash(P)$
generalizes the idea of deterministic and non-deterministic bugs.  A
bug is deterministic for $P$ if $\crash(P) = 1.0$, or equivalently,
$P$ is never observed to be true in a successful run ($S(P) =
0$) and $P$ is observed to be true in at least one failing run ($F(P) > 0$).
If $\crash(P) < 1.0$ then the bug is non-deterministic with respect to $P$.
Lower scores show weaker correlation between the predicate and
program failure.

Now $\crash(P)$ is a useful measure, but it is not good
enough for the first step of our algorithm. To see this, consider again the
code fragment given above in its original, deterministic form.  At line {\tt (b)} we
have $\crash(\mbox{\tt f == NULL}) = 1.0$, so this predicate is a good
candidate for the cause of the bug.
But on line {\tt (c)} we have the unpleasant fact that $\crash(\mbox{\tt x == 0}) = 1.0$ as well.
To understand why, observe that the 
predicate \texttt{x == 0} is always true at line {\tt (c)} and, in
addition,
only failing runs reach this line.
Thus $S(\mbox{\tt x == 0}) = 0$, and, so long as there is at least one run that
reaches line {\tt (c)} at all, $\crash(\mbox{\tt x == 0})$ at line {\tt (c)} is 1.0.

As this example
shows, just because $\crash(P)$ is high does not
mean $P$ is the cause of a bug.  In the case of {\tt x == 0}, the
decision that eventually causes the crash is made earlier, and the
high $\crash(\mbox{\tt x == 0})$ score merely reflects the fact that this
predicate is checked on a path where the program is already doomed.

\issue[Alice]{The Failure score is not a perfect definition of
  importance.  The context score is designed to address one specific
  problem (that of the innocent by-stander), but there are many
  others.  We should make this clear in the text.  [didn't modify
  text]}

A way to address this difficulty is to score a predicate not by the chance
that it implies failure, but by how much difference it makes that the predicate
is observed to be true versus simply reaching the line where the predicate is checked.
That is, on line {\tt (c)}, the probability of crashing is already 1.0 regardless
of the value of the predicate {\tt x == 0}, and thus the fact that {\tt x == 0} is
true does not increase the probability of failure at all.  This coincides with
our intuition that this predicate is irrelevant to the bug.

This leads us to the following definition:
\[
\context(P) \equiv \prob(\fail = 1 | P \lor \lnot P = 1)  
\]
Now, it is not the case that $P \lor \lnot P = 1$ in every run, because we are not working in a two-valued logic:
In a given run, it is possible that neither $P$ nor $\lnot P$ is observed because the site where this predicate occurs is not reached,
or is reached but not sampled.
%In any given run, there are three possibilities: (a) neither $P$ nor $\lnot P$ 
%is observed (because the site where this predicate occurs is not reached, or is reached but not sampled),
%(b) one of $P$ or $\lnot P$ is observed, or (c) both are observed (because the statement is executed
%multiple times and $P$ is sometimes true and sometimes false).
Thus, $\context(P)$ is the probability that
in the subset of runs where the site containing predicate $P$ is reached and sampled, the program fails.
We can estimate $\context(P)$ as follows:
\[ \context(P) = \frac{F(P \lor \lnot P)}{S(P \lor \lnot P) + F(P \lor \lnot P)} \]

The interesting quantity, then, is
\begin{equation*}
 \increase(P) \equiv \crash(P) - \context(P) \label{eqn:1}
\end{equation*}
%%
which can be read as: How much does $P$ being true increase the probability of failure
over simply reaching the line where $P$ is sampled?  For example, for the predicate {\tt x == 0} on line {\tt (c)},
we have
\[\crash(\mbox{\tt x == 0}) = \context(\mbox{\tt x == 0}) = 1.0 \]
and so $\increase(\mbox{\tt x == 0}) = 0$.

A predicate $P$ with $\increase(P) \leq 0$ has no predictive power.  Being true does not increase the
probability of failure, and we can safely discard all such predicates.
But because some $\increase(P)$ scores may be based on few observations of $P$, it is important
to attach confidence intervals to the scores.  Since $\increase()$ is a statistic, computing
a confidence interval for the underlying parameter is a well-understood problem. In our experiments we retain a predicate $P$
only if $\increase(P) > 0$ with 95\% confidence; this removes predicates from consideration that have high
increase scores but very low confidence because of few observations. 

Pruning predicates based on $\increase(P)$ has many desirable
properties.  It is easy to prove that large classes of irrelevant
predicates always have scores $\leq 0$.  For example, any predicate
that is unreachable, that is a program invariant, or that is obviously
control-dependent on a true cause is eliminated by this test.  It is
also worth pointing out that this test tends to localize bugs at
a point where the condition that causes the bug becomes true, rather than at
the crash site.  For example, in the code fragment given above, the bug is
attributed to the success of the conditional branch test {\tt f ==
NULL} on line {\tt (b)} rather than the pointer dereference on line
{\tt (d)}.  Thus, the cause of the bug discovered by the algorithm
points directly to the conditions under which the crash occurs, rather than
the line on which it occurs (which is usually available anyway in the
stack trace).

\subsection{Statistical Interpretation}
\label{sec:statisticalinterpretation}

We have explained the test $\increase(P) > 0$ using programming terminology,
but it also has a natural statistical interpretation as a simplified {\em likelihood ratio} hypothesis
test.  Consider the two classes of trial runs
of the program: failed runs $F$ and successful runs $S$.  For each
class, we can treat the predicate $P$ as a Bernoulli random variable
with heads probabilities $\pi_f(P)$ and $\pi_s(P)$, respectively, for the
two classes.  The heads
probability is the probability that the predicate is observed to be
true.  If a predicate causes a set of crashes, then $\pi_f$ should be
much bigger than $\pi_s$.  We can formulate two statistical hypotheses:
the null hypothesis $\H_0:
\pi_f \leq \pi_s$, versus the alternate hypothesis $\H_1: \pi_f > \pi_s$.  Since
$\pi_f$ and $\pi_s$ are not known, we must estimate them:
\begin{align*}
  \hat \pi_f(P) &= \frac{F(P)}{F(P \lor \lnot P)} &
  \hat \pi_s(P) &= \frac{S(P)}{S(P \lor \lnot P)}
\end{align*}

Although these proportion estimates of $\pi_f$ and $\pi_s$ approach the
actual heads probabilities as we increase the number of trial runs, they
still differ due to sampling.  With a certain probability, using these
estimates instead of the actual values results in the wrong
answer.  A \textit{likelihood ratio test} takes this uncertainty into
account, and makes use of the statistic $ Z = \frac{(\hat \pi_f - \hat
  \pi_s)}{V_{f,s}}$, where $V_{f,s}$ is a sample variance term (see
e.g., \cite{Lehmann:1986:hyptest}).  When
the data size is large, $Z$ can be approximated as a standard Gaussian
random variable.  Performed independently for each predicate $P$, the
test decides whether or not $\pi_f(P) \leq \pi_s(P)$ with a guaranteed
false-positive probability (i.e.,\ choosing $\H_1$ when $\H_0$ is true).
A necessary (but not sufficient) condition for choosing $\H_1$ is that
$\hat \pi_f(P) > \hat \pi_s(P)$.  However, this is
equivalent to the condition that $\increase(P) > 0$.  To see why,
let $a = F(P)$, $b = S(P)$, $c = F(P\lor\lnot P)$, and $d = S(P\lor\lnot P)$.
Then
\begin{gather*}
  \increase(P) > 0 \iff \crash(P) > \context(P) \\
  \iff \frac{a}{a+b} > \frac{c}{c+d}
  \iff a (c+d) > (a+b) c \\
  \iff ad > bc \iff \frac{a}{c} > \frac{b}{d}
  \iff \hat \pi_f(P) > \hat \pi_s(P)
\end{gather*}

\input{moss-view/stats-breakout}

\subsection{Balancing Specificity \& Sensitivity}
\label{sec:ranking}

We now turn to the question of ranking the predicates.
\autoref{tab:sorts} shows the top
predicates under different ranking schemes (explained below) for one of our
experiments.  We use
a simple {\em thermometer} to visualize the information for each
predicate.  The length of the thermometer shows the number of runs in
which the predicate was observed, plotted on a log scale (so small increases
in thermometer size indicate many more runs).  The thermometer has
a sequence of bands:
the black band on the left shows the context score;
the next lighter band shows the increase score, the next, even  lighter band (which is either not
visible or very small in all thermometers) shows the confidence
interval, and the white space at the right end of the thermometer
shows the successful runs in which the predicate was
observed to be true.  The tables show the thermometer as well as the
numbers for each of the quantities that make up the thermometer. 

The most important bug is the one that causes the greatest number
of failed runs.  This observation suggests:
\[ \importance(P) = F(P) \]
\autoref{tab:sorts-failure} shows the top predicates 
ranked by decreasing $F(P)$.\footnote{These predicates are ranked after predicates where $\increase(P) \leq 0$ are discarded.}
While the predicates in \autoref{tab:sorts-failure} are, as expected, involved
in many failing runs, these predicates are also highly 
non-deterministic, meaning they are also true in many successful runs
and are weakly correlated with bugs.  

Our experience with other
ranking strategies that emphasize the number of failed runs is also that
they select predicates involved in many failing, but also many
successful, runs.  The best of these predicates (the ones with high
$\increase()$ scores) are {\em super-bug predictors}:
predictors that include failures from more than one bug.  The
signature of super-bug predictors is that they account for a very
large number of failures (by combining the failures of multiple bugs)
but are also highly non-deterministic despite reasonably high $\increase()$
scores.

Another possibility is:
\[ \importance(P) = \increase(P) \]
\autoref{tab:sorts-increase} shows the top predicates ranked by decreasing
$\increase()$ score (note the thermometers in the different figures
are not drawn to the same scale).  These predicates do a much better
job of predicting failure; in fact, the program always fails when any
of these predicates is true. However, note that the number of failing
runs is very small.  These predicates are {\em sub-bug predictors}:
predictors for a subset of the failures caused by a bug.  Unlike
super-bug predictors, which are not useful in our experience, sub-bug
predictors that account for a significant fraction of the failures for
a bug often provide valuable clues, but still they represent special
cases and may not suggest other, more fundamental, causes of the bug.

Tables~\ref{tab:sorts-failure} and \ref{tab:sorts-increase} illustrate the difficulty of defining
``importance.''  We are looking for predicates with high \emph{sensitivity}
(meaning predicates that account for many failed runs) but also high \emph{specificity} (meaning predicates that do not mispredict failure in many successful
runs).  In information retrieval, the corresponding terms are recall and
precision.  A standard way to combine sensitivity and specificity is to compute
their harmonic mean; this measure prefers high scores in
both dimensions.  In our case, $\increase(P)$ measures specificity.  For
sensitivity, we have found it useful to consider a transformation $\phi$ 
of the raw counts, and to form the normalized ratio $\phi(F(P)) / \phi(\numfail)$, 
where $\numfail$ is the total number of failed runs.
In our work thus far $\phi$ has been a logarithmic
transformation, which moderates the impact of very large numbers of failures.
Thus our overall metric is the following:
%%
\[
\importance(P) =
\frac{2}{
  \frac{1}{\increase(P)}
  +
  \frac{1}{log(F(P)) / log(\numfail)}}
\]
%%
The results are given in \autoref{tab:sorts-harmonic}.  We observe that
all of the predicates on this list indeed have both high specificity and
sensitivity, accurately describing a large number of failures. 


\subsection{Predicate Elimination}
\label{sec:elimination}

The remaining problem with the results in \autoref{tab:sorts-harmonic}
is that there is substantial redundancy; it is easy to see that several of these
predicates are related, hiding other, distinct bugs that either have
fewer failed runs or more non-deterministic predictors further down the
list.  As discussed above, we use a simple recursive algorithm to eliminate
redundant predicates:
\begin{enumerate}

\item Rank predicates by $\importance()$.

\item Remove the top-ranked predicate $P$ and discard all runs $R$ (feedback reports) where  $R(P) = 1$.

\item Repeat (1) and (2) until the set of runs is empty or the set of predicates is empty.
\end{enumerate}

\issue[Mayur]{We should mention somewhere in the text that we do
  eliminate logically related predicates by a conservative static
  analysis, lest the reviewer finds so many seemingly logically
  related predicates in our table snapshots and complains that we
  haven't done a straightforward static analysis to prune the list of
  thousands of predicates.  We do in fact perform a straightforward
  static analysis, but it is conservative in the presence of sampling,
  namely, it does not perform an inter-site analysis but it does
  perform an intra-site analysis.  The latter, while unsound in
  principle, is sound in the limit, and it does reduce the number of
  predicates retained after the Increase <= 0 test by a factor of 3 or
  so.  I am not sure where exactly to put this stuff (maybe at the
  point where we say we discard predicates with Increase <= 0) but
  will look out for it during a future scan of the paper.}

We can now state an easy, but important, property of this algorithm.  
\begin{lemma}
\rm
Let $P_1,\ldots,P_n$ be a set of instrumented predicates and let ${\cal B}_1,\ldots,{\cal B}_m$ be a set of bugs.  Let
\[ {\cal Z} = \bigcup_{1 \leq i \leq n} \{ R | R(P_i) = 1 \} \]
Then if for all $1 \leq j \leq m$ we have ${\cal B}_j \cap {\cal Z} \neq \emptyset$, then 
the algorithm chooses at least one predicate from the list $P_1,\ldots,P_n$ that predicts
at least one failure due to ${\cal B}_j$.
\end{lemma}

Thus, the elimination algorithm chooses at least one predicate 
predictive of each bug represented by the input set of predicates.
The other property we might like, that the algorithm chooses exactly
one predicate to represent each bug, does not hold; we shall see in
\autoref{sec:experiments} that the algorithm sometimes selects a
strong sub-bug predictor as well as a more natural predictor.  Besides
always representing each bug, the algorithm works well for two other
reasons.  First, two predicates are redundant if they predict the same
(or nearly the same) set of failing runs.  Thus, simply removing the
set of runs in which a predicate is true automatically dramatically
reduces the importance of any related predicates in the correct
proportions. Second, because elimination is
iterative, it is only necessary that $\importance()$ selects a good
predictor at each step, and not necessarily the best one; any
predicate that predicts a different set of failing runs than all
higher-ranked predicates is selected eventually.


