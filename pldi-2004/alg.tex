
This section presents the algorithm that we have developed for
isolating bugs in programs in which multiple bugs are present
simultaneously.  As discussed in Section~\ref{sec-background}, our
approach is to count the number of times we observe pre-specified
predicates at each program program point to be true during program
execution.  Because our system has no {\em a priori} knowledge of what bugs
may be in the program, or even any model of what the program does, our
strategy is to make the set of predicates large in the expectation that
if the set covers enough facets of the program, every bug will be correlated
with some predicate in the set.

In fact, our instrumentation strategies generate very large sets
of predicates; in a typical application tens of thousands of distinct
predicates are randomly sampled during program execution.  Since the
number of distinct bugs in a program is (hopefully!) orders of
magnitude smaller than the number of instrumented predicates, the
algorithmic problem is at least as much about discarding irrelevant
predicates as it is about identifying relevant predicates.  This
observation is reflected in our algorithm, which consists of two phases:
\begin{enumerate}
\item Eliminate predicates that are not predictive of program failure.

\item Rank the predicates that remain.  The higher a predicate is ranked,
the more confident we are that is involved in a bug.
\end{enumerate}

Consider the following code fragment, which we use to motivate and illustrate
our technique:
\begin{verbatim}
if (f == NULL) {  (a)
        x = 0;    (b)
        *f;       (c)
}
\end{verbatim}
Consider the predicate {\tt f == NULL} at line {\tt (a)}.  Clearly
this predicate is highly correlated with failure; in fact, whenever it
is true this program will inevitably crash.  An important observaton,
however, is that even a ``smoking gun'' such as {\tt f == NULL} at
line {\tt (a)} cannot be a perfect predictor of failure when there are
multiple bugs in the program---since there are other bugs, even when
the predicate is false the program can still fail.  Put another way, we assume
that bugs are independent, and there is no reason to assume that
a predicate that is a good predictor for one bug is at all correlated
with any other bug.

The bug in the code fragment above is \termdef{deterministic} with respect
to {\tt f == NULL}: once {\tt f == NULL} becomes true, the program is guaranteed to eventually
fail.  In many cases it is simply impossible to observe the exact
conditions that cause failure; for example, buffer overrun bugs in a C
program may or may not cause the program to crash depending on runtime
system decisions about how data is layed out in memory.  Such bugs are
\termdef{non-deterministic} with respect to every predicate that we instrument:
even for the best predictor $P$, it is possible that $P$ is true and
the program terminates normally.  In the example above, if we replace line
{\tt (c)} by
\begin{verbatim}
if (random) f = ... some valid pointer ...;
*f;
\end{verbatim}
then the bug becomes non-deterministic.

To summarize, we can neither assume that when predicate is true that
the program fails failure nor that when the predicate is false that
the program succeeds. But we can express the probability that a predicate
being true implies failure.  Let $\fail$ be an atomic predicate that is
true for failing runs and false for successful runs.  We want to compute:
\[ \crash(P) = \prob(P \Rightarrow \fail) \]
for every predicate $P$ over the set of all runs.  Let $\#S(P)$ be the number
of successful runs in which $P$ is observed to be true, and let $\#F(P)$ be the number of
failing runs in which $P$ is observed to be true.  Then we have
\[ \crash(P) = \frac{\#F(P)}{\#S(P) + \#F(P)} \]

Notice that $\crash(P)$ is not affected by the set of runs in which
$P$ is observed to be false; thus, if $P$ is the cause of a bug, the
causes of other independent bugs do not affect $\crash(P)$.  Also
notice that the definition of $\crash(P)$ generalizes the idea of
deterministic and non-deterministic bugs.  A bug is deterministic for
$P$ if $\crash(P) = 1.0$ or, equivalently, $P$ is never observed to be
true in a successful run ($\#S(P) = 0$). If $\crash(P) < 1.0$ then the
bug is non-deterministic, with lower scores showing weaker correlation
between the predicate and program failure.

As we shall see, $\crash(P)$ is a useful measure, but it is not good
enough for step (1) of our algorithm. To see this, consider again the
code fragment given above (in its original form, not with the
modification to make the bug non-deterministic).  At line {\tt (a)} we
have $\crash(\mbox{\tt f == NULL}) = 1.0$, so this predicate is a good
candidate for the cause of the bug.  
But on line {\tt (b)} we have $\crash(\mbox{\tt x == 0}) = 1.0$ as well!
To understand why, observe that the predicat $\crash(\mbox{\tt x == 0})$ is always
true when we reach the point immediately after this line and, in addition, only failing runs reach this line.
Thus the number of successful runs in the denominator of $\crash()$ is zero,
and the ration is 1.0. Clearly this predicate has nothing to do with the bug.

As the predicate {\tt x == 0} after line {\tt (b)} of the example
shows, just because a predicate has a high \crash() score does not
mean it is the cause of a bug.  In the case of {\tt x == 0}, the
decision that eventually causes the crash was made earlier, and the
high \crash() score of {\tt x == 0} merely reflects the fact that this
predicate is checked on a path where the program is already doomed.

One way to address this difficulty is to score a predicate not by the chance
that it implies failure, but by how much difference it makes that the predicate
is observed to be true versus simply reaching the statement with the predicate.
That is, on line {\tt (b)}, the probability of crashing is already 1.0 regardless
of the value of the predicate {\tt x == 0}, and thus the fact that {\tt x == 0} is
true does not increase the probability of failure at all; this coincides with
the intuition that this predicate is irrelevant to the bug.

This leads us to the following definition:
\[ \context(P) = \prob((P \vee \neg P) \Rightarrow \fail) \]
Now, $P \vee \neg P$ is not the set of all runs, because we are not working in a two-valued logic.
In any given run, neither of $P$ or $\neg P$ may be observed (because the statement where this predicate is
sampled is not reached), or one may be observed, or both may be observed (because the statement is executed
multiple times and $P$ is sometimes true and sometimes false).  Thus, $\context(P)$ is the probability that
in the set of runs where $P$ is observed at all, the program fails. We can compute $\context(P)$ as follows:
\[ \context(P) = \frac{\#F(P \vee \neg P)}{\#S(P \vee \neg P) + \#F(P \vee \neg P)} \]

We can now state our algorithm:
\begin{enumerate}
\item Discard any predicate $P$ where $\crash(P) - \context(P) \leq 0$.

\item Sort the remaining predicates lexicographically first by $\crash(P)$ and then by $\context(P)$.
\end{enumerate}

A few comments on this algorithm are in order.  First, pruning
predicates out using $\crash(P) - \context(P) \leq 0$ has many
desirable properties.  It is possible to prove, for example, that
large classes of predicates always have scores $\leq 0$.  For example,
any predicate that is not reached, that is a program invariant, or
that is just control-dependent on a true cause will be eliminated by
this test.  It is also worth pointing out that this algorithm will
prefer to localize bugs at the earliest possible point, rather than
as close to the crash site as possible.  For example, in the code fragment
given above, the bug is attributed to the success of the conditional branch
test {\tt f == NULL} on line {\tt (a)} rather than the pointer derefence
on lince {\tt (c)}.  Section~\ref{sec-experiments} gives several examples of
this phenomenon while hunting for real bugs.

The purpose of sorting the surviving predicates in step (2) by
\crash() is to ensure that the highest confidence predicates (those
most likely to cause crashes) are listed first in the final result
list.  It should be clear that the algorithm is very efficient; step (1) is
linear and step (2) is just ${\cal O}(n\log n)$.

