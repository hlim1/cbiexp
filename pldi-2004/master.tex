%% -*- TeX-master: t -*-

%% \documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% standard texmf packages
%%

\usepackage{nicefrac}
\usepackage{fancyvrb}
\usepackage{xspace}

\usepackage[bookmarks, pdftitle={Bug Isolation in the Presence of
  Multiple Errors}, pdfauthor={Ben Liblit, Mayur Naik, Alice X.
  Zheng, Alex Aiken, and Michael I.  Jordan}, pdfsubject={D.2.4
  [Software Engineering]: Software/Program Verification -- statistical
  methods; D.2.5 [Software Engineering]: Testing and Debugging --
  debugging aids, distributed debugging, monitors, tracing; I.5.2
  [Pattern Recognition]: Design Methodology -- feature evaluation and
  selection}, pdfkeywords={bug isolation, random sampling, invariants,
  feature selection, statistical debugging},
pdfpagemode=UseOutlines]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% unique to this paper
%%

\usepackage{Autoref}

\usepackage{amsmath}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}
\newcommand{\prob}{\mbox{\textit{Prob}}}
\newcommand{\fail}{\mbox{\textit{Fail}}}
\newcommand{\crash}{\mbox{\textit{Crash}}}
\newcommand{\context}{\mbox{\textit{Context}}}
\newcommand{\increase}{\mbox{\textit{Increase}}}
\renewcommand{\H}{{\mathcal{H}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% remove the following before submission!
%%

\usepackage{color}
\newcommand{\placeholder}[1]{{\color[cmyk]{0,0.61,0.87,0}[#1]}}
\newcommand{\comment}[1]{}
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% front matter
%%

\title{Bug Isolation in the Presence of Multiple Errors
  %%
  \thanks{This research was supported in part by NASA Grant No.\ 
    NAG2-1210; NSF Grant Nos.\ EIA-9802069, CCR-0085949, ACI-9619020,
    and IIS-9988642; and DOE Prime Contract No.\ W-7405-ENG-48 through
    Memorandum Agreement No.\ B504962 with LLNL.  The information
    presented here does not necessarily reflect the position or the
    policy of the Government and no official endorsement should be
    inferred.}}

\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X.\ Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I.\ Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  document body
%%

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
\placeholder{Abstract needed here.}
\end{abstract}

\category{D.2.4}{Software Engineering}{Software/Program
  Verification}[statistical methods]
%%
\category{D.2.5}{Software Engineering}{Testing and
  Debugging}[debugging aids, distributed debugging, monitors, tracing]
%%
\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
  evaluation and selection]

\terms{Experimentation, Reliability}

\keywords{bug isolation, random sampling, invariants, feature
  selection, statistical debugging}


\section{Introduction}
\label{sec:introduction}

Programs are buggy.  We all know this, and yet we use them anyway.
The commercial reality is that most software ships with many known
bugs and untold numbers of bugs not yet discovered.  Software
engineers understand that it is neither practical nor feasible to
delay releasing code until the last bug has been fixed.

Statistical debugging is a body of tools and techniques that help to
improve software quality in an imperfect world.  Specially
instrumented programs monitor their own behavior and send feedback
reports to a central collection server.  Monitoring is both sparse and
random, which means that complete information is never available about
any single run.  However, monitoring is also lightweight and therefore
practical to deploy to user communities numbering in the thousands or
millions.  Statistical debugging does not seek to understand the
single cause of a single failure on one machine; rather, it identifies
broad statistical trends that isolate bug causes across many thousands
or millions of runs.  Because the process is driven by data from real
users, it implicitly attends to those program behaviors that cause the
most problems for the most users, most often.

Of course, our imperfect world is rarely so kind as to provide
programs with only a single bug.  A single complex commercial
application may have hundreds or thousands of latent flaws waiting to
be triggered.  Multiple bugs may come into play even within a single
run of an application.  Some errors may allow execution to continue
more-or-less normally, while one may eventually cause a crash.
Consider the fact that the Mozilla web browser project has a
``topcrash'' software QA team specifically dedicated to identifying
and tracking the top forty most common crash bugs.  This cut-off is
not because anyone believes that there are only forty bugs; it is an
acknowledgment that engineering resources are finite so one should
target the most important bugs first.  Fix one bug and another comes
into view.

Statistical isolation of a single bug is well understood
\cite{Zheng:2003:SDSP}.  In general terms, one looks for predicates on
program behavior which are strongly correlated with the binary outcome
of bug/no-bug, or crash/no-crash.  Both sparse sampling and
nondeterministic failure modes add noise to the process, but with a
large population of runs consistent trends emerge.  The presence of
multiple bugs complicates matters significantly, especially if we
assume that the number of distinct bugs is itself unknown.  Even if we
believe we have identified the direct cause of one bug, and even if we
had complete instead of sampled data, we should expect that the
``smoking gun'' will be absent from most failed runs, because no one
bug accounts for more than some small fraction of failures.

This paper presents an effective approach to isolating multiple bugs
in the face of such challenges.  Our discussion is organized as
follows.  \Autoref{sec:background} reviews the sampling
instrumentation infrastructure and the kind of feedback data that it
provides.  In \Autoref{sec:algorithm} we describe the main filtering
and ranking algorithm used to mine feedback data for bug causes.
\Autoref{sec:experiments:setup} defines the parameters of a case study
in multiple-bug isolation, while \Autoref{sec:experiments:results}
presents our results.  We review related work in
\Autoref{sec:related-work}, while \Autoref{sec:conclusions} concludes.

\section{Background}
\label{sec:background}

Software errors arise from unexpected interactions between inputs,
program internal state, and dynamic decisions made as the program
runs.  Software engineers reason about programs at the level of source
code statements and variables, and this is the level at which program
monitoring can expose properties of interest.  However, complete
source-level tracing of program behavior is impractical for field
deployment.  No end user would accept the performance overhead or
network bandwidth required to collect and transmit such a trace.

Instead, we use a combination of sparse random sampling and
client-side summarization.  The former controls performance overhead
while the later limits storage and transmission requirements.
However, each also adds noise and uncertainty to the resulting data.
The particulars of the sampling transformation have been reported
elsewhere \cite{PLDI`03*141}.  We highlight a few key properties here.
First, the sampling transformation is quite general: any collection of
statements within (or added to) a program may be designated as
``instrumentation'' and thereby sampled instead of run
unconditionally.  Second, samples are taken in a statistically fair
manner equivalent to a Bernoulli process: each potential sample is
taken or skipped randomly and independently as the program runs.
Lastly, the technique is highly effective at controlling performance
provided that sampling is sparse: average rates on the order of one
sample per hundred opportunities keep overhead low, often
unmeasurable.

The difficulty in using sparse sampling is that the resultant data is
both noisy and incomplete.  When \nicefrac{99}{100} of all monitored
program behaviors are omitted from any given report, it is impossible
to have a complete picture of what happened during a single run.  Some
predicate of interest may never have been true, or may simply have
been true but never recorded.  However, because sampling is fair,
large numbers of runs do converge on a truthful representation of
overall program behavior.

Orthogonal to the sampling transformation is the decision about what
instrumentation to introduce in the first place and how to concisely
summarize the resultant data.  This determines what behaviors one can
possibly observe once sampling is applied.  A useful instrumentation
scheme should be fairly general but selected to capture behaviors
which are likely to be of interest when hunting for bugs.  At present
our system offers the following instrumentation schemes:

\begin{description}
\item[branches:] Control flow is interesting.  At each conditional,
  count how often each branch is taken (equivalently, how often the
  conditional predicate is true versus false).  The observation is
  made just after the predicate is evaluated but before the selected
  branch is taken.  This scheme also applies to implicit conditionals
  in loops and logical operators (\texttt{\&\&}, \texttt{||},
  \texttt{?:}).  Each conditional induces one instrumentation site
  with two counters: number of times branched true and number of times
  branched false.

\item[returns:] Function return values are interesting.  In C, the
  sign of a result often used to encode the success or failure of some
  operation.  At each scalar-returning function call site, count how
  often the returned value is negative, zero, or positive.  For
  pointer-returning calls, this implicitly reduces to counting
  \texttt{NULL} versus non-\texttt{NULL}.  The observation is made
  just after the function returns but before the result is used by the
  original program.  An instrumentation site is added even if the
  source program discarded the return value, as unchecked return
  values are a common source of bugs.  Each call site induces one
  instrumentation site with three counters: number of negative
  returns, number of zero returns, and number of positive returns.

\item[scalar-pairs:] Values of variables are interesting.  Many bugs
  concern boundary issues exposed by the relationship between a pair
  of variables, or between a variable and some program constant.  At
  each scalar assignment \texttt{x = \dots}, identify each
  \emph{other} same-typed in-scope variable $\mathtt{y}_i$ and each
  constant expression $\mathtt{c}_j$.  Count how often the new value
  for \texttt{x} is less than, equal to, or greater than each
  $\mathtt{y}_i$ and each $\mathtt{c}_j$.  The observation is made
  after both sides of the assignment have been evaluated but just
  before the assignment itself takes place.  This lets us compare
  \texttt{x} to \texttt{x} as well, effectively comparing the new and
  old values of the left-hand side.  Each compared-to $\mathtt{y}_i$
  or $\mathtt{c}_j$ is treated as a distinct instrumentation site;
  thus a single assignment may induce a large number of sites.  Each
  such site maintains three counters: how often the value being
  assigned is less than, equal to, or greater than the compared-to
  variable or constant.
\end{description}

These schemes are quite broad; they represent a large set of wild
guesses as to what behavior may be interesting.  Nearly all of them
are wrong or irrelevant for any particular bug.  Engineers may enable
or disable any mix of schemes in a single binary, and may include or
exclude entire regions of code on a per-file or per-function basis.
For the most part, though, instrumenting an executable simply requires
switching to our instrumenting compiler.  By design the system
requires minimal human intervention at to build instrumented code.

The schemes described above share certain key properties.  In the
returns and scalar-pairs schemes, each site takes a large space of
possible observations (e.g.\ the exact value returned by a call) and
reduces it to a much smaller group of equivalence classes (e.g.\ the
sign of the returned value).  Furthermore, each site can be treated as
a set of predicates which form a complete, non-overlapping partition
of the behaviors observed by that site.  For example, at each
scalar-pairs comparison between \texttt{x} and \texttt{y}, one and
only one of $\mathtt{x} < \mathtt{y}$, $\mathtt{x} = \mathtt{y}$, or
$\mathtt{x} > \mathtt{y}$ can be true.  Thus one observation at one
site always updates exactly one of that site's counters.  An
observation that $\mathtt{x} < \mathtt{y}$ is therefore equivalent to
an observation that $\mathtt{x} \geq \mathtt{y}$.  This property is
exploited in \Autoref{sec:algorithm} to synthesize new predicates out
of existing observations.

The post-execution feedback report is a dump of the counter values for
each instrumentation site.  Reducing a trace to a set of counters
prevents us from reasoning about relative time ordering of events
during execution.  However, it also means that program actions early
in execution remain just as visible as those much later.  This
contrasts with traditional postmortem debugging tools which expose
only the final state of the program at the point of failure.

\section{Cause Isolation Algorithm}
\label{sec:algorithm}
\input{alg}

\section{Experimental Design}
\label{sec:experiments:setup}
\input{setup}

\section{Experimental Results}
\label{sec:experiments:results}
\input{experiments}

\section{Related Work}
\label{sec:related-work}

The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more probabilistic approach: we wish to identify broad trends over
time that correlate predicate violations with increased likelihood of
failure.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  We observe, however, that the
predicates injected by our instrumentor can approximate coverage: over
many runs, the sum of all predicate counters at a site converges on
the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses program slicing in conjunction
with Bayesian belief networks to filter and rank the possible causes
for a given bug.  Empirical evaluation show that the slicing component
alone finds 65\% of bug causes, while the probabilistic model
correctly identifies another 10\%.  This additional payoff may seem
small, especially in light of the effort, measured in multiple
man-years, required to distill experts' often tacit knowledge into a
formal belief network.  However, the approach does illustrate one
strategy for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al.\ \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but may be impractical to
deploy outside of a controlled testing environment.  As in our own
earlier work, Podgurski uses logistic regression to select features
which are highly predictive of failure.  \comment{Is it worth noting
  that we use different strategies for limiting the size of the set of
  selected features?  We use regularized logistic regression whereas
  Podgurski applies standard logistic regression to randomly selected
  subsets of the complete feature set and keeps the best-performing
  subset.}  Clustering tends to identify small, tight groups of runs
which do share a single cause but which are not always maximal.  (That
is, one cause may be split across several clusters.)

In contrast with Podgurski's statistical approach, widespread
industrial practice uses stack traces to cluster failure reports into
equivalence classes.  Two crash reports showing the same stack trace,
or perhaps only the same top-of-stack function, are presumed to be two
reports of the same failure.  This works to the extent that a single
cause corresponds to a single point of failure, but our experience
with \moss suggests that this assumption may not often hold.  We find
that only bugs \#2 and \#5 have truly unique ``signature'' stacks: a
crash location which is present if and only if the corresponding bug
was actually triggered.  These bugs are also our most deterministic.
Bugs \#4 and \#6 have nearly unique stack signatures, modulo small
changes to frames several levels removed from the point of failure.
The remaining bugs are much less consistent: each stack signature is
observed after a variety of different bugs, and each triggered bug
causes failure in a variety of different stack states.  Stack-based
clustering provides little insight for temporally extended bugs that
do not crash until well after the bad behavior.  A special case of
this is a program which produces incorrect output but exits normally
without crashing and therefore without any crash stack on which to
cluster.

Studies that attempt real-world deployment of monitored software must
address a host of practical engineering concerns, from distribution to
installation to user support to data collection and warehousing.
Elbaum and Hardojo \cite{Elbaum:2003:DISATA} have reported on a
limited deployment of instrumented Pine binaries.  Their experiences
have helped to guide our own design of a wide public deployment of
applications with sampled instrumentation, presently underway
\cite{Liblit:2003:CBIP}.

\comment{More recent work of interest:

  \begin{itemize}
  \item S.\ Elbaum, S.\ Kanduri and A.\ Andrews, ``Anomalies as
    Precursors of Field Failures, International Symposium of Software
    Reliability Engineering'', IEEE, November 2003.

  \item S.\ Elbaum and M.\ Hardojo, ``An Empirical Study on Profiling
    Strategies for Released Software and Their Application to QA
    Activities'', Technical Report TR03-09-01, University of Nebraska
    - Lincoln, September 2003.
  \end{itemize}

  Cannot find either online, but have written to Elbaum asking him for
  copies.}

For some highly available systems, even a single failure must be
avoided.  Once the behaviors that predict imminent failure are known,
automatic corrective measures may be able to prevent the failure from
occurring at all.  The Software Dependability Framework (SDF)
\cite{Gross:2003:PSMUST} uses the multivariate state estimation
technique to model and thereby predict impending system failures.
Instrumentation is assumed to be complete and is typically
domain-specific, whereas our sampled predicates cast a wider, less
specialized net.  \comment{We understand through informal
  communication that the SDF is able to anticipate when a player is
  about to lose an instrumented game of Tetris, and can intervene by
  removing rows to allow the game to continue.  But maybe I shouldn't
  say that, as it kind of makes their system sound like a joke.}

\comment{Cannot find any more recent work by these people in this
  area.  Where did they all go?  Porter has plenty of other recent
  work, but apparently nothing related.  Gross and McMaster have zero
  publication information on their home pages, while Umranov and Votta
  seem to have vanished entirely.  Have written to Gross and Porter
  asking if they have anything more recent I should look at.}

\section{Conclusions and Future Work}
\label{sec:conclusions}

When hunting for bugs, the first thing an engineer wants to know is
under what circumstances the failure occurs.  However, any complex
application is likely to contain not just one but many bugs, any of
which may have affected any single execution.  Isolating bug causes in
such an environment is a difficult process.

We have demonstrated an algorithm which turns multiple-bug isolation
into a numbers game.  Given feedback profiles of enough runs, overall
trends emerge which can help to guide an engineer to the most likely
causes of the most common bugs.  Our approach uses lightweight,
sampled instrumentation suitable for wide scale deployment to real end
users, which means that the system also performs implicit triage: it
learns the most, most quickly, about the bugs that happen most often.
The key property of our approach is that it filters and orders
potential causes based on the degree to which they \emph{increase} the
likelihood of failure.  This allows us to capture both deterministic
and non-deterministic bugs, and gives engineers a mechanism to gauge
the strength of the analysis results.

Our experience with \moss highlights the importance of selecting basic
instrumentation schemes that capture behavior commonly associated with
failure.  The branches and returns schemes yield a small number of
high quality bug causes.  Scalar-pairs instrumentation, while
potentially able to capture very subtle errors, suffers from an excess
of noise predicates from which the few ``smoking guns'' are difficult
to extract.  A more selective approach to injecting scalar-pairs
instrumentation warrants study.

Our \moss case study suggests several additional areas for future
development.  In the experiments reported here, sampling is
dynamically uniform: each crossing of each instrumentation site has
the same fixed chance of being sampled or skipped.  However not all
code is equally interesting.  Our approach can be generalized to
support non-uniform sampling.  By observing rare events more often, we
increase the odds of spotting key failure causes while ensuring that
the bulk of execution is spent in fast, sparsely-sampled code.

\placeholder{Yet to be addressed: need for better statistical models.
  This is a bit vague, but should at least discuss problems we
  encountered using logistic regression in the Moss experiment.}

The sampling instrumentor uses several program analyses to boost the
performance of instrumented code.  However, the post-run data analysis
described in \Autoref{sec:algorithm} is largely ignorant of program
structure.  These and other, deeper structural properties of the code
should be to help guide the filtering and ranking process.  For
example, the control flow graph can be used to direct engineers'
attention to likely first causes that appear earlier in the flow of
execution.

\bibliography{cacm1990,icse02,icse03,misc,paste02,pldi03,pods,ramss,refs}

\end{document}

%% LocalWords:  DIDUCE Burnell Horvitz Podgurski Elbaum Hardojo SDF
%% LocalWords:  cacm icse ramss pldi
