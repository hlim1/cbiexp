%% -*- TeX-master: t -*-

%% \documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% standard texmf packages
%%

\usepackage{amsmath}
\usepackage{ifthen}
\usepackage{nicefrac}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{xspace}

\usepackage[bookmarks, pdftitle={Statistical Debugging in the Presence of
  Multiple Errors}, pdfauthor={Ben Liblit, Mayur Naik, Alice X.
  Zheng, Alex Aiken, and Michael I.  Jordan}, pdfsubject={D.2.4
  [Software Engineering]: Software/Program Verification -- statistical
  methods; D.2.5 [Software Engineering]: Testing and Debugging --
  debugging aids, distributed debugging, monitors, tracing; I.5.2
  [Pattern Recognition]: Design Methodology -- feature evaluation and
  selection}, pdfkeywords={bug isolation, random sampling, invariants,
  feature selection, statistical debugging},
pdfpagemode=UseOutlines]{hyperref}

\ifthenelse{\isundefined{\pdfoutput}}{}{\usepackage{thumbpdf}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% unique to this paper
%%

\usepackage{Autoref}
\newcommand{\subfigureautorefname}[0]{Figure}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\rhythmbox}{\textsc{Rhythmbox}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}
\newcommand{\prob}{\mbox{\textit{Prob}}}
\newcommand{\fail}{\mbox{\textit{Crash}}}
\newcommand{\crash}{\mbox{\textit{Failure}}}
\newcommand{\context}{\mbox{\textit{Context}}}
\newcommand{\increase}{\mbox{\textit{Increase}}}
\renewcommand{\H}{{\mathcal{H}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% front matter
%%

\title{Statistical Debugging in the Presence of Multiple Errors
  %%
  \thanks{This research was supported in part by NASA Grant No.\
    NAG2-1210; NSF Grant Nos.\ EIA-9802069, CCR-0085949, ACI-9619020,
    and IIS-9988642; DOE Prime Contract No.\ W-7405-ENG-48 through
    Memorandum Agreement No.\ B504962 with LLNL; and DARPA ARO-MURI 
    ACCLIMATE DAAD-19-02-1-0383.  The information
    presented here does not necessarily reflect the position or the
    policy of the Government and no official endorsement should be
    inferred.}}
\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X.\ Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I.\ Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  document body
%%

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
  We present a statistical debugging algorithm that operates on very
  sparsely sampled data drawn from large numbers of user runs.  By
  identifying program behaviors that significantly increase the
  likelihood of failure, our technique helps guide software engineers
  to the most significant flaws in an application.  The approach has
  connections to statistical hypothesis testing and is validated
  using several case studies.
\end{abstract}


%\category{D.2.4}{Software Engineering}{Software/Program
%  Verification}[statistical methods]
%%
%\category{D.2.5}{Software Engineering}{Testing and
%  Debugging}[debugging aids, distributed debugging, monitors, tracing]
%%
%\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
%  evaluation and selection]

%\terms{Experimentation, Reliability}

%\keywords{bug isolation, random sampling, invariants, feature
%  selection, statistical debugging}


\section{Introduction}
\label{sec:introduction}

It is a fact of life that most software ships with many known and
unknown bugs.  Unfortunately, in most cases it is impractical to delay
releasing code until the last bug has been fixed.  For example, the
Mozilla web browser project has a ``topcrash'' software QA team
specifically dedicated to identifying and tracking the top forty most
common crash bugs.  This cut-off is not because anyone believes that
there are only forty bugs; it is an acknowledgment that engineering
resources are finite and so the most important bugs should be fixed
first.

Our vision is to use \termdef{statistical debugging} to improve
software quality after code is deployed.  Specially instrumented
programs monitor their own behavior and send feedback reports to a
central collection server.  Monitoring is both sparse and random, so
complete information is never available about any single run.
However, monitoring is also lightweight and therefore practical to
deploy to user communities numbering in the thousands or millions.
Statistical debugging does not seek to understand the single cause of
a single failure on one machine; rather, it identifies statistical
trends that isolate bug causes across many runs.  Because the process
is driven by data from actual user executions, it reflects the reality
of how the software is used and implicitly attends to those program
behaviors that cause the most problems for the most users, most often.

In previous work we examined the problem of automatically isolating
the causes of bugs and proposed two algorithms, the most general of
which was based on a statistical technique known as {\em logistic
regression} \cite{PLDI`03*141,Zheng:2003:SDSP}.  As we worked to apply
these methods to large programs under realistic conditions, we
discovered a number of scalability problems, the most serious of which
was that the performance of the techniques declined significantly 
when the applications being analyzed had multiple bugs (see
\Autoref{sec:comparison}).  As mentioned above, real applications
have many bugs.

The first contribution of this paper is a new algorithm for isolating
multiple bugs in complex applications (see \Autoref{sec:algorithm}).  This algorithm 
is based on an entirely different family of statistical techniques than our previous work and offers
a number of improvements:
it is much more efficient, the quality of the
results is high and unaffected by increases in either the size of the program
or the number of bugs, and it naturally produces information
that shows how important (in number of program crashes) each detected
cause of a bug is.  Furthermore, the method is statistically sound, meaning that
the results computed accurately summarize the analyzed program 
executions, and the results can be directly understood in terms of program behavior.
For example, here is a portion of a report generated for \rhythmbox,
a popular Open Source jukebox:
\begin{verbatim}
Predicate: g_source_remove() > 0 
Location: line 77 of disclosure-widget.c
Context: 0.08
Increase: 0.92   
Failure: 1.00   
\end{verbatim}
This information summarizes what is observed about a particular call site of the function {\tt g\_source\_remove}.
Any execution that reaches this function call has an 8\% chance of failure (the Context score).  If, however,
the function returns a
positive value\footnote{A common C programming idiom is for functions to signal success or failure by the sign of the return value.}, the probability that the program fails is 100\%; i.e., when this predicate on this line is true the program always crashes eventually.  We say that the predicate contributes an increase in the probability of failure of 100\% - 8\% = 92\%.

It turns out that the crash happens long after the call to {\tt g\_source\_remove} completes,
but the fact that {\tt g\_source\_remove} succeeds (indicated by the positive return value) is part of what causes the bug.  Thus,
our approach can help engineers isolate bugs by showing the conditions and early events that trigger the bug, rather than just the
program state at the point of failure.  Furthermore, our algorithm also allows accurate calculations of what fraction of all failures occur when this predicate
is true; in the example above, the predicate was true in 184 of 1873 \rhythmbox\ executions that crashed in our dataset, which means that
it is involved in 9.8\% of all the failures observed.  This information allows engineers to assign accurate priorities to bug fixes.


Our second contribution is a series of case studies evaluating the effectiveness of
our technique.  The first two studies compare our algorithm to the two techniques
we previously proposed (see \Autoref{sec:revisited}).  Each of these studies focuses on a
program with a single bug; in both cases the new algorithm
performs at least as well as the old one at identifying the conditions under which the bug occurs.  

The third study involves an application with multiple known bugs (see
\Autoref{sec:experiments:results}).  Knowing the bugs makes this a controlled study, 
in the sense that we are able to determine whether the algorithm fails to recognize any of the
bugs.  The results show that the algorithm correctly
isolates strong predictors for each of the bugs that occurred in our
experiment, even though the algorithm has no knowledge of how many
bugs are in the program or which bug caused any individual failure.
In addition, our algorithm isolated another bug in this application that was previously unknown.

We also use this third case study to investigate a speculative
application of our techniques.  Our algorithm relies only on the
ability to label the outcome of a program as ``correct'' or
``incorrect''.  Furthermore, because our techniques are statistical,
the labeling of correct/incorrect results need not be perfect; our
algorithm can tolerate noise in the labeling.  These features open up
the possibility of having a single approach that can detect a wide
range of program errors from low-level, crashing bugs to high-level
logic bugs that do not crash the program but simply cause it to
produce incorrect output, provided only that there is some way to
label program outcomes reasonably accurately.  Detecting incorrect
output can be done in a number of ways.  Certainly program crashes are
incorrect (e.g., in C programs), as are unhandled top-level exceptions
(e.g., in Java programs).  Internal program checks for consistency,
such as low-level assert statements and higher-level input validation
checks between untrusting components, can also be used to label
results as correct or incorrect.  Finally (and this is the speculative
part) users could also provide feedback on whether the output of a
program execution appears correct or not, allowing the automatic
isolation of bugs that are not otherwise observed by a crash or
internal consistency check.  The third study includes some subtle,
non-crashing bugs that our system is able to isolate correctly.


In the fourth study we apply our techniques to the current release
of \rhythmbox, the Open Source jukebox mentioned above (see
\Autoref{sec:rb}).  \rhythmbox\ is the largest and most complex of our
case studies, and the bugs in \rhythmbox\ are unknown to us.  Thus far
we have successfully isolated two serious crashing bugs in \rhythmbox\ 
using our approach.  At least in part because we were able to
quantify the importance of these bugs for the \rhythmbox\ developers, both
bug reports were given high priority and fixes were applied to the
main development branch within a few days.


The paper is organized as follows.  After providing some background
(\Autoref{sec:background}), we discuss our algorithm
(\Autoref{sec:algorithm}) and its advantages over our previous approach
based on logistic regression (\Autoref{sec:comparison}).  The case studies
are presented in Sections~\ref{sec:revisited}-\ref{sec:rb}.  We conclude with a
discussion of related work (\Autoref{sec:related-work}) and future
work (\Autoref{sec:conclusions}). 

\section{Background}
\label{sec:background}

This section summarizes ideas and terminology needed to to present our
algorithm.  The ideal program monitoring system would gather complete
execution traces and provide them to an engineer (or, more likely, a
tool) to mine for the causes of bugs.  However, complete tracing of
program behavior is simply impractical; no end user would accept the
required performance overhead or network bandwidth.

Instead, we use a combination of sparse random sampling, which controls
performance overhead, and client-side summarization of the data, which
limits the storage and transmission costs.  We briefly discuss
both aspects.

Random sampling is added to a program via a source-to-source transformation.
Our sampling transformation is general: any collection of
statements within (or added to) a program may be designated as
``instrumentation'' and thereby sampled instead of run
unconditionally.  That is, each time instrumentation code is reached,
a coin flip decides whether the instrumentation is executed or not.
Coin flipping is simulated in a statistically fair
manner equivalent to a Bernoulli process: each potential sample is
taken or skipped randomly and independently as the program runs.
We have found that a sampling rate of \nicefrac{1}{100} normally keeps the performance overhead
of instrumentation low, often unmeasurable.

Orthogonal to the sampling transformation is the decision about what
instrumentation to introduce and how to concisely
summarize the resultant data.  This determines what behaviors one can
possibly observe once the sampling transformation is applied.  A useful instrumentation
scheme should be fairly general but selected to capture behaviors
which are likely to be of interest when hunting for bugs.  At present
our system offers the following instrumentation schemes:

\begin{description}
\sloppy
\item[branches:] At each conditional, including implicit conditionals
such as loop tests and short-circuiting logical operators,
  count how often each branch is taken.  
%The observation is
%  made just after the predicate is evaluated but before the selected
%  branch is taken.  This scheme also applies to implicit conditionals
%  in loops and logical operators (\texttt{\&\&}, \texttt{||},
%  \texttt{?:}).  Each conditional induces one instrumentation site
%  with two counters: number of times branched true and number of times
%  branched false.

\item[returns:] In C, the
  sign of a return value is often used to encode an operation's success or failure.
  At each scalar-returning function call site, count how
  often the returned value is negative, zero, or positive.  For
  pointer-returning calls, this scheme reduces to counting
  \texttt{NULL} versus non-\texttt{NULL}.  
%The observation is made
%  just after the function returns but before the result is used by the
%  original program.  An instrumentation site is added even if the
%  source program discards the return value, as unchecked return
%  values are a common source of bugs.  Each call site induces one
%  instrumentation site with three counters: number of negative
%  returns, number of zero returns, and number of positive returns.

\item[scalar-pairs:] Many bugs
  concern boundary issues exposed by the relationship between a pair
  of variables, or between a variable and some program constant.  At
  each scalar assignment \texttt{x = \dots}, identify each
  \emph{other} same-typed in-scope variable $\mathtt{y}_i$ and each
  constant expression $\mathtt{c}_j$.  Count how often the new value
  for \texttt{x} is less than, equal to, or greater than each
  $\mathtt{y}_i$ and each $\mathtt{c}_j$.  
%The observation is made
%  after both sides of the assignment have been evaluated but
%  before the assignment takes place.  This lets us compare
%  \texttt{x} to \texttt{x} as well, effectively comparing the new and
%  old values of the left-hand side.  
Each compared-to $\mathtt{y}_i$
  or $\mathtt{c}_j$ is treated as a distinct instrumentation site;
  thus a single assignment may induce a large number of sites.  
% Each
%  such site maintains three counters: how often the value being
%  assigned is less than, equal to, or greater than the compared-to
%  variable or constant.
\end{description}

The common theme among these instrumentation schemes is that we
instrument program predicates and count the number of times that these
predicates are observed to be true or false when a sample is taken.
For the returns and scalar-pairs instrumentation schemes, the three
predicates that are tracked explicitly actually give rise to a family
of six predicates that we use in our analysis of the data.
For example, for each scalar-pairs comparison between \texttt{x} and \texttt{y}, one and
only one of the three predicates $\mathtt{x} < \mathtt{y}$, $\mathtt{x} = \mathtt{y}$, or
$\mathtt{x} > \mathtt{y}$ can be true.  Thus one observation at one
site always updates exactly one of that site's counters.  An
observation that $\mathtt{x} < \mathtt{y}$ is true is therefore equivalent to
an observation that $\mathtt{x} \geq \mathtt{y}$ is false.  Similarly, we can
infer from the data the two remaining predicates $\mathtt{x} \leq \mathtt{y}$ or
$\mathtt{x} \neq \mathtt{y}$.

Also note that because some counter is always incremented at every
sample in all three schemes, we can easily determine the number of
times a site was sampled by summing all the counters for the site
(two counters for the branches instrumentation and three counters for
the others).  Thus, we can distinguish between predicates that are
never observed during execution (all the counters at the site are 0)
and predicates that are observed but never true (the counter(s) for the predicate
is 0, but some other counter at the site is positive).

The post-execution feedback report consists of the final counter values for
each instrumentation site.  Reducing a trace to a set of counters
prevents us from reasoning about relative time ordering of events
during execution.  However, it also means that program actions early
in execution remain just as visible as those much later.  This
contrasts with traditional postmortem debugging tools which expose
only the final state of the program at the point of failure.

\section{Cause Isolation Algorithm}
\label{sec:algorithm}
\input{alg}

\section{Experiments Revisited}
\label{sec:revisited}
\input{revisited}


\section{A Controlled Experiment}
\label{sec:experiments:results}
\input{experiments}


\section{Rhythmbox}
\label{sec:rb}


For our final case study we instrumented \rhythmbox, an Open Source
jukebox application.  Thus far we have isolated two serious crashing
bugs in \rhythmbox, and fixes for both bugs have been accepted and
applied to the main branch of the \rhythmbox\ source tree.  We briefly
describe the two bugs and the lessons we have learned from working on
a large application with unknown bugs.

The first bug, discssed briefly in Section~\ref{sec:introduction}, is
predicted to occur when a particular call to {\tt g\_source\_remove}
returns a positive value.  This function is part of the GTK, a graphical
object toolbox for C.
%%an object system for C.  
%%(?? "an object system for C" doesn't make sense.  -- Alice)
Objects in GTK can be referred to both
via pointers and via object IDs; {\tt g\_source\_remove}
takes an object ID as its argument.

In this case the problem is that the object ID passed as an argument
is stale---the object it belongs to was destroyed much earlier in the
prorgram, but the object ID was retained in the owning object's fields.
A section of clean-up code much later in the program dutifully calls {\tt g\_source\_remove} on the
stale ID; if the ID has been reallocated, the unfortunate object that
has that ID is then destroyed, wreaking havoc in the rest of the execution.

A strong predictor for the second bug is 
\begin{verbatim}
monkey_media_player_get_uri() == NULL 
\end{verbatim} 
This function
returns the current play source (e.g., a CD library or internet radio
station) for \rhythmbox.  The interesting thing is that this source
should never be {\tt NULL} while \rhythmbox\ is still playing.  This
observation leads to the bug.  GTK is reference-counted, and the
programmer has freed a complex data structure by removing references
and nulling fields (thus the predicate above) in the belief that the objects are freed. But there
is a race condition: one object sometimes survives long enough to
invoke callbacks into the other objects that have been reclaimed,
resulting in a crash.

These bugs were the most complex that we dealt with in 
our experiments (although one or two of the \moss\ bugs are close).  The
call stack at the point of crash for both bugs is unhelpful (see Section~\ref{sec:conclusions}).  We feel confident in saying that without the use of
our tool we could not have isolated these bugs, and we believe
 the fact that these crashing bugs have gone undiscovered in \rhythmbox\ is
good evidence that they are not easy to find without tool support.

On the other hand, it should also be clear from our descriptions of the bugs
that our techniques did not point us as directly to the bugs as one might hope.
Working only from the output of our system and having no prior knowledge of
the rather involved architecture of \rhythmbox, it took us considerable time
to understand these two bugs.  

The lesson is that while our algorithm is theoretically sound and (in
our opinion) performs well in practice, it is only as good as the
predicates chosen for instrumentation.  Our current instrumentation
strategies focus on low-level properties of the C code, while \rhythmbox\
is really written in a higher-level language, namely at the level of
the GTK API.  We have already identified a new instrumentation strategy
that makes use of the semantics of the GTK and would have led us
much more quickly to the second bug; such a strategy, while GTK-specific,
would still be widely applicable, as GTK is widely used.  We expect
that such system- or API-specific instrumentation strategies would be
needed in many cases to obtain maximum benefit from our techniques.


\section{Related Work}
\label{sec:related-work}

In this section we briefly survey related work. There is currently a great
deal of interest in applying static analysis to improve software quality.  Some,
but certainly not all, of the bugs we found in our studies could also have
been detected by known static analyses.  While we firmly believe in
the use of static analysis to find and prevent bugs, our dynamic approach has
advantages. A dynamic analysis can observe actual run-time values, which is often
better than either making a very conservative static assumption about runtime
values to be sound, or allowing some even very simple bugs to escape undetected.
Another advantage of dynamic analysis, especially one that uses actual user
executions for its data, is the ability to assign an accurate importance to each
bug.
  
The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more statistical approach.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  Our
predicates, however, give coverage information: the sum of all predicate counters at a site converges to the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses program slicing in conjunction
with Bayesian belief networks to filter and rank the possible causes
for a given bug.  Empirical evaluation shows that the slicing component
alone finds 65\% of bug causes, while the probabilistic model
correctly identifies another 10\%.  This additional payoff may seem
small in light of the effort, measured in multiple
man-years, required to distill experts' often tacit knowledge into a
formal belief network.  However, the approach does illustrate one
strategy for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al.\ \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but greatly restricts the
instrumentation schemes that are practical to deploy outside of a
controlled testing environment.  As in our own earlier work, Podgurski
uses logistic regression to select features which are highly
predictive of failure.  
Clustering tends to identify small, tight groups of runs which do
share a single cause but which are not always maximal.  That is, one
cause may be split across several clusters.

In contrast, current
industrial practice uses stack traces to cluster failure reports into
equivalence classes.  Two crash reports showing the same stack trace,
or perhaps only the same top-of-stack function, are presumed to be two
reports of the same failure.  This works to the extent that a single
cause corresponds to a single point of failure, but our experience
with \moss\ and \rhythmbox\ suggests that this assumption may not often hold.  In \moss\ find
that only bugs 2 and 5 have truly unique ``signature'' stacks: a
crash location which is present if and only if the corresponding bug
was actually triggered.  These bugs are also our most deterministic.
Bugs 4 and 6 also have nearly unique stack signatures.
The remaining bugs are much less consistent: each stack signature is
observed after a variety of different bugs, and each triggered bug
causes failure in a variety of different stack states.  In \rhythmbox,
both bugs caused crashes so long after the bad behavior that the crash stacks
were not useful at all.

Studies that attempt real-world deployment of monitored software must
address a host of practical engineering concerns, from distribution to
installation to user support to data collection and warehousing.
Elbaum and Hardojo \cite{Elbaum:2003:DISATA} have reported on a
limited deployment of instrumented Pine binaries.  Their experiences
have helped to guide our own design of a wide public deployment of
applications with sampled instrumentation, presently underway
\cite{Liblit:2003:CBIP}.

For some highly available systems, even a single failure must be
avoided.  Once the behaviors that predict imminent failure are known,
automatic corrective measures may be able to prevent the failure from
occurring at all.  The Software Dependability Framework (SDF)
\cite{Gross:2003:PSMUST} uses the multivariate state estimation
technique to model and thereby predict impending system failures.
Instrumentation is assumed to be complete and is typically
domain-specific.

\section{Conclusions and Future Work}
\label{sec:conclusions}

We have demonstrated a practical algorithm for isolating multiple bugs
in complex software systems.  Given feedback profiles of enough runs,
overall trends emerge which can help to guide an engineer to the most
likely causes of the most common bugs.  Our approach uses lightweight,
sampled instrumentation suitable for wide scale deployment to real end
users, which means that the system also performs implicit triage: it
learns the most, most quickly, about the bugs that happen most often.
The key property of our approach is that it filters potential causes
based on the degree to which they increase the likelihood of failure.
Our algorithm appears to do a good job of isolating a wide variety of
bugs, even when multiple bugs are present simultaneously.

When hunting for bugs, the first thing an engineer wants to know is
under what circumstances the failure occurs.  Perhaps the most salient
feature of our approach is the ability to pinpoint the circumstances
under which bugs occur.

We also see several possible avenues for improvement, which we leave
as future work:
\begin{itemize}

\item The scalar-pairs instrumentation scheme induces many
predicates and finds fewer bugs than the branches and
returns instrumentation schemes.  A more specialized version of scalar-pairs
would be useful.

\item To date, we have sampled all sites at the same rate.  However,
rarely executed code, such as code that executes once on system start-up,
can be sampled at a higher rate with no impact on overall performance.
Moreover, infrequently executed code is more likely
to harbor bugs.  By observing rare events more often, we would need fewer
runs to isolate bugs.

\item The algorithm we present here analyzes every predicate independently
of every other predicate.  We believe we can do more to exploit
statistical correlations between predicates.


\end{itemize}


\bibliography{cacm1990,icse02,icse03,misc,paste02,pldi03,pods,ramss,refs}

\end{document}

%% LocalWords:  DIDUCE Burnell Horvitz Podgurski Elbaum Hardojo SDF
%% LocalWords:  topcrash cacm icse ramss pldi Podgurski's Kanduri
%% LocalWords:  McMaster Umranov Votta
