\documentclass{acm_proc_article-sp}
%% \documentclass{sig-alternate}

%% standard texmf packages
\usepackage{color}
\usepackage{hyperref}
\usepackage{nicefrac}
\usepackage{xspace}

%% unique to this paper
\usepackage{Autoref}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}

%% remove the following before submission!
\newcommand{\placeholder}[1]{{\color[cmyk]{0,0.61,0.87,0}[#1]}}
\pagenumbering{arabic}

\title{\placeholder{Title Needed Here}
  %%
  \thanks{\placeholder{This grant boilerplate is from the PLDI'03
    paper.  Newer text is needed here.}  This research was supported
    in part by NASA Grant No. NAG2-1210; NSF Grant Nos. EIA-9802069,
    CCR-0085949, ACI-9619020, and IIS-9988642; DOE Prime Contract
    No. W-7405-ENG-48 through Memorandum Agreement No. B504962 with
    LLNL; and a Lucent GRPW Fellowship.  The information presented
    here does not necessarily reflect the position or the policy of
    the Government and no official endorsement should be inferred.}}

\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X. Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I. Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
\placeholder{Abstract needed here.}
\end{abstract}

\category{D.2.5}{Software Engineering}{Testing and
  Debugging}[distributed debugging]
%%
\category{G.3}{Mathematics of Computing}{Probability and
  Statistics}[correlation and regression analysis]
%%
\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
  evaluation and selection]

\terms{Experimentation, Performance, Reliability}

\keywords{bug isolation, random sampling, assertions, feature
  selection, statistical debugging, logistic regression}

\placeholder{The preceding categories, terms, and keywords were copied
  from the PLDI'03 paper.  Some may need to be changed.  Note that the
  categories and terms are \emph{not} free-form; see
  \url{http://www.acm.org/sigs/pubs/proceed/sigguide-v2.1sp.htm}
  sections 2.3.2 and 2.3.3 for details.}


\section{Introduction}
\label{sec:introduction}

\placeholder{Page budget: 1.}


\section{Background}
\label{sec:background}

\placeholder{Page budget: 1.}


\section{\placeholder{Algorithm}}
\label{sec:algorithm}

\placeholder{Page budget: 2.}


\section{\placeholder{Experiments}}
\label{sec:experiments}

\subsection{\placeholder{Setup}}
\label{sec:experiments:setup}

\placeholder{Page budget: 1.}

\input{setup}

\subsection{\placeholder{Results}}
\label{sec:experiments:results}

\placeholder{Page budget: 2, including pictures.}


\section{\placeholder{Future Work / Alternate Approaches}}
\label{sec:future-work}

\placeholder{Page budget: \nicefrac{1}{2}.}

Our experience with \moss suggests several key areas for future
development.

In the experiments reported here, sampling is dynamically uniform:
each crossing of each instrumentation site has the same fixed chance
of being sampled or skipped.  However not all code is equally
interesting.  The details of a one-time configuration action may have
more impact on a bug than some bulk initialization loop that iterates
millions of times.  Two generalizations of our approach support
non-uniform sampling.

One might replace the single global ``next sample'' countdown with
several countdowns having a variety of means.  Each acyclic region
uses one of these counters for all of its sites.  Thus,
heavily-trafficked code might be sampled at a rate of
\nicefrac{1}{1000}, while moderate-use code uses \nicefrac{1}{100} and
rarely executed initialization and error-handling code uses
\nicefrac{1}{10} or even \nicefrac{1}{1} (i.e. no sampling; complete
data collection).

A complementary extension is to allow individual sites to decrement
the ``next sample'' countdown more than once.  Decrementing once is
equivalent to tossing a biased coin once and only taking a sample if
that toss comes up (e.g.) heads.  Decrementing twice simulates taking
a sample if either of two tosses comes up heads.  For a basic
underlying countdown with mean $d$, decrementing $n$ times yields an
effective sampling rate of $(1 - (1 - \frac{1}{d})^n)$.  For purposes
of checking countdown thresholds at the tops of acyclic regions, a
site that decrements $n$ times is equivalent to $n$ sites that
decrement once.  Thus each global ``next sample'' countdown actually
supports a family of sampling rates selectable on a per-site basis.

\placeholder{The above assumes that we have already given a refresher
  on global countdowns and threshold checks earlier in the paper.
  Rewording will be needed if we don't get into that amount of
  detail.}

\placeholder{Yet to be addressed: how you actually decide what rate to
  use for a given site.  Presumably some mix of ad-hoc guessing,
  coverage information from in-house testing, and dynamic adjustment
  in the field based on what you're seeing in the feedback reports
  you've gotten so far.}

\placeholder{Yet to be addressed: need for better statistical models.
  This is a bit vague, but should at least discuss problems we
  encountered using logistic regression in the Moss experiment.}

The sampling instrumentor uses several program analyses to boost the
performance of instrumented code.  However, the post-run data analysis
described in \Autoref{sec:algorithm} is largely ignorant of program
structure.  Exploiting properties visible in the source code can allow
further filtering of suspect predicates.  We expect that in any given
failed execution, there are two points of greatest interest to an
engineer: the place where things first started to go wrong, and the
place where an immediate failure became truly inevitable.  Thus,
predicates with high crash scores \placeholder{Is ``crash score'' the
  right term here?  Check against \Autoref{sec:algorithm}.} are most
interesting when they appear close to program entry or close to the
location of an actual crash.

This suggests an approach based on traversal of the control flow
graph.  To find earliest causes, search forward from program entry for
points at which the chance of failure substantially increases.  To
find latest causes, search backward from crash locations for points at
which the chance of failure substantially decreases.  Predicate
observations provide additional clues, and could be incorporated to
further restrict the search to only those paths which are consistent
with observed behavior.  \placeholder{The preceding feels too detailed
  for an algorithm we haven't actually implemented.  I'd rather
  provide sketches of more distinct ideas rather than presenting just
  one idea in this much detail.}

\section{Related Work}
\label{sec:related-work}

\placeholder{Page budget: 1.}

The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more probabilistic approach: we wish to identify broad trends over
time that correlate predicate violations with increased likelihood of
failure.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  We observe, however, that the
predicates injected by our instrumentor can approximate coverage: over
many runs, the sum of all predicate counters at a site converges on
the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses a mix of program slicing and
Bayesian belief networks to filter and rank the possible causes for a
given bug.  Empirical evaluation show that the slicing component alone
finds 65\% of bug causes, while the probabilistic model correctly
identifies another 10\%.  This additional payoff may seem small,
especially in light of the effort, measured in multiple man-years,
required to distill experts' often tacit knowledge into a formal
belief network.  However, the approach does illustrate one strategy
for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but may be impractical to
deploy outside of a controlled testing environment.  As in our own
earlier work, Podgurski uses logistic regression to select features
which are highly predictive of failure.  \placeholder{Is it worth
  noting that we use different strategies for limiting the size of the
  set of selected features?  We use regularized logistic regression
  whereas Podgurski applies standard logistic regression to randomly
  selected subsets of the complete feature set and keeps the
  best-performing subset.}  Clustering tends to identify small, tight
groups of runs which do share a single cause but which are not always
maximal.  (That is, one cause may be split across several clusters.)

\placeholder{Should also discuss recent developments by anybody from
  the RAMSS workshop.  The following appear worth following up on:

  \begin{itemize}
  \item Sebastian Elbaum and Madeline Hardojo, ``Deploying
    Instrumented Software to Assist the Testing Activity''.  Discusses
    experiences with a deployment of instrumented Pine binaries.

    More recent work of interest:

    \begin{itemize}
    \item S. Elbaum, S.  Kanduri and A. Andrews, ``Anomalies as
      Precursors of Field Failures, International Symposium of
      Software Reliability Engineering'', IEEE, November 2003.

    \item S. Elbaum and M. Hardojo, ``An Empirical Study on
      Profiling Strategies for Released Software and Their
      Application to QA Activities'', Technical Report TR03-09-01,
      University of Nebraska - Lincoln, September 2003.
    \end{itemize}

    Cannot find either online, but have written to Elbaum asking him
    for copies.

  \item Kenny C. Gross, Scott McMaster, Adam Porter, ``Proactive
    System Maintenance Using Software Telemetry''.  Discusses
    technique for predicting imminent program failure and
    intervening with corrective measures.

    Cannot find any more recent work in this area.  What happened to
    these people?  Porter has plenty of other recent work, but
    apparently nothing related.  Gross and McMaster have zero
    publication information on their home pages, while Umranov and
    Votta seem to have vanished entirely.  Have written to Gross and
    Porter asking if they have anything more recent I should look at.
    \end{itemize}}

\section{Conclusions}
\label{sec:conclusions}

\bibliography{cacm1990,icse02,icse03,misc,paste02,pods,refs}
\placeholder{Page budget for references: 1.}


\end{document}
