%% -*- TeX-master: t -*-

\documentclass{acm_proc_article-sp}
%% \documentclass{sig-alternate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% standard texmf packages
%%

\usepackage{nicefrac}
\usepackage{fancyvrb}
\usepackage{xspace}

\usepackage[bookmarks, pdftitle={Statistical Debugging in the Presence
  of Multiple Bugs}, pdfauthor={Ben Liblit, Mayur Naik, Alice X.
  Zheng, Alex Aiken, and Michael I.  Jordan}, pdfsubject={D.2.4
  [Software Engineering]: Software/Program Verification -- statistical
  methods; D.2.5 [Software Engineering]: Testing and Debugging --
  debugging aids, distributed debugging, monitors, tracing; I.5.2
  [Pattern Recognition]: Design Methodology -- feature evaluation and
  selection}, pdfkeywords={bug isolation, random sampling, invariants,
  feature selection, statistical debugging},
pdfpagemode=UseOutlines]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% unique to this paper
%%

\usepackage{Autoref}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}
\newcommand{\prob}{\mbox{\textit{Prob}}}
\newcommand{\fail}{\mbox{\textit{Fail}}}
\newcommand{\crash}{\mbox{\textit{Crash}}}
\newcommand{\context}{\mbox{\textit{Context}}}
\newcommand{\increase}{\mbox{\textit{Increase}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% remove the following before submission!
%%

\usepackage{color}
\newcommand{\placeholder}[1]{{\color[cmyk]{0,0.61,0.87,0}[#1]}}
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% front matter
%%

\title{Statistical Debugging in the Presence of Multiple Bugs
  %%
  \thanks{\placeholder{This grant boilerplate is from the PLDI'03
      paper.  Newer text is needed here.}  This research was supported
    in part by NASA Grant No.\ NAG2-1210; NSF Grant Nos.\ EIA-9802069,
    CCR-0085949, ACI-9619020, and IIS-9988642; DOE Prime Contract No.\ 
    W-7405-ENG-48 through Memorandum Agreement No.\ B504962 with LLNL;
    and a Lucent GRPW Fellowship.  The information presented here does
    not necessarily reflect the position or the policy of the
    Government and no official endorsement should be inferred.}}

\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X.\ Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I.\ Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  document body
%%

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
\placeholder{Abstract needed here.}
\end{abstract}

\category{D.2.4}{Software Engineering}{Software/Program
  Verification}[statistical methods]
%%
\category{D.2.5}{Software Engineering}{Testing and
  Debugging}[debugging aids, distributed debugging, monitors, tracing]
%%
\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
  evaluation and selection]

\terms{Experimentation, Reliability}

\keywords{bug isolation, random sampling, invariants, feature
  selection, statistical debugging}


\section{Introduction}
\label{sec:introduction}

\placeholder{Page budget: 1.}


\section{Background}
\label{sec:background}

\placeholder{Page budget: 1.}

Software errors arise from unexpected interactions between inputs,
program internal state, and dynamic decisions made as the program
runs.  Software engineers reason about programs at the level of source
code statements and variables, and this is the level at which program
monitoring can expose properties of interest.  However, complete
source-level tracing of program behavior is impractical for field
deployment.  No end user would accept the performance overhead or
network bandwidth required to collect and transmit such a trace.

Instead, we use a combination of sparse random sampling and
client-side summarization.  The former controls performance overhead
while the later limits storage and transmission requirements.
However, each also adds noise and uncertainty to the resulting data.
We highlight a few key properties here.  First, the sampling
transformation is quite general: any collection of statements within
(or added to) a program may be designated as ``instrumentation'' and
thereby sampled instead of run unconditionally.  Second, samples are
taken in a statistically fair manner equivalent to a Bernoulli
process: each potential sample is taken or skipped randomly and
independently as the program runs.  Lastly, the technique is highly
effective at controlling performance provided that sampling is sparse:
average rates on the order of one sample per hundred opportunities
keep overhead low, often unmeasurable.

The difficulty in using sparse sampling is that the resultant data is
both noisy and incomplete.  When \nicefrac{99}{100} of all monitored
program behaviors are omitted from any given report, it is impossible
to have a complete picture of what happened during a single run.  Some
predicate of interest may never have been true, or may simply have
been true but never recorded.  However, because sampling is fair,
large numbers of runs do converge on a truthful representation of
overall program behavior.

The particulars of the sampling transformation have been reported
elsewhere \cite{PLDI`03*141}.  Orthogonal to that is the decision
about what instrumentation to introduce in the first place.  This
decision determines what behaviors one can possibly observe once
sampling is applied.  A instrumentation useful scheme should be fairly
general but selected to capture behaviors which are likely to be of
interest when hunting for bugs.  At present our system offers the
following instrumentation schemes:
  
\begin{description}
\item[branches] Control flow is interesting.  At each conditional,
  count how often each branch is taken (equivalently, how often the
  conditional predicate is true versus false).  The observation is
  made just after the predicate is evaluated but before the selected
  branch is taken.  This scheme also applies to implicit conditionals
  in loops and logical operators (\texttt{\&\&}, \texttt{||},
  \texttt{?:}).  Each conditional induces one instrumentation site
  with two counters: number of times brached true and number of times
  branched false.
  
\item[returns] Function return values are interesting.  In C, the sign
  of a result often used to encode the success or failure of some
  operation.  At each scalar-returning function call site, count how
  often the returned value is negative, zero, or positive.  For
  pointer-returning calls, this implicitly reduces to counting
  \texttt{NULL} versus non-\texttt{NULL}.  The observation is made
  just after the function returns but before the result is used by the
  original program.  An instrumentation site is added even if the
  source program discarded the return value, as ignoring returned
  error codes is a common source of bugs.  Each call site induces one
  instrumentation site with three counters: number of negative
  returns, number of zero returns, and number of positive returns.
  
\item[scalar-pairs] Values of variables are interesting.  Many bugs
  concern boundary issues exposed by the relationship between a pair
  of variables, or between a variable and some program constant.  At
  each scalar assignment \texttt{x = \dots}, identify each
  \emph{other} same-typed in-scope variable $\mathtt{y}_i$ and each
  constant expression $\mathtt{c}_j$.  Count how often the new value
  for \texttt{x} is less than, equal to, or greater than each
  $\mathtt{y}_i$ and each $\mathtt{c}_j$.  The observation is made
  after both sides of the assignment have been evaluated but just
  before the assignment itself takes place.  This lets us compare
  \texttt{x} to \texttt{x} as well, effectively comparing the new and
  old values of the left-hand side.  Each compared-to $\mathtt{y}_i$
  or $\mathtt{c}_j$ is treated as a distinct instrumentation site;
  thus a single assignment may induce a large number of sites.  Each
  such site maintains three counters: how often the value being
  assigned is less than, equal to, or greater than the compared-to
  variable or constant.
\end{description}

These schemes are quite broad; they represent a large set of wild
guesses as to what behavior may be interesting.  Nearly all of them
are wrong or irrelevant for any particular bug.  Engineers may enable
or disable any mix of schemes in a single binary, and may include or
exclude entire regions of code on a per-file or per-function basis.
For the most part, though, instrumenting an executable simply requires
switching to our instrumenting compiler.  By design the system
requires minimal human intervention at to build instrumented code.

The schemes described above share certain key properties.  In the
returns and scalar-pairs schemes, each site takes a large space of
possible observations (e.g.\ the exact value returned by a call) and
reduces it to a much smaller group of equivalence classes (e.g.\ the
sign of the returned value).  Furthermore, each site can be treated as
a set of predicates which form a complete, non-overlapping partition
of the behaviors observed by that site.  For example, at each
scalar-pairs comparison between \texttt{x} and \texttt{y}, one and
only one of $\mathtt{x} < \mathtt{y}$, $\mathtt{x} = \mathtt{y}$, or
$\mathtt{x} > \mathtt{y}$ can be true.  Thus one observation at one
site always updates exactly one of that site's counters.  An
observation that $\mathtt{x} < \mathtt{y}$ is therefore equivalent to
an observation that $\mathtt{x} \ge \mathtt{y}$.  This property is
exploited in \Autoref{sec:algorithm} to synthesize new predicates out
of existing observations.

The post-execution feedback report is a dump of the counter values for
each instrumentation site.  Reducing a trace to a set of counters
prevents us from reasoning about relative time ordering of events
during execution.  However, it also means that program actions early
in execution remain just as visible as those much later.  This
contrasts with traditional postmortem debugging tools which expose
only the final state of the program at the point of failure.

\section{Algorithm}
\label{sec:algorithm}

\placeholder{Page budget: 2.}

\input{alg}

\section{\placeholder{Experiments}}
\label{sec:experiments}

\subsection{\placeholder{Setup}}
\label{sec:experiments:setup}

\placeholder{Page budget: 1.}

\input{setup}

\subsection{\placeholder{Results}}
\label{sec:experiments:results}

\placeholder{Page budget: 2, including pictures.}


\section{\placeholder{Future Work / Alternate Approaches}}
\label{sec:future-work}

\placeholder{Page budget: \nicefrac{1}{2}.}

Our experience with \moss suggests several key areas for future
development.

In the experiments reported here, sampling is dynamically uniform:
each crossing of each instrumentation site has the same fixed chance
of being sampled or skipped.  However not all code is equally
interesting.  The details of a one-time configuration action may have
more impact on a bug than some bulk initialization loop that iterates
millions of times.  Two generalizations of our approach support
non-uniform sampling.

One might replace the single global ``next sample'' countdown with
several countdowns having a variety of means.  Each acyclic region
uses one of these counters for all of its sites.  Thus,
heavily-trafficked code might be sampled at a rate of
\nicefrac{1}{1000}, while moderate-use code uses \nicefrac{1}{100} and
rarely executed initialization and error-handling code uses
\nicefrac{1}{10} or even \nicefrac{1}{1} (i.e.\ no sampling; complete
data collection).

A complementary extension is to allow individual sites to decrement
the ``next sample'' countdown more than once.  Decrementing once is
equivalent to tossing a biased coin once and only taking a sample if
that toss comes up (e.g.) heads.  Decrementing twice simulates taking
a sample if either of two tosses comes up heads.  For a basic
underlying countdown with mean $d$, decrementing $n$ times yields an
effective sampling rate of $(1 - (1 - \frac{1}{d})^n)$.  For purposes
of checking countdown thresholds at the tops of acyclic regions, a
site that decrements $n$ times is equivalent to $n$ sites that
decrement once.  Thus each global ``next sample'' countdown actually
supports a family of sampling rates selectable on a per-site basis.

\placeholder{The above assumes that we have already given a refresher
  on global countdowns and threshold checks earlier in the paper.
  Rewording will be needed if we don't get into that amount of
  detail.}

\placeholder{Yet to be addressed: how you actually decide what rate to
  use for a given site.  Presumably some mix of ad-hoc guessing,
  coverage information from in-house testing, and dynamic adjustment
  in the field based on what you're seeing in the feedback reports
  you've gotten so far.}

\placeholder{Yet to be addressed: need for better statistical models.
  This is a bit vague, but should at least discuss problems we
  encountered using logistic regression in the Moss experiment.}

The sampling instrumentor uses several program analyses to boost the
performance of instrumented code.  However, the post-run data analysis
described in \Autoref{sec:algorithm} is largely ignorant of program
structure.  Exploiting properties visible in the source code can allow
further filtering of suspect predicates.  We expect that in any given
failed execution, there are two points of greatest interest to an
engineer: the place where things first started to go wrong, and the
place where an immediate failure became truly inevitable.  Thus,
predicates with high crash scores \placeholder{Is ``crash score'' the
  right term here?  Check against \Autoref{sec:algorithm}.} are most
interesting when they appear close to program entry or close to the
location of an actual crash.

This suggests an approach based on traversal of the control flow
graph.  To find earliest causes, search forward from program entry for
points at which the chance of failure substantially increases.  To
find latest causes, search backward from crash locations for points at
which the chance of failure substantially decreases.  Predicate
observations provide additional clues, and could be incorporated to
further restrict the search to only those paths which are consistent
with observed behavior.  \placeholder{The preceding feels too detailed
  for an algorithm we haven't actually implemented.  I'd rather
  provide sketches of more distinct ideas rather than presenting just
  one idea in this much detail.}

\section{Related Work}
\label{sec:related-work}

\placeholder{Page budget: 1.}

The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more probabilistic approach: we wish to identify broad trends over
time that correlate predicate violations with increased likelihood of
failure.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  We observe, however, that the
predicates injected by our instrumentor can approximate coverage: over
many runs, the sum of all predicate counters at a site converges on
the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses a mix of program slicing and
Bayesian belief networks to filter and rank the possible causes for a
given bug.  Empirical evaluation show that the slicing component alone
finds 65\% of bug causes, while the probabilistic model correctly
identifies another 10\%.  This additional payoff may seem small,
especially in light of the effort, measured in multiple man-years,
required to distill experts' often tacit knowledge into a formal
belief network.  However, the approach does illustrate one strategy
for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al.\ \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but may be impractical to
deploy outside of a controlled testing environment.  As in our own
earlier work, Podgurski uses logistic regression to select features
which are highly predictive of failure.  \placeholder{Is it worth
  noting that we use different strategies for limiting the size of the
  set of selected features?  We use regularized logistic regression
  whereas Podgurski applies standard logistic regression to randomly
  selected subsets of the complete feature set and keeps the
  best-performing subset.}  Clustering tends to identify small, tight
groups of runs which do share a single cause but which are not always
maximal.  (That is, one cause may be split across several clusters.)

Studies that attempt real-world deployment of monitored software must
address a host of practical engineering concerns, from distribution to
installation to user support to data collection and warehousing.
Elbaum and Hardojo \cite{Elbaum:2003:DISATA} have reported on a
limited deployment of instrumented Pine binaries.  Their experiences
have helped to guide our own design of a wide public deployment of
applications with sampled instrumentation, presently underway
\cite{Liblit:2003:CBIP}.

\placeholder{More recent work of interest:

  \begin{itemize}
  \item S.\ Elbaum, S.\ Kanduri and A.\ Andrews, ``Anomalies as
    Precursors of Field Failures, International Symposium of Software
    Reliability Engineering'', IEEE, November 2003.
    
  \item S.\ Elbaum and M.\ Hardojo, ``An Empirical Study on Profiling
    Strategies for Released Software and Their Application to QA
    Activities'', Technical Report TR03-09-01, University of Nebraska
    - Lincoln, September 2003.
  \end{itemize}
  
  Cannot find either online, but have written to Elbaum asking him for
  copies.}

For some highly available systems, even a single failure must be
avoided.  Once the behaviors that predict imminent failure are known,
automatic corrective measures may be able to prevent the failure from
occurring at all.  The Software Dependability Framework (SDF)
\cite{Gross:2003:PSMUST} uses the multivariate state estimation
technique to model and thereby predict impending system failures.
Instrumentation is assumed to be complete and is typically
domain-specific, whereas our sampled predicates cast a wider, less
specialized net.  \placeholder{We understand through informal
  communication that the SDF is able to anticipate when a player is
  about to lose an instrumented game of Tetris, and can intervene by
  removing rows to allow the game to continue.  But maybe I shouldn't
  say that, as it kind of makes their system sound like a joke.}

\placeholder{Cannot find any more recent work by these people in this
  area.  Where did they all go?  Porter has plenty of other recent
  work, but apparently nothing related.  Gross and McMaster have zero
  publication information on their home pages, while Umranov and Votta
  seem to have vanished entirely.  Have written to Gross and Porter
  asking if they have anything more recent I should look at.}

\section{Conclusions}
\label{sec:conclusions}

\bibliography{cacm1990,icse02,icse03,misc,paste02,pldi03,pods,ramss,refs}
\placeholder{Page budget for references: 1.}

\end{document}

%% LocalWords:  DIDUCE Burnell Horvitz Podgurski Elbaum Hardojo SDF
%% LocalWords:  cacm icse ramss
