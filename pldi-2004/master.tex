%% -*- TeX-master: t -*-

%% \documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% standard texmf packages
%%

\usepackage{amsmath}
\usepackage{ifthen}
\usepackage{nicefrac}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{xspace}

\usepackage[bookmarks, pdftitle={Bug Isolation in the Presence of
  Multiple Errors}, pdfauthor={Ben Liblit, Mayur Naik, Alice X.
  Zheng, Alex Aiken, and Michael I.  Jordan}, pdfsubject={D.2.4
  [Software Engineering]: Software/Program Verification -- statistical
  methods; D.2.5 [Software Engineering]: Testing and Debugging --
  debugging aids, distributed debugging, monitors, tracing; I.5.2
  [Pattern Recognition]: Design Methodology -- feature evaluation and
  selection}, pdfkeywords={bug isolation, random sampling, invariants,
  feature selection, statistical debugging},
pdfpagemode=UseOutlines]{hyperref}

\ifthenelse{\isundefined{\pdfoutput}}{}{\usepackage{thumbpdf}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% unique to this paper
%%

\usepackage{Autoref}
\newcommand{\subfigureautorefname}[0]{Figure}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}
\newcommand{\prob}{\mbox{\textit{Prob}}}
\newcommand{\fail}{\mbox{\textit{Fail}}}
\newcommand{\crash}{\mbox{\textit{Crash}}}
\newcommand{\context}{\mbox{\textit{Context}}}
\newcommand{\increase}{\mbox{\textit{Increase}}}
\renewcommand{\H}{{\mathcal{H}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% front matter
%%

\title{Bug Isolation in the Presence of Multiple Errors
  %%
  \thanks{This research was supported in part by NASA Grant No.\
    NAG2-1210; NSF Grant Nos.\ EIA-9802069, CCR-0085949, ACI-9619020,
    and IIS-9988642; DOE Prime Contract No.\ W-7405-ENG-48 through
    Memorandum Agreement No.\ B504962 with LLNL; and ONR MURI
    N00014-00-1-0637.  The information
    presented here does not necessarily reflect the position or the
    policy of the Government and no official endorsement should be
    inferred.}}

\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X.\ Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I.\ Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  document body
%%

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
  Software errors are a fact of life, with complex programs exhibiting
  multiple bugs even within a single run.  Debugging can be difficult
  when neither the causes nor even the number of distinct errors are
  known.  We present a multiple-bug isolation algorithm that operates
  on sparsely sampled data drawn from large numbers of user runs.  By
  identifying those program behaviors which significantly increase the
  likelihood of failure, our technique helps guide software engineers
  to the most significant flaws in an application.  The approach has
  connections to statistical hypothesis testing, and is validated
  using a case study of a complex application.
\end{abstract}

\category{D.2.4}{Software Engineering}{Software/Program
  Verification}[statistical methods]
%%
\category{D.2.5}{Software Engineering}{Testing and
  Debugging}[debugging aids, distributed debugging, monitors, tracing]
%%
\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
  evaluation and selection]

\terms{Experimentation, Reliability}

\keywords{bug isolation, random sampling, invariants, feature
  selection, statistical debugging}


\section{Introduction}
\label{sec:introduction}

Programs are buggy.
%We all know this, and yet we use them anyway.
Most software ships with many known and unknown bugs; software
engineers understand that it is impractical to delay releasing code
until the last bug has been fixed.

Our vision is to use \termdef{statistical debugging} to
improve software quality.  Specially
instrumented programs monitor their own behavior and send feedback
reports to a central collection server.  Monitoring is both sparse and
random, so complete information is never available about
any single run.  However, monitoring is also lightweight and therefore
practical to deploy to user communities numbering in the thousands or
millions.  Statistical debugging does not seek to understand the
single cause of a single failure on one machine; rather, it identifies
broad statistical trends that isolate bug causes across many thousands
or millions of runs.  Because the process is driven by data from real
users, it implicitly attends to those program behaviors that cause the
most problems for the most users, most often.

In our previous work we examined the problem of automatically
isolating the cause of a single bug
\cite{PLDI`03*141,Zheng:2003:SDSP}.  As discussed above, however, no
realistic application has only a single bug, or even a handful of
bugs.  Consider the fact that the Mozilla web browser project has a
``topcrash'' software QA team specifically dedicated to identifying
and tracking the top forty most common crash bugs.  This cut-off is
not because anyone believes that there are only forty bugs; it is an
acknowledgment that engineering resources are finite so the most
important bugs should be fixed first.

This paper is about simultaneously isolating multiple bugs in complex
applications.  The main contributions are:

\begin{itemize}

\item We give a new algorithm for isolating multiple bugs
(\Autoref{sec:algorithm}).  Our algorithm is very efficient and works well in the
presence of multiple bugs, sparse sampling, and non-deterministic failure
modes.

\item We present a significant experimental study showing the
effectiveness of our algorithm on a complex application
(Sections~\ref{sec:experiments:setup}
and~\ref{sec:experiments:results}).  We seeded a program with nine
bugs, most
of which were taken directly from the application's bug log.  Our
system correctly identified a distinct cause for seven of the bugs; the
other two bugs never caused the program to fail in our experiment.
Our algorithm also discovered a new, previously unknown bug in the
application.

\end{itemize}

After providing some background on our previous work in single-bug
isolation (\Autoref{sec:background}), we discuss the algorithm (\Autoref{sec:algorithm}),
the design of our experiment (\Autoref{sec:experiments:setup}), and
the experimental results (\Autoref{sec:experiments:results}).
The paper winds up with a discussion of related work (\Autoref{sec:related-work})
and future work and conclusions (\Autoref{sec:conclusions}).

\section{Background}
\label{sec:background}

This section summarizes our previous work in isolating single bugs
\cite{PLDI`03*141,Zheng:2003:SDSP}, which is the starting point for
this work.  The ideal program monitoring system would gather complete
execution traces and provide them to an engineer (or, more likely, a
tool) to mine for the causes of bugs.  However, complete tracing of
program behavior is simply impractical; no end user would accept the
required performance overhead or network bandwidth.

Instead, we use a combination of sparse random sampling, which controls
performance overhead, and client-side summarization of the data, which
limits the storage and transmission costs.  We briefly discuss
both aspects.

Random sampling is added to a program via a source-to-source transformation.
Our sampling transformation is general: any collection of
statements within (or added to) a program may be designated as
``instrumentation'' and thereby sampled instead of run
unconditionally.  That is, each time instrumentation code is reached,
a coin flip decides whether the instrumentation is executed or not.
Coin flipping is simulated in a statistically fair
manner equivalent to a Bernoulli process: each potential sample is
taken or skipped randomly and independently as the program runs.
We have found that a sampling rate of \nicefrac{1}{100} keeps the performance overhead
of instrumentation low, often unmeasurable.

Orthogonal to the sampling transformation is the decision about what
instrumentation to introduce and how to concisely
summarize the resultant data.  This determines what behaviors one can
possibly observe once the sampling transformation is applied.  A useful instrumentation
scheme should be fairly general but selected to capture behaviors
which are likely to be of interest when hunting for bugs.  At present
our system offers the following instrumentation schemes:

\begin{description}
\sloppy
\item[branches:] At each conditional,
  count how often each branch is taken.  The observation is
  made just after the predicate is evaluated but before the selected
  branch is taken.  This scheme also applies to implicit conditionals
  in loops and logical operators (\texttt{\&\&}, \texttt{||},
  \texttt{?:}).  Each conditional induces one instrumentation site
  with two counters: number of times branched true and number of times
  branched false.

\item[returns:] In C, the
  sign of a return value is often used to encode an operation's success or failure.
  At each scalar-returning function call site, count how
  often the returned value is negative, zero, or positive.  For
  pointer-returning calls, this reduces to counting
  \texttt{NULL} versus non-\texttt{NULL}.  The observation is made
  just after the function returns but before the result is used by the
  original program.  An instrumentation site is added even if the
  source program discards the return value, as unchecked return
  values are a common source of bugs.  Each call site induces one
  instrumentation site with three counters: number of negative
  returns, number of zero returns, and number of positive returns.

\item[scalar-pairs:] Many bugs
  concern boundary issues exposed by the relationship between a pair
  of variables, or between a variable and some program constant.  At
  each scalar assignment \texttt{x = \dots}, identify each
  \emph{other} same-typed in-scope variable $\mathtt{y}_i$ and each
  constant expression $\mathtt{c}_j$.  Count how often the new value
  for \texttt{x} is less than, equal to, or greater than each
  $\mathtt{y}_i$ and each $\mathtt{c}_j$.  The observation is made
  after both sides of the assignment have been evaluated but
  before the assignment takes place.  This lets us compare
  \texttt{x} to \texttt{x} as well, effectively comparing the new and
  old values of the left-hand side.  Each compared-to $\mathtt{y}_i$
  or $\mathtt{c}_j$ is treated as a distinct instrumentation site;
  thus a single assignment may induce a large number of sites.  Each
  such site maintains three counters: how often the value being
  assigned is less than, equal to, or greater than the compared-to
  variable or constant.
\end{description}

The common theme among these instrumentation schemes is that we
instrument program predicates and count the number of times that these
predicates are observed to be true or false when a sample is taken.
For the returns and scalar-pairs instrumentation schemes, the three
predicates that are tracked explicitly actually give rise to a family
of six predicates that we use in our analysis of the data.
For example, for each scalar-pairs comparison between \texttt{x} and \texttt{y}, one and
only one of the three predicates $\mathtt{x} < \mathtt{y}$, $\mathtt{x} = \mathtt{y}$, or
$\mathtt{x} > \mathtt{y}$ can be true.  Thus one observation at one
site always updates exactly one of that site's counters.  An
observation that $\mathtt{x} < \mathtt{y}$ is true is therefore equivalent to
an observation that $\mathtt{x} \geq \mathtt{y}$ is false.  Similarly, we can
infer from the data the two remaining predicates $\mathtt{x} \leq \mathtt{y}$ or
$\mathtt{x} \neq \mathtt{y}$.

Also note that because some counter is always incremented at every
sample in all three schemes, we can easily determine the number of
times a site was sampled by summing all the counters for the site
(two counters for the branches instrumentation and three counters for
the others).  Thus, we can distinguish between predicates that are
never observed during execution (all the counters at the site are 0)
and predicates that are observed but never true (the counter(s) for the predicate
is 0, but some other counter at the site is positive).

The post-execution feedback report consists of the final counter values for
each instrumentation site.  Reducing a trace to a set of counters
prevents us from reasoning about relative time ordering of events
during execution.  However, it also means that program actions early
in execution remain just as visible as those much later.  This
contrasts with traditional postmortem debugging tools which expose
only the final state of the program at the point of failure.

\section{Cause Isolation Algorithm}
\label{sec:algorithm}
\input{alg}

\section{Experiment Design}
\label{sec:experiments:setup}
\input{setup}

\section{Experimental Results}
\label{sec:experiments:results}
\input{experiments}


\section{Related Work}
\label{sec:related-work}

In this section we briefly survey related work.
The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more statistical approach.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  We observe, however, that the
predicates injected by our instrumentor can approximate coverage: over
many runs, the sum of all predicate counters at a site converges on
the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses program slicing in conjunction
with Bayesian belief networks to filter and rank the possible causes
for a given bug.  Empirical evaluation shows that the slicing component
alone finds 65\% of bug causes, while the probabilistic model
correctly identifies another 10\%.  This additional payoff may seem
small in light of the effort, measured in multiple
man-years, required to distill experts' often tacit knowledge into a
formal belief network.  However, the approach does illustrate one
strategy for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al.\ \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but greatly restricts the
instrumentation schemes that are practical to deploy outside of a
controlled testing environment.  As in our own earlier work, Podgurski
uses logistic regression to select features which are highly
predictive of failure.  
Clustering tends to identify small, tight groups of runs which do
share a single cause but which are not always maximal.  That is, one
cause may be split across several clusters.

In contrast, current
industrial practice uses stack traces to cluster failure reports into
equivalence classes.  Two crash reports showing the same stack trace,
or perhaps only the same top-of-stack function, are presumed to be two
reports of the same failure.  This works to the extent that a single
cause corresponds to a single point of failure, but our experience
with \moss suggests that this assumption may not often hold.  We find
that only bugs \#2 and \#5 have truly unique ``signature'' stacks: a
crash location which is present if and only if the corresponding bug
was actually triggered.  These bugs are also our most deterministic.
Bugs \#4 and \#6 have nearly unique stack signatures, modulo small
changes to frames several levels removed from the point of failure.
The remaining bugs are much less consistent: each stack signature is
observed after a variety of different bugs, and each triggered bug
causes failure in a variety of different stack states.  Stack-based
clustering provides little insight for temporally extended bugs that
do not crash until well after the bad behavior.  A special case of
this is a program which produces incorrect output but exits normally
without crashing and therefore without any crash stack on which to
cluster.

Studies that attempt real-world deployment of monitored software must
address a host of practical engineering concerns, from distribution to
installation to user support to data collection and warehousing.
Elbaum and Hardojo \cite{Elbaum:2003:DISATA} have reported on a
limited deployment of instrumented Pine binaries.  Their experiences
have helped to guide our own design of a wide public deployment of
applications with sampled instrumentation, presently underway
\cite{Liblit:2003:CBIP}.

For some highly available systems, even a single failure must be
avoided.  Once the behaviors that predict imminent failure are known,
automatic corrective measures may be able to prevent the failure from
occurring at all.  The Software Dependability Framework (SDF)
\cite{Gross:2003:PSMUST} uses the multivariate state estimation
technique to model and thereby predict impending system failures.
Instrumentation is assumed to be complete and is typically
domain-specific, whereas our sampled predicates cast a wider, less
specialized net.

\section{Conclusions and Future Work}
\label{sec:conclusions}

We have demonstrated a practical algorithm for isolating multiple bugs
in complex software systems.  Given feedback profiles of enough runs,
overall trends emerge which can help to guide an engineer to the most
likely causes of the most common bugs.  Our approach uses lightweight,
sampled instrumentation suitable for wide scale deployment to real end
users, which means that the system also performs implicit triage: it
learns the most, most quickly, about the bugs that happen most often.
The key property of our approach is that it filters potential causes
based on the degree to which they increase the likelihood of failure.
Our algorithm appears to do a good job of isolating a wide variety of
bugs, even when multiple bugs are present simultaneously.

When hunting for bugs, the first thing an engineer wants to know is
under what circumstances the failure occurs.  Perhaps the most salient
feature of our approach is the ability to pinpoint the circumstances
under which bugs occur.

We also see several possible avenues for improvement, which we leave
as future work:
\begin{itemize}

\item The scalar-pairs instrumentation scheme induces many
predicates and finds fewer bugs than the branches and
returns instrumentation schemes.  A more specialized version of scalar-pairs
would be useful.

\item To date, we have sampled all sites at the same rate.  However,
rarely executed code, such as code that executes once on system start-up,
can be sampled at a higher rate with no impact on overall performance.
Moreover, infrequently executed code is more likely
to harbor bugs.  By observing rare events more often, we would need fewer
runs to isolate bugs.

\item The algorithm we present here analyzes every predicate independently
of every other predicate.  While we have not yet found a statistical algorithm that exploits
correlations between predicates effectively in the presence of
multiple bugs, we believe this is a worthwhile direction
to explore.
\end{itemize}


\bibliography{cacm1990,icse02,icse03,misc,paste02,pldi03,pods,ramss,refs}

\end{document}

%% LocalWords:  DIDUCE Burnell Horvitz Podgurski Elbaum Hardojo SDF
%% LocalWords:  topcrash cacm icse ramss pldi Podgurski's Kanduri
%% LocalWords:  McMaster Umranov Votta
