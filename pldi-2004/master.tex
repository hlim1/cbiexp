%% -*- TeX-master: t -*-

\documentclass{acm_proc_article-sp}
%% \documentclass{sig-alternate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% standard texmf packages
%%

\usepackage{nicefrac}
\usepackage{fancyvrb}
\usepackage{xspace}

\usepackage[bookmarks,pdfauthor={Ben Liblit, Mayur Naik, Alice X.
  Zheng, Alex Aiken, and Michael I.
  Jordan},pdfpagemode=UseOutlines]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% unique to this paper
%%

\usepackage{Autoref}

%% assorted handy macros
\newcommand{\moss}{\textsc{Moss}\xspace}
\newcommand{\termdef}[1]{\textit{#1}}
\newcommand{\prob}{\mbox{\textit{Prob}}}
\newcommand{\fail}{\mbox{\textit{Fail}}}
\newcommand{\crash}{\mbox{\textit{Crash}}}
\newcommand{\context}{\mbox{\textit{Context}}}
\newcommand{\increase}{\mbox{\textit{Increase}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% remove the following before submission!
%%

\usepackage{color}
\newcommand{\placeholder}[1]{{\color[cmyk]{0,0.61,0.87,0}[#1]}}
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% front matter
%%

\title{Statistical Debugging in the Presence of Multiple Bugs
  %%
  \thanks{\placeholder{This grant boilerplate is from the PLDI'03
      paper.  Newer text is needed here.}  This research was supported
    in part by NASA Grant No.\ NAG2-1210; NSF Grant Nos.\ EIA-9802069,
    CCR-0085949, ACI-9619020, and IIS-9988642; DOE Prime Contract No.\ 
    W-7405-ENG-48 through Memorandum Agreement No.\ B504962 with LLNL;
    and a Lucent GRPW Fellowship.  The information presented here does
    not necessarily reflect the position or the policy of the
    Government and no official endorsement should be inferred.}}

\numberofauthors{3}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\moreauthors}[0]{\end{tabular}\\\vspace{-.5\baselineskip}\begin{tabular}{c}}

\author{
  \alignauthor Ben Liblit \eecs \\
  \alignauthor Mayur Naik \stan \\
  \alignauthor Alice X.\ Zheng \eecs \\
  \moreauthors
  \global\multiply\auwidth by 3
  \global\divide\auwidth by 2
  \alignauthor Alex Aiken \stan \\
  \alignauthor Michael I.\ Jordan \both
  \moreauthors
  \alignauthor
  \affaddr{\eecs Department of Electrical \\ Engineering and Computer Science} \\
  \affaddr{\stat Department of Statistics} \\
  \affaddr{University of California, Berkeley} \\
  \affaddr{Berkeley, CA 94720-1776}
  \alignauthor
  \affaddr{\stan Computer Science Department} \\
  \affaddr{353 Serra Mall} \\
  \affaddr{Stanford University} \\
  \affaddr{Stanford CA 94305-9025}
}

\bibliographystyle{abbrv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  document body
%%

\begin{document}

\conferenceinfo{PLDI'04,}{June 9--11, 2004, Washington, DC, USA.}
\CopyrightYear{2004}
%% \crdata{}
\maketitle

\begin{abstract}
\placeholder{Abstract needed here.}
\end{abstract}

\category{D.2.5}{Software Engineering}{Testing and
  Debugging}[distributed debugging]
%%
\category{G.3}{Mathematics of Computing}{Probability and
  Statistics}[correlation and regression analysis]
%%
\category{I.5.2}{Pattern Recognition}{Design Methodology}[feature
  evaluation and selection]

\terms{Experimentation, Performance, Reliability}

\keywords{bug isolation, random sampling, assertions, feature
  selection, statistical debugging, logistic regression}

\placeholder{The preceding categories, terms, and keywords were copied
  from the PLDI'03 paper.  Some may need to be changed.  Note that the
  categories and terms are \emph{not} free-form; see
  \url{http://www.acm.org/sigs/pubs/proceed/sigguide-v2.1sp.htm}
  sections 2.3.2 and 2.3.3 for details.}


\section{Introduction}
\label{sec:introduction}

\placeholder{Page budget: 1.}


\section{Background}
\label{sec:background}

\placeholder{Page budget: 1.}

We begin by reviewing the sampled instrumentation infrastructure.  We
highlight the ways in which the design constraints discussed earlier
\placeholder{reword if we don't do that in the introduction} impact
the kind and form of data collected.  The challenge taken up in
\Autoref{sec:algorithm} is to use this data in a way that helps
isolate problems in buggy software.

Software errors arise from unexpected interactions between inputs,
program internal state, and dynamic decisions made as the program
runs.  Software engineers reason about programs at the level of source
code statements and variables, and this is the level at which program
monitoring can expose properties of interest.  However, complete
source-level tracing of program behavior is impractical for field
deployment.  No end user would accept the performance overhead or
network bandwidth required to collect and transmit such a trace.

Instead, we use a combination of client-side summarization and fair
random sampling.  The former controls limits storage and transmission
requirements while the later controls performance overhead.  However,
each also adds noise and uncertainty to the resulting data.  We
briefly review these two techniques and their implications here;
additional details including performance assessments have been
published elsewhere \cite{PLDI`03*141}.  Although our approach is
applicable to a wide variety of programming languages, our current
implementation and all examples that follow are targeted at C.

\subsection{Schemes and Summarization}

An \termdef{instrumentation scheme} is a set of rules describing what
information to collect at each program point of interest.  Our
instrumentation schemes are quite general but are selected to capture
behaviors which are likely to be of interest when hunting for bugs.
At present our system offers the following instrumentation schemes:
  
\begin{description}
\item[branches] Control flow is interesting.  Record which branch is
  taken at each conditional (equivalently, whether the conditional
  predicate was true or false).  The observation is made just after
  the predicate is evaluated but before the selected branch is taken.
  This scheme also applies to implicit conditionals in loops and
  logical operators (\texttt{\&\&}, \texttt{||}, \texttt{?:}).  Each
  branch induces one instrumentation site.
  
\item[returns] Function return values are interesting.  In C, the sign
  of a result often used to encode the success or failure of some
  operation.  At each scalar-returning function call site, record
  whether the returned value is negative, zero, or positive.  For
  pointer-returning calls, this implicitly reduces to \texttt{NULL} or
  non-\texttt{NULL}.  The observation is made just after the function
  returns but before the result is used by the original program.  An
  instrumentation site is added even if the source program discarded
  the return value, as ignoring returned error codes is a common
  source of bugs..  Each call induces one instrumentation site.
  
\item[scalar-pairs] Values of variables are interesting.  Many bugs
  concern boundary issues exposed by the relationship between a pair
  of variables, or between a variable and some program constant.  At
  each scalar assignment \texttt{x = \dots}, identify each
  \emph{other} same-typed in-scope variable $\mathtt{y}_i$ and each
  constant expression $\mathtt{c}_j$.  Record how often the new value
  for \texttt{x} is less than, equal to, or greater than each
  $\mathtt{y}_i$ and each $\mathtt{c}_j$.  The observation is made
  after both sides of the assignment have been evaluated but just
  before the assignment itself takes place.  This lets us compare
  \texttt{x} to \texttt{x} as well, effectively comparing the new and
  old values of the left-hand side.  Each compared-to $\mathtt{y}_i$
  or $\mathtt{c}_j$ is treated as a distinct instrumentation site;
  thus a single assignment may induce a large number of sites.
\end{description}

These schemes are quite broad; they represent a large set of wild
guesses as to what behavior may be interesting for any particular bug.
Engineers may enable or disable any mix of schemes in a single binary,
and may include or exclude entire regions of code on a per-file or
per-function basis.  For the most part, though, instrumenting an
executable simply requires switching to our instrumenting compiler.
By design the instrumentor requires minimal human intervention at
application build time.

In the returns and scalar-pairs schemes, each site takes a large space
of possible observations (e.g.\ the exact value returned by a call)
and reduces it to a much smaller group of equivalence classes (e.g.\ 
the sign of the returned value).  This is already a summarization of
behavior.  However, we must shrink the program profile even further,
down to a flat behavior profile whose size is both moderate and fixed
regardless of how long a given application runs.  Therefore, we do not
emit a stream of observations from each site.  Rather, each site
maintains small group of counters which tally how often each behavior
is observed.  Thus, each branch site has two counters: how often the
program branched true and how often it branched false.  Each returns
site uses three counters to record how often a negative, zero, or
positive value is seen.  Each scalar-pairs site uses three counters as
well to record how often the assigned value is less than, equal to, or
greater than the compared-to variable or constant.

These counters are maintained within the program's global address
space during execution.  Note that each site can be treated as a set
of predicates which form a complete, non-overlapping partition of the
behaviors observed by that site.  For example, at each scalar-pairs
comparison between \texttt{x} and \texttt{y}, one and only one of
$\mathtt{x} < \mathtt{y}$, $\mathtt{x} = \mathtt{y}$, or $\mathtt{x} >
\mathtt{y}$ can be true.  Thus one observation at one site always
updates exactly one of that site's counters.  An observation that
$\mathtt{x} < \mathtt{y}$ is therefore equivalent to an observation
that $\mathtt{x} \ge \mathtt{y}$.  This property will be used in
\Autoref{sec:algorithm} to synthesize new predicates out of existing
observations.

The post-execution feedback report is a dump of the counter values for
each instrumentation site.  Reducing a trace to a set of counters
prevents us from reasoning about relative time ordering of events
during execution.  However, it also means that program actions early
in execution remain just as visible as those much later.  This is in
stark contrast with traditional postmortem debugging tools which
expose only the final state of the program at the point of failure.

\subsection{Sparse Random Sampling}

Each individual instrumentation site is fast, but in large numbers
they will harm performance.  Controlling this overhead requires that
we skip some observations.  If sampling is sparse, then the common
case of skipping a sample can be optimized as a ``fast path'' For such
partial instrumentation to be statistically meaningful, it must be
fair in a strict sense: each site when crossed at run time must have
an identical and independent chance of being sampled or omitted
regardless of how that same decision was made for any other dynamic
crossing of any other site.

The requirement suggests tossing a biased coin at each site.  However,
generating these biased random bits would be slower than simply
performing all observations unconditionally.  We must amortize the
decision cost across larger numbers of sites.  Consider a sequence of
biased random bits with 0 (skip) much more common than 1 (sample).
Instead of storing the sequence of bits directly, record only how many
0's appear before the next 1: how many sites are to be skipped before
the next sample is taken.  This is the \termdef{inter-arrival time} of
1's, and can be computed directly by selecting random numbers from a
geometric distribution.  A geometric distribution with mean 100 gives
inter-arrival times for a stream where samples are taken on average
once per hundred opportunities.

Instrumented code uses a global ``next sample'' countdown to predict
upcoming observations.  At each site, we decrement the countdown by
one.  If that drives the countdown to zero, then we take an
observation and reset the countdown to a new geometrically distributed
random value.  This counter has a useful near-term predictive quality:
if the counter is 58, then we know that none of the next 58 sites
crossed will be sampled.  This fact lets us amortize the sample/skip
decision across larger regions of code.  Consider any program point at
the top of an acyclic region.  There are only a finite number of
forward paths within that region, and therefore a finite maximum
number of sites along those paths.  We may know, for example, that no
forward path through a given region crosses more than 24 sites.  If
the next sample is more than 24 sites away, then we know in advance
that no sample will be taken on this pass through the region.  We
instead branch off into an alternate copy of the code which contains
no instrumentation beyond simply updating the global next sample
countdown.  This maximum site count becomes the \termdef{threshold}
for that region.

\placeholder{\dots}

This is an explicit trade-off between overhead and noise.  Sparser
sampling yields lower overhead but more noise (uncertainty) in each
feedback report.  We compensate for this uncertainty by looking for
broad trends across many runs.  With thousands or millions of users,
any individual feedback report may be noisy but the behavior of the
system as a whole will converge on an accurate representation of real
program behavior.

\section{Algorithm}
\label{sec:algorithm}

\input{alg}

\placeholder{Page budget: 2.}


\section{\placeholder{Experiments}}
\label{sec:experiments}

\subsection{\placeholder{Setup}}
\label{sec:experiments:setup}

\placeholder{Page budget: 1.}

\input{setup}

\subsection{\placeholder{Results}}
\label{sec:experiments:results}

\placeholder{Page budget: 2, including pictures.}


\section{\placeholder{Future Work / Alternate Approaches}}
\label{sec:future-work}

\placeholder{Page budget: \nicefrac{1}{2}.}

Our experience with \moss suggests several key areas for future
development.

In the experiments reported here, sampling is dynamically uniform:
each crossing of each instrumentation site has the same fixed chance
of being sampled or skipped.  However not all code is equally
interesting.  The details of a one-time configuration action may have
more impact on a bug than some bulk initialization loop that iterates
millions of times.  Two generalizations of our approach support
non-uniform sampling.

One might replace the single global ``next sample'' countdown with
several countdowns having a variety of means.  Each acyclic region
uses one of these counters for all of its sites.  Thus,
heavily-trafficked code might be sampled at a rate of
\nicefrac{1}{1000}, while moderate-use code uses \nicefrac{1}{100} and
rarely executed initialization and error-handling code uses
\nicefrac{1}{10} or even \nicefrac{1}{1} (i.e.\ no sampling; complete
data collection).

A complementary extension is to allow individual sites to decrement
the ``next sample'' countdown more than once.  Decrementing once is
equivalent to tossing a biased coin once and only taking a sample if
that toss comes up (e.g.) heads.  Decrementing twice simulates taking
a sample if either of two tosses comes up heads.  For a basic
underlying countdown with mean $d$, decrementing $n$ times yields an
effective sampling rate of $(1 - (1 - \frac{1}{d})^n)$.  For purposes
of checking countdown thresholds at the tops of acyclic regions, a
site that decrements $n$ times is equivalent to $n$ sites that
decrement once.  Thus each global ``next sample'' countdown actually
supports a family of sampling rates selectable on a per-site basis.

\placeholder{The above assumes that we have already given a refresher
  on global countdowns and threshold checks earlier in the paper.
  Rewording will be needed if we don't get into that amount of
  detail.}

\placeholder{Yet to be addressed: how you actually decide what rate to
  use for a given site.  Presumably some mix of ad-hoc guessing,
  coverage information from in-house testing, and dynamic adjustment
  in the field based on what you're seeing in the feedback reports
  you've gotten so far.}

\placeholder{Yet to be addressed: need for better statistical models.
  This is a bit vague, but should at least discuss problems we
  encountered using logistic regression in the Moss experiment.}

The sampling instrumentor uses several program analyses to boost the
performance of instrumented code.  However, the post-run data analysis
described in \Autoref{sec:algorithm} is largely ignorant of program
structure.  Exploiting properties visible in the source code can allow
further filtering of suspect predicates.  We expect that in any given
failed execution, there are two points of greatest interest to an
engineer: the place where things first started to go wrong, and the
place where an immediate failure became truly inevitable.  Thus,
predicates with high crash scores \placeholder{Is ``crash score'' the
  right term here?  Check against \Autoref{sec:algorithm}.} are most
interesting when they appear close to program entry or close to the
location of an actual crash.

This suggests an approach based on traversal of the control flow
graph.  To find earliest causes, search forward from program entry for
points at which the chance of failure substantially increases.  To
find latest causes, search backward from crash locations for points at
which the chance of failure substantially decreases.  Predicate
observations provide additional clues, and could be incorporated to
further restrict the search to only those paths which are consistent
with observed behavior.  \placeholder{The preceding feels too detailed
  for an algorithm we haven't actually implemented.  I'd rather
  provide sketches of more distinct ideas rather than presenting just
  one idea in this much detail.}

\section{Related Work}
\label{sec:related-work}

\placeholder{Page budget: 1.}

The Daikon project \cite{ernst2001} monitors instrumented applications
to discover likely program invariants.  It collects extensive trace
information at run time and uses this offline to accept or reject any
of a wide variety of guessed candidate predicates.  The DIDUCE project
\cite{ICSE02*291} tests a more restricted set of predicates within the
client program, and attempts to relate state changes in candidate
predicates to manifestation of bugs.  Both projects assume complete
monitoring, such as within a controlled test environment.  Our goal is
to use lightweight partial monitoring, suitable for deployment to end
users.  We never have complete information, and therefore must use a
more probabilistic approach: we wish to identify broad trends over
time that correlate predicate violations with increased likelihood of
failure.

\termdef{Software tomography} as realized through the GAMMA system
\cite{PASTE'02*2,Orso:2003:LFDIART} shares our goal of low-overhead
distributed monitoring of deployed code.  GAMMA collects code coverage
data to support a variety of code evolution tasks.  Our
instrumentation exposes a broader family of data- and
control-dependent predicates on program behavior and uses randomized
sparse sampling to control overhead.  We observe, however, that the
predicates injected by our instrumentor can approximate coverage: over
many runs, the sum of all predicate counters at a site converges on
the relative coverage of that site.

Efforts to directly apply statistical modeling principles to debugging
have met with mixed results.  Early work in this area by Burnell and
Horvitz \cite{Burnell:1995:SCM} uses a mix of program slicing and
Bayesian belief networks to filter and rank the possible causes for a
given bug.  Empirical evaluation show that the slicing component alone
finds 65\% of bug causes, while the probabilistic model correctly
identifies another 10\%.  This additional payoff may seem small,
especially in light of the effort, measured in multiple man-years,
required to distill experts' often tacit knowledge into a formal
belief network.  However, the approach does illustrate one strategy
for integrating information about program structure into the
statistical modeling process.

In more recent work, Podgurski et al.\ \cite{ICSE`03*465} apply
statistical feature selection, clustering, and multivariate
visualization techniques to the task of classifying software failure
reports.  The intent is to bucket each report into an equivalence
group believed to share the same underlying cause.  Features are
derived offline from fine-grained execution traces without sampling;
this reduces the noise level of the data but may be impractical to
deploy outside of a controlled testing environment.  As in our own
earlier work, Podgurski uses logistic regression to select features
which are highly predictive of failure.  \placeholder{Is it worth
  noting that we use different strategies for limiting the size of the
  set of selected features?  We use regularized logistic regression
  whereas Podgurski applies standard logistic regression to randomly
  selected subsets of the complete feature set and keeps the
  best-performing subset.}  Clustering tends to identify small, tight
groups of runs which do share a single cause but which are not always
maximal.  (That is, one cause may be split across several clusters.)

Studies that attempt real-world deployment of monitored software must
address a host of practical engineering concerns, from distribution to
installation to user support to data collection and warehousing.
Elbaum and Hardojo \cite{Elbaum:2003:DISATA} have reported on a
limited deployment of instrumented Pine binaries.  Their experiences
have helped to guide our own design of a wide public deployment of
applications with sampled instrumentation, presently underway
\cite{Liblit:2003:CBIP}.

\placeholder{More recent work of interest:

  \begin{itemize}
  \item S.\ Elbaum, S.\ Kanduri and A.\ Andrews, ``Anomalies as
    Precursors of Field Failures, International Symposium of Software
    Reliability Engineering'', IEEE, November 2003.
    
  \item S.\ Elbaum and M.\ Hardojo, ``An Empirical Study on Profiling
    Strategies for Released Software and Their Application to QA
    Activities'', Technical Report TR03-09-01, University of Nebraska
    - Lincoln, September 2003.
  \end{itemize}
  
  Cannot find either online, but have written to Elbaum asking him for
  copies.}

For some highly available systems, even a single failure must be
avoided.  Once the behaviors that predict imminent failure are known,
automatic corrective measures may be able to prevent the failure from
occurring at all.  The Software Dependability Framework (SDF)
\cite{Gross:2003:PSMUST} uses the multivariate state estimation
technique to model and thereby predict impending system failures.
Instrumentation is assumed to be complete and is typically
domain-specific, whereas our sampled predicates cast a wider, less
specialized net.  \placeholder{We understand through informal
  communication that the SDF is able to anticipate when a player is
  about to lose an instrumented game of Tetris, and can intervene by
  removing rows to allow the game to continue.  But maybe I shouldn't
  say that, as it kind of makes their system sound like a joke.}

\placeholder{Cannot find any more recent work by these people in this
  area.  Where did they all go?  Porter has plenty of other recent
  work, but apparently nothing related.  Gross and McMaster have zero
  publication information on their home pages, while Umranov and Votta
  seem to have vanished entirely.  Have written to Gross and Porter
  asking if they have anything more recent I should look at.}

\section{Conclusions}
\label{sec:conclusions}

\bibliography{cacm1990,icse02,icse03,misc,paste02,pldi03,pods,ramss,refs}
\placeholder{Page budget for references: 1.}

\end{document}

%% LocalWords:  DIDUCE Burnell Horvitz Podgurski Elbaum Hardojo SDF
%% LocalWords:  cacm icse ramss
