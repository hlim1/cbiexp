%% -*- LaTeX -*-

\input{table}

In this section we present the results of applying the algorithm
described in \autoref{sec:algorithm} in five case
studies.\footnote{More complete data may be browsed interactively at
  \url{http://www.cs.berkeley.edu/~liblit/pldi-2005}.}
\autoref{tab:exps} shows the summary statistics for each of the
experiments.  In each study we ran the programs on about 32,000 random
inputs.  The number of instrumentation sites varies with the size of
the program, as does the number of predicates those instrumentation
sites yield.  Our algorithm is very effective in reducing the number
of predicates the user must examine.  For example, in the case of
\rhythmbox an initial set of 857,384 predicates is reduced to 537 by the $\increase(P) > 0$
test, a reduction of 99.9\%.  The elimination algorithm then yields 15 predicates, a further
reduction of 97\%.  The other case studies show a similar reduction in the number of
predicates by 3-4 orders of magnitude.

The results we discuss are all on sampled data.  Sampling can have
serious negative effects on our algorithm.  Assume $P_1$ and $P_2$ are
equivalent bug predictors and both are sampled at a rate of
$\nicefrac{1}{100}$ and both are reached once per run.  Then even though
$P_1$ and $P_2$ are equivalent, they will be observed in nearly disjoint
sets of runs and treated as close to independent by the elimination
algorithm.

To address this problem, we set the sampling rates of predicates to be
inversely proportional to their frequency of execution.  Based on a
training set of 1,000 executions, we choose a sampling rate for $P$ to
achieve an expected 100 observations of $P$ during program execution,
but never less than $\nicefrac{1}{100}$.  Thus, rarely executed code has a
much higher sampling rate than very frequently executed code; a
similar strategy has been pursued for similar reasons in \cite{chil04}.  We
have validated this approach by comparing the results for each
experiment with results obtained with no sampling at all (i.e., the
sampling rate of all predicates set to 100\%).  The results are
identical except for the \rhythmbox and \moss experiments, where we
judge the differences to be minor: sometimes a different but logically
equivalent predicate is chosen, the ranking of predictors of different
bugs is slightly different, or one or the other version has a few
extra, weak predictors at the tail end of the list.  


\subsection{A Validation Experiment}

To validate our algorithm we first performed an experiment where we
knew the set of bugs in advance.  We added nine bugs to \moss, a
widely used service for detecting plagiarism in software
\cite{Schleimer:2003:WLA}.  Six of these were previously discovered
and repaired bugs in \moss that we reintroduced; the other three were
variations on three of the original bugs, to see if our algorithm could
discriminate between pairs of bugs with very similar behavior but
distinct causes.  The nature of the bugs varies: four
buffer overruns, a null file pointer dereference in certain cases, a
missing end-of-list check in the traversal of a hash table bucket, a missing
out-of-memory check, and a violation of a subtle invariant that must be maintained between two
parts of a complex data structure comprise the eight crashing bugs.  

The last bug, incorrect handling of comments in certain cases, only
causes incorrect output, not a crash.  While the failure can be
detected for the other bugs by noting program crashes (in most
cases---some of the bugs are non-deterministic and do not always
cause crashes), this last bug cannot be detected in such an easy fashion.
For our experiment we also ran a correct version of \moss\ and
compared the output of the two versions, labeling any run where the
output was different as a failure.  The purpose of using this oracle
is to show that, in principle, bugs other than crashing bugs can also
be isolated using our techniques, provided there is some way, whether
by automatic self-checking or human inspection, to recognize failing
runs.

\input{moss-view/table}

\autoref{tab:mossdilute} shows the results of the experiment.  The
predicates listed were selected by the elimination algorithm in the
order shown.  The second column is the initial thermometer for each
predicate,  showing the \context\ and \increase\ scores before
elimination is performed. The fourth column is the {\em effective}
thermometer, showing the \context\ and \increase\ scores for a
predicate $P$ at the time $P$ is selected (i.e., when it is the
top-ranked predicate).  The scores in the first and third columns are
the initial and effective importance (harmonic mean) scores, respectively.

As part of the experiment we separately recorded the exact set of
bugs that occurred in each run.
The table at the far right of \autoref{tab:mossdilute} shows for
each selected predicate $P$ and each bug ${\cal B}$ the number of runs where
bug ${\cal B}$ occurs and $P$ is observed to be true.  Note that while each
predicate has a very strong spike at one bug, indicating it is a
strong predictor of that bug, there are always some runs with other
bugs present.  For example, the top-ranked predicate, which is
overwhelmingly a predictor of bug \#5, also includes some runs where
bugs \#3, \#4, and \#9 occurred.  This situation is not the result of
misclassification of failing runs by our algorithm.  As observed in
\autoref{sec:introduction}, more than one bug may occur in a run;
it simply happens that in some runs bugs \#5 and \#3 both occur (to
pick just one possible combination).

A particularly interesting case of this phenomenon is bug \#7, one of
the buffer overruns.  Bug \#7 is not strongly predicted by any
predicate on the list but in fact occurs in at least a few of the
failing runs of most predicates.  We have examined the runs of bug \#7
in detail and found that the only failing runs bug \#7 occurs in also
have at least one other bug.  That is, even though the bug \#7 overrun
happens (though rarely), it never causes incorrect output or a crash
in any run.  Bug \#8, another overrun, is not even shown because the
overrun is never triggered in our data (its column would be all
0's).\footnote{Bug \#8 was originally found by a code inspection.}
There is no way our algorithm can find causes of bugs that do not
occur, but recall that part of our purpose in sampling user executions
is to get an accurate picture of the most important bugs. It is
consistent with this goal that if a bug never causes a problem, it is
not only not worth fixing, it is not even worth reporting.

The other bugs all have strong predictors on the list.  In fact,
the top eight predicates have exactly one predictor for each of the seven
bugs that occur, with the exception of bug \#1, which has one very
strong sub-bug predictor in the second spot and another predictor
in the sixth position.  Notice that even the rarest bug, bug \#2,
which occurs more than an order of magnitude less frequently than
the most common bug, is identified immediately after the last of
the other bugs.\footnote{The peculiar eighth predicate, \texttt{f < f},
says that after an assignment the new value of {\tt f} is less than
the old value of {\tt f}.}  Furthermore, we have verified by hand that
the selected predicates would, in our judgment, lead an engineer to
the cause of the bug. Overall, the elimination algorithm does an excellent
job of listing separate causes of each of the bugs in order of priority,
with very little redundancy.

Below the eighth position there are no new bugs to report and every
predicate is correlated with predicates higher on the list.  Even
without the table of numbers at the right it is easy to spot the
eighth position as the natural cutoff.  Notice that the initial and
effective thermometers for the first eight predicates are essentially
identical, with the exception of the predicate at position six, indicating
that it is related to some predicate listed earlier (i.e., its sub-bug
at position two).  All of the predicates below the eighth line have very
different initial and effective thermometers (either many fewer
failing runs, or much more non-deterministic, or both) showing that these
predicates are strongly affected by 
higher-ranked predicates.

\subsection{Additional Experiments}

Our algorithm has a drawback illustrated by the \moss\ experiment: It
is not easy to identify the predicates to which a predicate is closely
related.  Such a feature is useful in confirming whether two selected
predicts represent different bugs or are in fact related to the same
bug.  We have a measure of how strongly $P$ implies another predicate
$P'$: How does removing the runs where $R(P) = 1$ affect the
importance of $P'$?  The more closely related $P$ and $P'$ are, the
more $P'$'s importance drops when $P$'s failing runs are removed.  In
our implementation, each predicate $P$ in the final, ranked list of
predicates is hyperlinked to an {\em affinity} list of all predicates
ranked by how much $P$ causes their ranking score to decrease.

\input{views/table}

\subsubsection{\ccrypt}

We analyzed \ccrypt 1.2, which has a known bug \cite{Selinger:2003:cqual}.  
The results are shown in \autoref{tab:views-ccrypt}.
Our algorithm reports two predictors, which both point directly to the single bug. 
It is easy to discover that the two predictors are for the same bug; the first predicate
is listed first in the second predicate's affinity list, indicating the first predicate
is a sub-bug predictor of the second predicate.

\subsubsection{\bc}

\bc 1.06 has a previously reported buffer overrun.  Our results are shown in \autoref{tab:views-bc}.
The outcome is the same as for \ccrypt: two predicates are retained by elimination, and the second
predicate lists the first predicate at the top of its affinity list, indicating the first predicate
is a sub-bug predictor of the second.  Both predicates point to the cause of the overrun.  
This bug causes a crash long after the overrun occurs and there is no useful
information on the stack at the point of the crash to assist in isolating this bug.

\subsubsection{\exif}

\autoref{tab:views-exif} shows results for \exif 0.6.9, an open source image processing program.
The three predicates are predictors of three separate, and previously unknown, crashing bugs.  It took less
than 20 minutes of work to find and verify the cause of each of the bugs using these predicates
and the additional highly correlated predicates on their affinity lists.

We will next elucidate how one of the predicates reported in \autoref{tab:views-exif},
\exifpred, enabled us to effectively isolate the cause of one of the bugs.
The bug has the following unique stack trace:
\begin{verbatim}
main
    exif_data_save_data
        exif_data_save_data_content
            exif_data_save_data_content
                exif_data_save_data_entry
                    exif_mnote_data_save
                        exif_mnote_data_canon_save
                            memcpy
\end{verbatim}
and the code in the vicinity of the crash site is as follows:
\begin{verbatim}
// snippet of exif_mnote_data_canon_save
for (i = 0; i < n->count; i++) {
     ...
(c)  memcpy(*buf + doff, n->entries[i].data, s);
     ...
}
\end{verbatim}
This alone provided hardly any insights into the cause of the bug.
But combined with the fact that the predicate {\tt o + s > buf\_size} in function
{\tt exif\_mnote\_data\_canon\_load} is highlighted by our algorithm,
it led us to construct the following call sequence by a quick inspection of the
source code:
\begin{verbatim}
main
    exif_loader_get_data 
        exif_data_load_data
            exif_mnote_data_canon_load
    exif_data_save_data
        exif_data_save_data_content
            exif_data_save_data_content
                exif_data_save_data_entry
                    exif_mnote_data_save
                        exif_mnote_data_canon_save
                            memcpy
\end{verbatim}
The code in the vicinity of the predicate {\tt o + s > buf\_size} is as follows:
\begin{verbatim}
// snippet of exif_mnote_data_canon_load
for (i = 0; i < c; i++) {
     ...
     n->count = i + 1;
     ...
(a)  if (o + s > buf_size) return;
     ...
(b)  n->entries[i].data = malloc(s); 
     ...
}
\end{verbatim}
It is apparent from the above code snippets and the
call sequence that whenever the predicate {\tt o + s > buf\_size} is true:
\begin{itemize}
\item
the function {\tt exif\_mnote\_data\_canon\_load} returns on line (a),
thereby skipping the call to {\tt malloc} on line (b) and leaving {\tt n->entries[i]->data}
uninitialized for some value of {\tt i}, and
\item
the function {\tt exif\_mnote\_data\_canon\_save} passes the uninitialized
{\tt n->entries[i]->data} to {\tt memcpy} on line (c), which reads it
and eventually crashes.
\end{itemize}
In summary, our algorithm enabled us to effectively isolate
the cause of a previously unknown, crashing bug in source code unfamiliar to us in
a couple of minutes and without explicit specifications.
\subsubsection{\rhythmbox}

\autoref{tab:views-rhythmbox} shows our results for \rhythmbox
0.6.5, an interactive, graphical, open source music player.
\rhythmbox is a complex event-driven system, written using
a library providing object-oriented primitives in C.  Event-driven
systems use event queues; each event performs some computation and
possibly adds more events to some queues.  We know of no static
analysis today that can analyze event-driven systems accurately,
because no static analysis is currently capable of analyzing the
heap-allocated event queues with sufficient precision.
Crash-reporting systems are also of limited utility in analyzing
event-driven systems, as the stack in the main event loop is
unchanging and all of the interesting state is in the queues.

We isolated two distinct bugs in \rhythmbox.  The first predicate led
us to a race condition.  The second predicate was not useful directly,
but we were able to isolate the bug using the predicates in its
affinity list.  This second bug revealed what turned out to be a very
common incorrect pattern of accessing the underlying object library
(recall \autoref{sec:introduction}).  These bugs
were confirmed and patches applied within a few days, in part because
we could quantify the bugs as important crashing bugs.  It required
several hours to isolate each of the two bugs, in part because \rhythmbox
is complex and we were unfamiliar with it and in part because the bugs
were violations of subtle invariants.  We could not have even begun to
understand these bugs without the information provided by our tool.
