%% -*- LaTeX -*-

\input{table}

In this section we present the results of applying the algorithm
described in \autoref{sec:algorithm} in five case
studies.\footnote{More complete data may be browsed interactively at
  \url{http://www.cs.berkeley.edu/~liblit/pldi-2005}.}
\autoref{tab:exps} shows the summary statistics for each of the
experiments.  In each study we ran the programs on about 32,000 random
inputs.  The number of instrumentation sites varies with the size of
the program, as does the number of predicates those instrumentation
sites yield.  Our algorithm is very effective in reducing the number
of predicates the user must examine.  For example, in the case of
\rhythmbox an initial set of 857,384 predicates is reduced to 537 by the $\increase(P) > 0$
test, a reduction of 99.9\%.  The elimination algorithm then yields 15 predicates, a further
reduction of 97\%.  The other case studies show a similar reduction in the number of
predicates by 3-4 orders of magnitude.

The results we discuss are all on sampled data.  Sampling creates
additional challenges that must be faced by our algorithm.  Assume $P_1$ and $P_2$ are
equivalent bug predictors and both are sampled at a rate of
$\nicefrac{1}{100}$ and both are reached once per run.  Then even though
$P_1$ and $P_2$ are equivalent, they will be observed in nearly disjoint
sets of runs and treated as close to independent by the elimination
algorithm.

To address this problem, we set the sampling rates of predicates to be
inversely proportional to their frequency of execution.  Based on a
training set of 1,000 executions, we set the sampling rate of each predicate so 
as to obtain an expected 100 samples of each predicate in subsequent program 
executions; the sampling rate is clamped to a minimum of $\nicefrac{1}{100}$.  
Thus, rarely executed code has a
much higher sampling rate than very frequently executed code.  (A
similar strategy has been pursued for similar reasons in related work \cite{chil04}.)  We
have validated this approach by comparing the results for each
experiment with results obtained with no sampling at all (i.e., the
sampling rate of all predicates set to 100\%).  The results are
identical except for the \rhythmbox and \moss experiments, where we
judge the differences to be minor: sometimes a different but logically
equivalent predicate is chosen, the ranking of predictors of different
bugs is slightly different, or one or the other version has a few
extra, weak predictors at the tail end of the list.  

\subsection{A Validation Experiment}

To validate our algorithm we first performed an experiment in which we
knew the set of bugs in advance.  We added nine bugs to \moss, a
widely used service for detecting plagiarism in software
\cite{Schleimer:2003:WLA}.  Six of these were previously discovered
and repaired bugs in \moss that we reintroduced; the other three were
variations on three of the original bugs, to see if our algorithm could
discriminate between pairs of bugs with very similar behavior but
distinct causes.  The nature of the eight crasing bugs varies: four
buffer overruns, a null file pointer dereference in certain cases, a
missing end-of-list check in the traversal of a hash table bucket, a missing
out-of-memory check, and a violation of a subtle invariant that must be maintained between two
parts of a complex data structure.  

The last bug -- incorrect handling of comments in certain cases -- only
causes incorrect output, not a crash.  For most of the other bugs, failure can 
be detected by noting program crashes (though some of these bugs are non-deterministic 
and do not always cause crashes).  The last bug, however, cannot be detected 
in such an easy fashion.
For our experiment we also ran a correct version of \moss\ and
compared the output of the two versions, labeling any run where the
output was different as a failure.  The purpose of using this oracle
is to show that, in principle, bugs other than crashing bugs can also
be isolated using our techniques, provided there is some way, whether
by automatic self-checking or human inspection, to recognize failing
runs.

\input{moss-view/table}

\autoref{tab:mossdilute} shows the results of the experiment.  The
predicates listed were selected by the elimination algorithm in the
order shown.  The second column is the initial thermometer for each
predicate,  showing the \context\ and \increase\ scores before
elimination is performed. The fourth column is the {\em effective}
thermometer, showing the \context\ and \increase\ scores for a
predicate $P$ at the time $P$ is selected (i.e., when it is the
top-ranked predicate).  The scores in the first and third columns are
the initial and effective importance (harmonic mean) scores, respectively.

As part of the experiment we separately recorded the exact set of
bugs that occurred in each run.
The table at the far right of \autoref{tab:mossdilute} shows, for
each selected predicate and for each bug, the number of runs in which
both the selected predicate is observed to be true and the bug occurs.
Note that while each
predicate has a very strong spike at one bug, indicating it is a
strong predictor of that bug, there are always some runs with other
bugs present.  For example, the top-ranked predicate, which is
overwhelmingly a predictor of bug \#5, also includes some runs where
bugs \#3, \#4, and \#9 occurred.  This situation is not the result of
misclassification of failing runs by our algorithm.  As observed in
\autoref{sec:introduction}, more than one bug may occur in a run;
it simply happens that in some runs bugs \#5 and \#3 both occur (to
pick just one possible combination).

A particularly interesting case of this phenomenon is bug \#7, one of
the buffer overruns.  Bug \#7 is not strongly predicted by any
predicate on the list but in fact occurs in at least a few of the
failing runs of most predicates.  We have examined the runs of bug \#7
in detail and found that the only failing runs bug \#7 occurs in also
have at least one other bug.  That is, even though the bug \#7 overrun
happens (though rarely), it never causes incorrect output or a crash
in any run.  Bug \#8, another overrun, is not even shown because the
overrun is never triggered in our data (its column would be all
0's).\footnote{Bug \#8 was originally found by a code inspection.}
There is no way our algorithm can find causes of bugs that do not
occur, but recall that part of our purpose in sampling user executions
is to get an accurate picture of the most important bugs. It is
consistent with this goal that if a bug never causes a problem, it is
not only not worth fixing, it is not even worth reporting.

The other bugs all have strong predictors on the list.  In fact,
the top eight predicates have exactly one predictor for each of the seven
bugs that occur, with the exception of bug \#1, which has one very
strong sub-bug predictor in the second spot and another predictor
in the sixth position.  Notice that even the rarest bug, bug \#2,
which occurs more than an order of magnitude less frequently than
the most common bug, is identified immediately after the last of
the other bugs.\footnote{The peculiar eighth predicate, \texttt{f < f},
says that after an assignment the new value of {\tt f} is less than
the old value of {\tt f}.}  Furthermore, we have verified by hand that
the selected predicates would, in our judgment, lead an engineer to
the cause of the bug. Overall, the elimination algorithm does an excellent
job of listing separate causes of each of the bugs in order of priority,
with very little redundancy.

Below the eighth position there are no new bugs to report and every
predicate is correlated with predicates higher on the list.  Even
without the table of numbers at the right it is easy to spot the
eighth position as the natural cutoff.  Keep in mind that the length of 
the thermometer is on a log scale, hence changes in larger magnitudes may 
appear less evident.  Notice that the initial and
effective thermometers for the first eight predicates are essentially
identical, with the exception of the predicate at position six, indicating
that it is somewhat affected by a predicate listed earlier (i.e., its companion
sub-bug predictor 
at position two).  All of the predicates below the eighth line have very
different initial and effective thermometers (either many fewer
failing runs, or much more non-deterministic, or both) showing that these
predicates are strongly affected by 
higher-ranked predicates.

\issue[Mayur]{This is a good way to explain why we have the initial
  and effective thermometers in the table, but I don't see much
  difference b/w them even for the predicate at position six.  I guess
  the key difference is the number of failing runs, and since the
  lenght of the thermometer is on a log scale, the difference is not
  evident.  Alex, is this the only difference?  If yes, we can remind
  the reader again that it is a log scale (you have already mentioned
  this in sec 3.3 but we can mention it again).}

\issue[Alex]{Mentioning the log scale again would be fine.  The
  context score has decreased as has the number of failing runs in
  number 6.  You could change it to say "somewhat affected"; certainly
  it is not as dramatically affected as the predicates further down
  the list.}

\issue[Alice]{I've implemented Alex's suggestion above.  Mayur or Alex,
  check to see if you like it.}

\subsection{Additional Experiments}

Our algorithm has a drawback illustrated by the \moss\ experiment: It
is not easy to identify the predicates to which a predicate is closely
related.  Such a feature is useful in confirming whether two selected
predicates represent different bugs or are in fact related to the same
bug.  We have a measure of how strongly $P$ implies another predicate
$P'$: How does removing the runs where $R(P) = 1$ affect the
importance of $P'$?  The more closely related $P$ and $P'$ are, the
more $P'$'s importance drops when $P$'s failing runs are removed.  In
our implementation, each predicate $P$ in the final, ranked list of
predicates is hyperlinked to an {\em affinity} list of all predicates
ranked by how much $P$ causes their ranking score to decrease.

\input{views/table}

\subsubsection{\ccrypt}

We analyzed \ccrypt 1.2, which has a known bug \cite{Selinger:2003:cqual}.  
The results are shown in \autoref{tab:views-ccrypt}.
Our algorithm reports two predictors, which both point directly to the single bug. 
It is easy to discover that the two predictors are for the same bug; the first predicate
is listed first in the second predicate's affinity list, indicating the first predicate
is a sub-bug predictor associated with the second predicate.

\subsubsection{\bc}

\bc 1.06 has a previously reported buffer overrun.  Our results are shown in \autoref{tab:views-bc}.
The outcome is the same as for \ccrypt: two predicates are retained by elimination, and the second
predicate lists the first predicate at the top of its affinity list, indicating the first predicate
is a sub-bug predictor of the second.  Both predicates point to the cause of the overrun.  
This bug causes a crash long after the overrun occurs and there is no useful
information on the stack at the point of the crash to assist in isolating this bug.

\subsubsection{\exif}

\autoref{tab:views-exif} shows results for \exif 0.6.9, an open source image processing program.
The three predicates are predictors of three separate, and previously unknown, crashing bugs.  It took less
than 20 minutes of work to find and verify the cause of each of the bugs using these predicates
and the additional highly correlated predicates on their affinity lists.

\issue[Mayur]{I am not sure whether the EXIF example is at the right place.}

We will next elucidate how one of the predicates reported in \autoref{tab:views-exif},
\exifpred, enabled us to effectively isolate the cause of one of the bugs.
The bug has the following unique stack trace:
\begin{quote}
\begin{verbatim}
main
  exif_data_save_data
    exif_data_save_data_content
      exif_data_save_data_content
        exif_data_save_data_entry
          exif_mnote_data_save
            exif_mnote_data_canon_save
              memcpy
\end{verbatim}
\end{quote}
and the code in the vicinity of the crash site is as follows:
\begin{quote}
\begin{verbatim}
// snippet of exif_mnote_data_canon_save
for (i = 0; i < n->count; i++) {
     ...
(c)  memcpy(*buf + doff,
            n->entries[i].data, s);
     ...
}
\end{verbatim}
\end{quote}
This alone provides little insight into the cause of the bug.
However, our algorithm highlights {\tt o + s > buf\_size} in function
{\tt exif\_mnote\_data\_canon\_load} as a strong failure predictor.
This leads us to construct the following call sequence by a quick inspection of the
source code:
\begin{quote}
\begin{verbatim}
main
  exif_loader_get_data 
    exif_data_load_data
      exif_mnote_data_canon_load
  exif_data_save_data
    exif_data_save_data_content
      exif_data_save_data_content
        exif_data_save_data_entry
          exif_mnote_data_save
            exif_mnote_data_canon_save
              memcpy
\end{verbatim}
\end{quote}
The code in the vicinity of the predicate {\tt o + s > buf\_size} is as follows:
\begin{quote}
\begin{verbatim}
// snippet of exif_mnote_data_canon_load
for (i = 0; i < c; i++) {
     ...
     n->count = i + 1;
     ...
(a)  if (o + s > buf_size) return;
     ...
(b)  n->entries[i].data = malloc(s); 
     ...
}
\end{verbatim}
\end{quote}
It is apparent from the above code snippets and the
call sequence that whenever the predicate {\tt o + s > buf\_size} is true:
\begin{itemize}
\item
the function {\tt exif\_mnote\_data\_canon\_load} returns on line (a),
thereby skipping the call to {\tt malloc} on line (b) and leaving {\tt n->entries[i]->data}
uninitialized for some value of {\tt i}, and
\item
the function {\tt exif\_mnote\_data\_canon\_save} passes the uninitialized
{\tt n->entries[i]->data} to {\tt memcpy} on line (c), which reads it
and eventually crashes.
\end{itemize}
In summary, our algorithm enabled us to effectively isolate
the cause of a previously unknown, crashing bug in source code unfamiliar to us in
a couple of minutes and without explicit specifications.
\subsubsection{\rhythmbox}

\autoref{tab:views-rhythmbox} shows our results for \rhythmbox
0.6.5, an interactive, graphical, open source music player.
\rhythmbox is a complex event-driven system, written using
a library providing object-oriented primitives in C.  Event-driven
systems use event queues; each event performs some computation and
possibly adds more events to some queues.  We know of no static
analysis today that can analyze event-driven systems accurately,
because no static analysis is currently capable of analyzing the
heap-allocated event queues with sufficient precision.
Crash-reporting systems are also of limited utility in analyzing
event-driven systems, as the stack in the main event loop is
unchanging and all of the interesting state is in the queues.

We isolated two distinct bugs in \rhythmbox.  The first predicate led
us to a race condition.  The second predicate was not useful directly,
but we were able to isolate the bug using the predicates in its
affinity list.  This second bug revealed what turned out to be a very
common incorrect pattern of accessing the underlying object library
(recall \autoref{sec:introduction}).  These bugs
were confirmed and patches applied within a few days, in part because
we could quantify the bugs as important crashing bugs.  It required
several hours to isolate each of the two bugs, in part because \rhythmbox
is complex and we were unfamiliar with it and in part because the bugs
were violations of subtle invariants.  Note, however, that we could not have 
even begun to
understand these bugs without the information provided by our tool.

\issue[Mayur]{Saying that we were unfamiliar with rhythmbox probably
  connotes that we were familiar with bc, ccrypt, and exif, when in
  fact we weren't.  So I think it should go.  Secondly, saying that
  the bugs were violations of subtle invariants may connote that those
  found in the other experiments weren't violations of subtle
  invarianta, which I think is not true.

  How about saying that the bugs in \rhythmbox{} are related to
  violations of complex heap invariants (as Alex noted long back) and
  then mention that we currently do not track predicates on heap
  structures (as Alex mentions at the end of the Background section
  (sec 2)) and then round off the para by saying that we leave this
  for future work?}
