This section discusses background for the experimental results
reported in \Autoref{sec:experiments:results}.

%While all software
%experiments are difficult to do well, we have learned the hard way
%that there are some particular problems that must be addressed to do
%our experiment well.
While all software experiments are difficult to design, we have
learned the hard way that there are some particular problems that must
be addressed in order to design our experiment correctly.
Thus, this section discusses the set-up for our
experiment in some detail, especially how we have compensated for
potential sources of bias.

The basic framework of our experiment is a straightforward five step
process:
\begin{enumerate}
\item select an existing software application,
\item modify the source code to inject bugs into the program,
\item instrument the modified program,
\item gather results from a large number of runs performed with automatically generated data and
\item apply our algorithm to the results.
\end{enumerate}
We discuss each step; the reader who is not interested in
these details may wish to proceed to \Autoref{sec:experiments:results} and
use this section only for reference.

We chose \moss\ \cite{Schleimer:2003:WLA} as our benchmark program.  \moss\ is a
software plagiarism detection service\footnote{That is,
\moss\ detects copying in large sets of programs.  The typical \moss
user is a professor or teaching assistant in a programming course.}
that has been available since the late 1990's and has several thousand
users worldwide.  As such, \moss\ has many of the characteristics of
real software: it has users who depend on it, it is constantly
undergoing revision as its purpose and the environment in which it
runs evolves, and it is complex enough to be composed of several
interacting subsystems.  
From our point of view, \moss\ has the additional advantage that it
was written and is maintained by one of the authors.

The next step, injecting bugs into the software, is problematic, as
the choice of bugs to include or exclude can dramatically affect the
results.  Nearly all of the bugs were taken directly from the bug
logs for \moss.  In some cases the code had evolved since the original
bug was fixed, in which case we had to judge how to modify the
bug to inject it into the code.  We also included three bugs that
were not \moss\ bugs.  One of these is a known bug from another system
where there is an obviously analogous place to add that bug to \moss\
(see below). The other two are duplicates of two different buffer
overrun bugs in \moss.  In each case, we restored the original bug,
and then added a second, very similar buffer overrun in a different
place, the purpose being to see if our algorithm could not only detect
the overruns, but also distinguish between them.

We briefly describe the nine bugs we added to \moss:
\begin{enumerate}
\item To correctly report the location of duplicate code \moss\ must
track line numbers.  We introduced a bug that causes the number of
lines in C-style multi-line comments to be counted incorrectly.  The
bug only occurs under a special set of circumstances: the option to
match comments must be on (normally \moss\ ignores comments
completely, and that is a separate code path with no bug), the
programs involved must have C multi-line comments, and in addition the
position of these comments must ultimately affect the output.
Note that this bug is not only non-deterministic in
the sense defined in \Autoref{sec:algorithm}, it also does not
cause the program to crash; the program simply generates incorrect
output.

\item \moss\ has the option to dump its internal data structures in a
binary file format called a \termdef{database}.  We removed the check for a
null {\tt FILE} pointer in the case that the database cannot be opened
for writing.  This bug is analogous to one reported in {\tt ccrypt}
\cite{Selinger:2003:cqual}.  This is a deterministic bug, and in fact the
program crashes almost immediately after failing to open the file.

\item Loading a \moss\ database is complex, as a number of data
structures must be kept in sync.  We removed an array bounds update
in the database loading routine, so that even though a database was
loaded, the pointer to the end of one array {\tt A} was not moved to
reflect that new data had been added to the end of {\tt A}. The
program behaves normally unless a second database is loaded, at which
point the second database at least partially overwrites that portion
of the first database stored in {\tt A}.  This bug has unpredictable
effects.  Depending on what files are compared and the contents of the
databases loaded, the result might be that the program terminates with
correct output, that it terminates with incorrect output, or that it
crashes.  This was a particularly difficult bug to find originally.

\item We removed a size check that prevented users from supplying command-line arguments
that could cause the program to overrun the bounds of an array.  When
this bug is triggered the program may terminate with correct output,
terminate with incorrect output, or crash.


\item \moss\ handles Lisp programs differently from other languages;
at one time all languages where handled in the same manner, but the
others have been gradually ported to an improved algorithm.  The Lisp
processing involves a standard hash table; we removed one of the
end-of-bucket checks, which causes a crash when the program scans to
the end of a hash bucket and tries to dereference a \texttt{NULL} pointer. 

\item For efficiency \moss\ preallocates a large area of memory for its primary data structure.
When this area of memory is filled, the program should fail
gracefully.  We removed the out-of-memory check.  The original bug
was more complex, but cannot be reproduced exactly because this
portion of the code has been revised.

\item \moss\ has a routine that scans an array for multiple copies of a data value.
We removed the limit check that prevents the code from searching past the end of the array.  This is another
buffer overrun, but of a different kind.  First, whether the overrun occurs is very data dependent and in fact it is
difficult to construct a test case by hand that triggers the bug.  Second, the routine in question only reads
past the end of the array (no memory locations are written), so it is quite likely that the program will
succeed in spite of the error.  This bug is synthetic (it never occurred in \moss) but is derived from bug \#8.

\item This is a variant on bug \#7, in another routine that deals with duplicates, but
bug \#8 occurs under an even rarer set of circumstances.  In
fact, this bug was never known to have caused a failure in \moss; it
was discovered by a code review.

\item This bug is a variant of bug \#4, but involves a different command-line argument and
a different array.
\end{enumerate}

In summary, the nine bugs are all either real bugs in \moss\ or bugs
closely related to real bugs in \moss\ or other programs.  The bugs
range from typical C coding errors (e.g., \texttt{NULL} pointer dereferences
and array overruns) to high-level violations of a system's internal
invariants (e.g., bugs \#1 and \#3).

To allow us to measure the accuracy of our techniques we also added code to \moss\ to
log when each bug was triggered.  We were careful to exclude this code from the
code that was instrumented for sampling, as predicates on the logging code would be
very highly correlated with program failures.

In the next step we ran our source-to-source instrumentor on \moss,
modified to include the bugs and the logging code.  We excluded the logging
code from instrumentation as well as code automatically generated by the tool
{\tt flex}.  While examining predicates on {\tt flex}'s internal state
would quite possibly yield some useful clues about the sources of
bugs, it is very unlikely that any programmer working on \moss\ would
be able to interpret such predicates.

After instrumenting \moss\ we ran both the buggy version and the
original version on the same random inputs. We recorded whether the buggy
version succeeded or failed by examining its exit code and, in the
case that it terminated normally, by checking whether the output of
both versions matched.  In practice, we envision that users would have
a way to give simple feedback on program executions in the case that a
program terminates normally but produces incorrect output.  We have
modeled that in our experiment by recording one bit (success or
failure) for each run based on whether its output matches that of the
unmodified \moss\ on the same inputs.

The randomly generated inputs are produced according to the following
scheme.  A different probability distribution is associated with each
command line option.  For the numeric options, the distribution gives
the probability that the option takes on a small, medium, or large
value, or is absent altogether.  For options that are essentially
enumerated types (such as the programming language used) the
distribution just includes the probability of each element of the
enumeration as well as the probability that the option is absent.
Depending on the programming language chosen, files to submit to
\moss\ are chosen randomly from a collection of thousands of C, Lisp,
and Java files.  The number of programs to submit with each run is
another probability distribution, which guarantees that there are at
least some runs with a very small number of files and some with a very
large number of files submitted.

In the process of performing the experiment we discovered several ways
in which our inputs were not as random as they could be---we had
accidentally coupled two or more choices that could be independent.
We removed the sources of coupling among input parameters that we
discovered.  

Finally, the infrastructure for this experiment was sufficiently complex
that we found it necessary to automatically discard executions that
failed in ways we could not handle.  For example, rarely the buggy version of
\moss\ would hang instead of crashing, in which case it was eventually killed
by a watchdog process we set up for that purpose.  In these cases no report
was generated, so we could not make use of the run.

The analysis of the results of applying our algorithm is the
subject of \Autoref{sec:experiments:results}.

%% LocalWords:  ccrypt
