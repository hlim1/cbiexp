Put name markers in brackets after items in the to-do list:

  +Foo: things Foo is best suited to work on or plans to work on
  -Foo: things Foo is definitely not able to work on
  *Foo: things Foo is working on right now

If you see a problem but either cannot fix it or don't want to work on
it now, add it to the list below so that it will not be overlooked or
forgotten.  Try to keep to-do items roughly organized by section,
except for whole-paper issues which are listed first.

As you fix outstanding items, remove them from the list and check in
this updated to-do list along with your changes.  When this list is
empty, that basically means we're done!


------------------------------------------------------------------------
--
--  overall

There may be some inconsistency about the use of "rate" or "sampling
rate".  A sampling rate should always be a fraction between 0 and 1,
such as 1/10 or 1/100.  The inverse of a sampling rate is a sampling
density, such as 10 or 100.  Higher sampling density is equivalent to
lower sampling rate.


------------------------------------------------------------------------
--
--  Section 2
--

Within Section 2.1 [Expected Benefits], there may be too much
conceptual overlap between "Richer Diagnostic Language for Complex
Bugs" and "Improved Precision and Recall".
[*Jake]

From Mulhern, reading revision 1518: Several times you mention other automated analysis tools. Page 2 left column is one instance but it certainly occurs more than once. You suggest that the results of your tool can be input to some other tool. Unless you are more specific this leaves the reader hanging, thinking, "How? What tools? What would the other tools do? Does this mean their tool has some obvious flaw that other tools redress? What are they not telling me?" You should be more specific or leave this suggestion out altogether.

From Mulhern, reading revision 1518: In Section 2.1 (Richer Diagnostic Language for Complex Bugs) Sentence: Bugs with easily-described causes partition cleanly using a single bug predictor, with (all or most) successes in one subspace and (all or most) failures in the other. My comment: We're trying to identify bugs but in this sentence you suddenly start talking about failures and successes instead of bug causes. If you're only dealing with a single bug you should make that explicit. If you're dealing with multiple bugs then you need to revise this statement.

From Mulhern, reading revision 1518: In Section 2.1 (Richer Diagnostic Language for Complex Bugs) Sentence: However, the success/failure boundary may be considerably more irregular. Comment: This sentence seems like filler and may also be incorrect (See point 2).

From Mulhern, reading revision 1518: In Section 2.1 (Richer Diagnostic Language for Complex Bugs) Sentence: A richer language of candidate bug predictors describes more complex shapes within the space of runs, and thereby diagnoses the causes of more complex failures. Suggested rewording: A richer language of candidate bug predictors can describe more complex shapes within the space of runs.

From Mulhern, reading revision 1518: In Section 2.1 (Richer Diagnostic Language for Complex Bugs) Sentence: Of course, a fiendishly convoluted (but hopefully rare) bug may require a predicate as complex as the entire original program to describe its behavior. Comment: Does this sentence really make sense? It asks the reader to do a lot. I am happy to view a program as a predicate; but I tend to think of it as dividing the inputs into ones that it succeeds on and ones that it fails on.

From Mulhern, reading revision 1518: Section 2.1 (Richer Diagnostic Language for Complex Bugs). Overall comment: I think the basic point is that complex predicates have an advantage in that they can be used to partition the run space more precisely but that the computational cost of analyzing arbitrarily complex predicates is high and that very complex predicates may be too precise to be useful. There may be a much clearer and more succinct way of getting this point across.


------------------------------------------------------------------------
--
--  Section 3
--

Section 3.1 needs to better define what it means for a predicate to be
true or false.  If I observe a predicate true once and false once in a
single run, is that predicate considered true, false, or both for
purposes of your three-valued logic?
[+tchen]

Section 3.3 is fairly technical and not easy to follow.  An example
with concrete numbers might help here, but I'm not sure.  This may
just be inherently tricky stuff.
[+tchen]

------------------------------------------------------------------------
--
--  Section 4
--

Section 4 offers some interesting ideas, though they aren't explored
or justified as fully as they could be.  I'm not convinced that
Definition 4.2 is working in the right direction.  If two predicates
are highly correlated according to this definition, doesn't that mean
they are nearly identical and therefore not especially interesting as
a pair?  I would have thought that an interesting pair predicates
would be ones with very little apparent connection in the PDG, but
which nonetheless are tightly connected in how they predict failures.
[+chen]

------------------------------------------------------------------------
--
--  Section 5
--

Section 5: I'm not sure that these are properly considered as
*internal* threats to validity, but I don't really know that taxonomy
of threat types very well.

A third case study would be nice here, but is probably not essential.
[-tchen]

------------------------------------------------------------------------
--
--  Section 6
--

Section 6 presents some results with conjunctions and disjunctions
broken out, but not all.  I'd like to see that done with the rest,
such as for Figure 2, to see if there's anything interesting going on.
[-Ben] [+arumuga]

The first paragraph of Section 6.4 [Effect of Sampling Rate] claims
that the number of interesting conjunctions is comparable to the
number of interesting simple predicates even at 1/1000 sampling.  Is
that really true?  You can't tell by squinting at the plots, and I
cannot find these raw numbers anywhere in the "data/*.txt" files.

Several of the figures need layout or color-selection work, especially
for monochrome printing.  [*Ben]

However, I don't buy *either* explanation for the weird conjunction
behavior.  Offline downsampling is supposed to be applied before
binarization, not after.  Assuming you did that correctly, offline
downsampling gives numbers which are completely indistinguishable from
online sampling.  And just getting a weird run of numbers from your
random number generator is unlikely to show the same conjunction hump
in six different applications.
[-tchen]

You also don't say anything about how long it takes to perform this
analysis.  Seconds?  Hours?  Weeks?  Readers will want to know.
[-Ben]

Section 6.1 [Top Scoring Predicates] no longer matches the data
actually shown in Figure 2.  We need to understand why the new Figure
2 doesn't look like the old one.  If the new figure is correct, the
discussion in Section 6.1 should be updated accordingly.

Figure 2 has relatively little information in it.  Perhaps this
one-bar plot should be replaced with either a small table or just
prose discussion with no table or figure at all.


------------------------------------------------------------------------
--
--  Section 7
--

------------------------------------------------------------------------
--
--  Section 8
--

Section 8 concludes on a weak note.  We ought to be able to make this
a bit sexier while still being honest about the technique's
limitations. [+Ben]

Regarding those limitations ... do you have further ideas in mind for
pulling the interesting predictors out from the big (quadratic) pool
of complex predicates?  Is it just a matter of implementation time to
try out something you think might work?  Or are we essentially out of
ideas at this point?  If you have ideas, then that's where I could see
more revisions to the implementation.  Otherwise, no, I don't think
more implementation work is needed.  Rather, it's a matter of revising
what's already here to get it really polished.
