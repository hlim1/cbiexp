% -*- TeX-master: "master" -*-

\section{Usefulness Metrics for Complex Predicates}
\label{sec-metrics}
In our experiments, we often observe hundreds of complex predicates with similar or even identical high scores.  The redundancy elimination algorithm will select the top predicate randomly from all those tied for the top score; a human programmer finding a predicate to use in debugging is likely to make a similar choice.  Since $\Importance$ measures predictive power all high-scoring predicates should be good bug predictors, but even a perfect predictor may be difficult for a programmer to use in finding and fixing a bug.

Bug-fixing using a simple predicate is aided by understanding the connection between the predicate and the bug it predicts; for a complex predicate, the programmer must also understand the connection between its components.  Given a set of complex predicates with similar high scores, those that can be easily understood by a human are preferable.  In this section we propose two metrics based on this criteria for selecting predicates from the large set of highly-scored predictors.  Only predicates selected by the metrics are presented to the user (if predicate data is to analyzed by an automated tool it may not be advantageous to employ these metrics).  Both metrics use criteria unrelated to a predicate's $\Importance$ score, making them orthogonal to pruning as discussed in \autoref{sec-pruning}.

The first metric models the debugging effort required from the programmer to find a direct connection between component predicates.  We adapt the distance metric of Cleve and Zeller \cite{1062522} for this purpose.  In this metric, the score of a predicate is the fraction of code that can be ignored while searching for the bug.  We use a similar metric called \emph{effort} for a complex predicate.

\begin{defn}
\label{def-effort}
The effort required by a programmer while using a complex predicate $C = \phi(p_1, p_2)$ is inversely proportional to the smaller fraction of code ignored in a breadth-first bidirectional search for $p_1$ from $p_2$ and vice-versa.
\end{defn}

The idea behind this metric is that the larger the distance between the two predicates, the greater the effort required to understand their relationship.  Also, if a large number of other branches are seen during the search, the programmer should keep track of these dependencies too.  Per Cleve and Zeller \cite{1062522}, we use the program dependence graph (PDG) to model the program rather than the source code. We perform a BFS starting from $p_1$ until $p_2$ is reached and count the total number of vertices visited during the search. The fraction of code covered is the ratio of the number of visited vertices to the total number of PDG vertices. 

The second metric is to consider the correlation between the two predicates.  Two predicates may be easily reached from one another without having an apparent connection - a complex predicate formed from them would provide little help to the programmer.  On the other hand, two predicates which are both affected by some shared area of code may have a connection which a programmer can easily discern.  The correlation between two predicates is defined based on the program dependency graph.  Given a single predicate $p$, we define its \textit{predecessor set} as the set of vertices in the PDG that can influence $p$.

\begin{defn}
\label{dfn5}
The correlation between two predicates of a complex predicate is defined as the number of vertices in the intersection of the two predecessor sets.
\end{defn}

The idea behind this metric is that a larger intersection between the $predecessor\ sets$ means it is possible that they are closely related.  We expecte correlation to mitigate the issue of disjunctive predicates raised in \autoref{sec-bground}, namely that the disjunction of predictors for two separate bugs will be scored very highly.  The predictors of two unrelated program faults are likely to reside in different areas of the program, and therefore the intersection of their predecessor sets would be smaller than two related predictors for the same bug, which are likely to be in closer proximity.

%Changed ``higher'' to ``better'' in the second-to-last sentence below, since what we actually want for metric 1 is a *lower* result.
The above two metrics could be applied both \emph{proactively} and \emph{reactively}.  A proactive use of the metrics will remove complex predicates whose metric values fall below a certain threshold of usefulness.  This will eliminate them from being computed and hence improve performance.  A reactive use of the metrics will retain all the predicates but break ties by giving higher ranks to those with better values for the metrics. This is desirable if neither computing time nor space is a concern.  We use CodeSurfer to build the PDG of a program and to compute these two metrics.

