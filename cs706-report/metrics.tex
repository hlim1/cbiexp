% -*- TeX-master: "report" -*-

\section{Usefulness Metrics for Complex Predicates}
\label{sec-metrics}
While complex predicates could help to better predict bugs, in the worst case, the upper bound on the number of interesting complex predicates is still quadratic in the number of simple predicates.  In our experiments, we often observe hundreds of complex predicates with similar or even identical high scores.  The overwhelming number of predicates makes it laborious for users to find a complex predicate with which they can start debugging.  In this section, we propose two metrics to reduce the number of complex predicates.

The first metric models the debugging effort required from the programmer as a filtering metric.  We adapt the distance metric of Cleve and Zeller \cite{1062522} for this purpose.  In this metric, the score of a predicate is the fraction of code that can be ignored while searching for the bug.  We use a similar metric called \emph{effort} for a complex predicate.

\begin{defn}
\label{def-effort}
The effort required by a programmer while using a complex predicate $C = \phi(p_1, p_2)$ is inversely proportional to the smaller fraction of code ignored in a breadth-first bidirectional search for $p_1$ from $p_2$ and vice-versa.
\end{defn}

The idea behind this metric is that the larger the distance between the two predicates, the greater the effort required to understand their relationship.  Also, if a large number of other branches are seen during the search, the programmer should keep track of these dependencies too.  Per Cleve and Zeller \cite{1062522}, we use the program dependence graph (PDG) to model the program rather than the source code. We perform a BFS starting from $p_1$ until $p_2$ is reached and count the total number of vertices visited during the search. The fraction of code covered is the ratio of the number of visited vertices to the total number of PDG vertices. 

The second metric is to consider the correlation between the two predicates.  Intuitively a complex predicate with two relatively independent predicates is less interesting because it doesn't provide much help to the users in finding anything new, besides the two individual predicates.  The correlation between two predicates is defined based on the program dependency graph.  Given a single predicate $p$, we define its \textit{predecessor set} as the set of vertices in the PDG that can influence $p$.

\begin{defn}
\label{dfn5}
The correlation between two predicates of a complex predicate is defined as the number of vertices in the intersection of the two predecessor sets.
\end{defn}

The idea behind this metric is that a larger intersection between the $predecessor\ sets$ means it is possible that they are closely related.  We expecte correlation to mitigate the issue of disjunctive predicates raised in \autoref{sec-bground}, namely that the disjunction of predictors for two separate bugs will be scored very highly.  The predictors of two unrelated program faults are likely to reside in different areas of the program, and therefore the intersection of their predecessor sets would be smaller than two related predictors for the same bug, which are likely to be in closer proximity.

%Changed ``higher'' to ``better'' in the second-to-last sentence below, since what we actually want for metric 1 is a *lower* result.
The above two metrics could be applied both \emph{proactively} and \emph{reactively}.  A proactive use of the metrics will prune away complex predicates whose metric values fall below a certain threshold of usefulness.  This will eliminate them from being computed and hence improve performance.  A reactive use of the metrics will retain all the predicates but break ties by giving higher ranks to those with better values for the metrics. This is desirable if neither computing time nor space is a concern.  We use CodeSurfer to build the PDG of a program and to compute these two metrics.

