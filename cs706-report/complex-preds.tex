% -*- TeX-master: "master" -*-

\section{Complex Predicates}
\label{sec-complex-preds}
%         *         *         *         *         *         *        80 columns |
A complex predicate $C$ is defined as $C = \phi(p_1, p_2, \ldots p_k)$ where 
$p_1, p_2, \ldots p_k$ are simple predicates and $\phi$ is a function in 
conjunctive normal form (CNF).  Evaluating $C$ requires combining predicates 
using $\wedge$ (``and'') and $\vee$ (``or''); a negation operator is not required 
because by design, the negation of every CBI predicate $p$ is also a predicate.  
For $N$ predicates there are $2^{2^N}$ such Boolean functions 
\cite{MathWorld:BoolFuncs}.  There may be hundreds of simple predicates involved 
in the analysis, and so this a prohibitively large number.

To reduce complexity we consider only functions of two predicates.  Out of the
16 ($2^{2^2}$) such functions, we consider only conjunction and disjunction since
they are likely to be most easily understood by programmers.  Our revised
definition is $C = \phi(p_1, p_2)$ where $\phi \in \{\vee, \wedge\}$.  Conjunction
and disjunction are commutative, and the reflexive cases ($p_1 \wedge p_1$ and 
$p_1 \vee p_1$) are uninteresting.  This reduces the number of complex predicates
to just ${N \choose 2} = \frac{N (N-1)}{2}$ binary conjunctions and an equal number 
of binary disjunctions.  These may be evaluated for a set of $R$ runs in $O(|R| N^2)$ 
time.  The revised definition for a complex predicate is used throughout; related
definitions may be trivially extended to the general case.

\subsection{Measuring Complex Predicates}
\label{sec-measuring}

For a predicate $p$ and a run $r$, $r(p)$ is $\true$ if and only if $p$ was observed to be true at least once during run $r$.  Similarly we could define $r(C)$ as follows:
\begin{defn}
\label{dfn1}
For a complex predicate $C = \phi(p_1, p_2)$, $r(C)$ is $\true$ iff at some point during the execution of the program, $C$ was observed to be true.
\end{defn}

The difficulty with this notion of complex predicates is that $C$ must be explicitly monitored during the program execution.  For example, $r(p_1) = \true$ and $r(p_2) = \true$ does not imply that $p_1 \wedge p_2$ is ever true at a single program point.  $p_1$ and $p_2$ may be true at different stages of execution but never true at the same time.  Furthermore, when $p_1$ and $p_2$ appear at different source locations, there may be no single point in time at which both are even well-defined and therefore simultaneously observable.  In order to be able to estimate the value of $C$ from its components, we adapt a less time-sensitive definition as follows:
\begin{defn}
\label{dfn2}
For a complex predicate $C = \phi(p_1, p_2)$, $r(C)$ is $\true$ iff $\phi(r(p_1), r(p_2)) = \true$
\end{defn}

In other words, we treat $r$ as distributive over $\phi$, effectively removing the requirement that all $p_i$ be observed simultaneously.  This can lead to false positives, because $r(C)$ may be computed to $\true$ when $C$ is actually $\false$ at all moments in time.  False negatives, however, cannot arise.  The impact of this assumption on the score of $C$ may be either positive or negative depending on whether $r$ failed or succeeded.

\subsection{Three-Valued Logic}
\label{sec-tvl}
This section explains how conjunctions and disjunctions are actually computed.  Three-valued logic is required because the value of a predicate in a run may not be certain. This can arise in two situations:
\begin{enumerate}
\item The predicate was not observed in a run because the run did not reach the line where it was defined.
\item The program reached the line where the predicate was defined but was not observed because of sampling.
\end{enumerate}

%         *         *         *         *         *         *        80 columns |

In such cases, the value of a predicate $p$ is considered unknown.  For the 
analysis introduced in \autoref{sec-background}, it is enough to consider whether 
$r(p)$ was $\true$ or $\neg \true$ (either $\false$ or $\unknown$).  When 
constructing compound predicates, however, considering the sub-cases of 
$\neg \true$ separately allows additional run information to be derived.  
Constructing a compound predicate requires generating two bits for each program
run: whether $C$ was observed at least once during run $r$ and whether $C$ was
observed $\true$ at least once during run $r$.  The case where a compound 
predicate is observed but not true can only be generated by considering whether 
the same was true for its components.

Consider a complex predicate $C = p_1 \wedge p_2$.  If either $r(p_1)$ or $r(p_2)$ was $\false$ in a run $r$, then $r(C) = \false$, since one false value disproves a conjunction.  If both $r(p_1)$ and $r(p_2)$ were $\true$, then $C$ was observed to be $\true$.  Otherwise, the value of $r(C)$ is $\unknown$. This is shown using a three-valued truth table in \autoref{tab:and}.

Similarly one true value proves a disjunction, and so a disjunction is $\false$ only if its components were observed $\false$ and not $\unknown$.  The truth table for complex predicate $D = p_1 \vee p_2$ is shown in \autoref{tab:or}.

\begin{table}
  \caption{Three-valued Truth Table for $C = p_1 \wedge p_2$}
  \label{tab:and}
  \centering
  \begin{tabular}{c|ccc}
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    $p_1 \diagdown p_2$ & T & F & ? \\
    \hline
    T & T & F & ? \\
    F & F & F & F \\
    ? & ? & F & ? \\
  \end{tabular}
\end{table}


\begin{table}
  \caption{Three-valued Truth Table for $D = p_1 \vee p_2$}
  \label{tab:or}
  \centering
  \begin{tabular}{c|ccc}
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    $p_1 \diagdown p_2$ & T & F & ? \\
    \hline
    T & T & T & T \\
    F & T & F & ? \\
    ? & T & ? & ? \\
  \end{tabular}
\end{table}

\subsection{Interesting Complex Predicates}
Even using the revised definition for $C$ the number of complex predicates is still quadratic in the number of simple predicates.  A large number of complex predicates formed by this procedure are likely to be useless in the analysis of the program.  Certainly a complex predicate that has a lower score than one of its components is useless: the component (simple) predicate with a higher score is a better predictor of failure, and so the complex predicate adds nothing to the analysis.

\begin{defn}
\label{dfn3}
A complex predicate $C = \phi(p_1, p_2)$ is ``interesting'' iff $\Importance(C) > \Max{\Importance(p_1)}{\Importance(p_2)}$
\end{defn}

In the case where the complex predicate has the same score as a component predictor, the simpler one is preferable.  Keeping only interesting combinations of predicates reduces the memory burden of storing them, and helps ensure the utility of a complex predicate that is presented to the user.

\subsection{Pruning}
\label{sec-pruning}
Constructing complex predicates from simple ones requires generating two bits of information (according to the three-valued logic discussed in \autoref{sec-tvl}) for each program run.  As CBI is meant to analyze deployed software, program runs potentially number in the hundreds of thousands, if not millions.  The measure of a complex predicate's relevance to a bug is its $\Importance$ score, which can only be derived once run information has been generated.  Complex predicates that are not interesting provide no information useful in debugging and are not presented to the programmer; in such a case the effort to form the predicate has been wasted.  This suggests pruning combinations before run information is generated based on an estimate of their resulting scores.  If this estimate falls below the threshold required for the predicate to be presented to the programmer, the complex predicate is not constructed.  Efficiently pruning predicates before they are constructed reduces a $O(|R|)$ operation (constructing run information) to one that executes in constant time.

The threshold $\Importance$ value, which complex predicates must potentially exceed to be constructed, can be calculated in two ways:
\begin{enumerate}
\item Only interesting complex predicates are retained; for a complex predicate $\phi(p_1, p_2)$ the threshold for $\Importance(C)$ is therefore $\Max{\Importance(p_1)}{\Importance(p_2)}$.
\item During redundancy elimination, only the predicate with the highest score is retained at each iteration.  The threshold for $\Importance(C)$ is therefore the highest score yet seen (including those of simple predicates).
% Note: as of 12/06 our implementation didn't do this.  This may have changed with recent modifications...
\end{enumerate}

%         *         *         *         *         *         *        80 columns |
To simplify the formulae derived in this section, we introduce some new terms and
notations.  If $R \in \{F, S\}$ is the set of program runs under consideration, then
$\obs{R}{p}$ denotes the number of runs in which $p$ was observed at least
once.  $R(p)$ is the number of runs in which $p$ was observed $\true$ at least once.
$\obsFalse{R}{p}$ is the number of runs in which predicate $p$ was observed
at least once but never observed $\true$.  It is equal to $\obs{R}{p} - R(p)$.
For some unknown quantity $x$, let $\ub{x}$ and $\lb{x}$ denote estimated upper
and lower bounds on the exact value of $x$, respectively.

Using an upper bound as the estimate of $\Importance$ guarantees that no
predicate is pruned erroneously.  This upper bound can be determined by maximizing
$\Increase(p)$ and $F(p)$ under constraints based on the propositional operation.
$\Importance(P)$ (\autoref{eqn2}), being a harmonic mean of these two terms, will
likewise be maximized.  From \autoref{eqn1}, an increase in $F(p)$ or $\obs{S}{p}$
or a decrease in $S(p)$ or $\obs{F}{p}$ will increase the value of $\Increase(p)$.
So an upper bound on $\Increase(p)$ would be:

\begin{equation}
\label{eqn3}
\ub{\Increase(p)} \equiv
\frac{\ub{F(p)}}{\lb{S(p)} + \ub{F(p)}}
-
\frac{\lb{\obs{F}{p}}}{\ub{\obs{S}{p}} + \lb{\obs{F}{p}}}
\end{equation}

Consider a conjunction $C = p_1 \wedge p_2$ and a set of runs $R$.  From \autoref{tab:and}, $C$ is
observed $\true$ if both $p_1$ and $p_2$ were observed $\true$.  Equivalently, the set
of runs in which $C$ was observed $\true$ is the intersection,
and thus cannot exceed the sizes, of the sets of
runs in which $p_1$ was observed $\true$ and $p_2$ was observed $\true$.
\begin{equation}
  \ub{R(C)} = \Min{R(p_1)}{R(p_2)}
\end{equation}

Likewise, $R(C)$ is minimized when there is minimum overlap between the set
of runs in which $p_1$ was observed $\true$ and $p_2$ was observed $\true$.
The minimum overlap is $0$ as long as $R(p_1) + R(p_2)$ does not exceed $|R|$,
the total number of runs.  Otherwise, both $p_1$ and $p_2$ must be observed
together in at least $R(p_1) + R(p_2) - |R|$ runs.  Thus,
\begin{equation}
  \lb{R(C)} = \Max{0}{R(p_1) + R(p_2) - |R|}
\end{equation}

Next consider $\obs{R}{C}$.  $C$ is observed when
\begin{enumerate}
\item both $p_1$ and $p_2$ are observed $\true$, or
\item either $p_1$ or $p_2$ is observed $\false$.
\end{enumerate}

To maximize $\obs{R}{C}$, applications of both rules above should be
maximized.
There are $\Min{R(p_1)}{R(p_2)}$ applications of Rule 1 when the set
of runs in which $p_1$ and $p_2$ are observed completely overlap.
There are $\obsFalse{R}{p_1} + \obsFalse{R}{p_2}$ applications of Rule
2 when the sets of runs in which $p_1$ and $p_2$ are observed and never $\true$ are
completely non-overlapping.
However, there has to be an overlap between these cases if their sum exceeds
$|R|$, the total number of runs.  Thus,
\begin{equation}
  \ub{\obs{R}{C}} = \Min{|R|}{\obsFalse{R}{p_1} + \obsFalse{R}{p_2}
                   + \Min{R(p_1)}{R(p_2)}}
\end{equation}

Finally, to minimize $\obs{R}{C}$, applications of the two rules above are
minimized.  This can happen when
\begin{enumerate}
\item the $\false$ observations of $p_1$ and $p_2$ completely overlap, contributing
$\Max{\obsFalse{R}{p_1}}{\obsFalse{R}{p_2})}$ to $\obs{R}{C}$, and
\item the $\true$ observations of $p_1$ and $p_2$ do not overlap.
However, the outcomes of $\Min{\obsFalse{R}{p_1}}{\obsFalse{R}{p_2}}$ runs have
been fixed in the previous rule, effectively reducing the number of runs to
$|R'| = |R| - \Min{\obsFalse{R}{p_1}}{\obsFalse{R}{p_2}}$.  Among these runs,
$R(p_1) + R(p_2)$
runs must be selected without overlap, failing which there would be an
$R(p_1) + R(p_2) - |R'|$ overlap
between the two sets.
\end{enumerate}

Thus,
\begin{multline}
  \lb{\obs{R}{C}} = \Max{\obsFalse{R}{p_1}}{\obsFalse{R}{p_2}} + \\
  \Max{0}{R(p_1) + R(p_2) - |R'|}
\end{multline}

For simplicity and generality, the preceding discussion derives the
bounds for a general set of runs $R$.  \autoref{tab:bounds-conj} lists
the specific bounds required in \autoref{eqn3} for conjunctions by
substituting the set of successful runs $S$ or the set of failed runs
$F$ in place of $R$.  The approach for disjunctions is similar.  In
interest of space, we omit the derivation and just list the bounds for
a disjunction $D = p_1 \vee p_2$ in \autoref{tab:bounds-disj}.

\begin{table}
  \caption{Bounds required in \autoref{eqn3} for a conjunction}
  \label{tab:bounds-conj}
  \centering
  \begin{tabular}{ll}
    \toprule
    Quantity & Bounds for $C = p_1 \wedge p_2$ \\
    \midrule
    $\ub{F(C)} $
      &
      $\Min{F(p_1)}{F(p_2)}$
      \\
      
    $\lb{S(C)}$
      &
      $\Max{0}{S(p_1) + S(p_2) - |S|}$
      \\
      
    $\lb{\obs{F}{C}}$
      &
      $\Max{\obsFalse{F}{p_1}}{\obsFalse{F}{p_2}} +$
      \\
      
    % empty first column
      &
      $\Max{0}{F(p_1) + F(p_2) - |F'|}$
      \\
      
    % empty first column
      &
      \multicolumn{1}{r}{where $|F'| = |F| - \Min{\obsFalse{F}{p_1}}{\obsFalse{F}{p_2}}$}
      \\
      
    $\ub{\obs{S}{C}}$
      &
      $\Min{|S|}{\obsFalse{S}{p_1} + \obsFalse{S}{p_2} + \Min{S(p_1)}{S(p_2)}}$
      \\
      \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \caption{Bounds required in \autoref{eqn3} for a disjunction}
  \label{tab:bounds-disj}
  \centering
  \begin{tabular}{ll}
    \toprule
    Quantity & Bounds for $D = p_1 \vee p_2$ \\
    \midrule
    $\ub{F(D)} $
      &
      $\Min{|F|}{F(p_1) + F(p_2)}$
      \\
      
    $\lb{S(D)}$
      &
      $\Max{S(p_1)}{S(p_2)}$
      \\
      
    $\lb{\obs{F}{D}}$
      &
      $\Max{F(p_1)}{F(p_2)} +$
      \\
      
    % empty first column
      &
      $\Max{0}{\obsFalse{F}{p_1} + \obsFalse{F}{p_2} - |F'|}$
      \\
      
    % empty first column
      &
      \multicolumn{1}{r}{where $|F'| = |F| - \Min{F(p_1)}{F(p_2)}$}
      \\
      
    $\ub{\obs{S}{D}}$
      &
      $\Min{|S|}{S(p_1) + S(p_2) + \Min{\obsFalse{S}{p_1}}{\obsFalse{S}{p_2}}}$
      \\
      \bottomrule
  \end{tabular}
\end{table}

As a concrete example, consider $C = p_1 \wedge p_2$.  Assume that there are 1,000
successful runs, 1,000 failed runs, and that $p_1$ and $p_2$ are observed in all runs.
Furthermore, assume
$F(p_1) = 500$, $F(p_2) = 1000$, $S(p_1) = 250$ and $S(p_2) = 500$.
Substituting $\obsFalse{R}{p} = 1000 - R(p)$ and computing the bounds listed in
\autoref{tab:bounds-conj} we get $\ub{F(C)} = 500, \lb{S(C)} = 0, \lb{\obs{F}{C}} = 1000$ and
$\ub{\obs{S}{C}} = 1000$.  As a result, the upper bound of $\Increase(C)$ will be
$\frac{500}{500} - \frac{1000}{2000}=0.5$. Finally the upper bound of $\Importance(C)$ would be $2 / (\frac{1}{0.5}+\frac{1}{\log{1000}/\log{1000}}) = 0.666\dots$.

All upper- and lower-bound estimates are conservative.  Pruning complex predicates using the above calculations reduces the computational intensity of the analysis without affecting the results.  The effectiveness of pruning in practice is examined in \autoref{sec-effectprune}.

% LocalWords:  ccc tvl eqn disj effectprune
