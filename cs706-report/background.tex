% -*- TeX-master: "master" -*-

\section{Background}
\label{sec-bground}
CBI uses lightweight instrumentation to collect feedback reports that contain truth values of predicates in an execution as well as the outcome (e.g., crash or non-crash) of the execution.  A large number of these reports are collected and analyzed using statistical debugging techniques.  These techniques identify \emph{bug predictors}: predicates that, when true, herald failure due to a specific bug.  Bug predictors indicate areas of the code related to program failure and so provide information useful when correcting program faults.  Feedback reports can be collected from deployed software in the hands of end users, who may encounter bugs not identified in program testing.  CBI can therefore be used to monitor software after its release and help direct program patches by identifying bugs as they manifest in the field.

\subsection{Finding Bug Predictors}
\label{sec-elimalg}
The feedback report for a particular program execution is formed as a bit-vector, with two bits for each predicate (observed and true), and one final bit representing success or failure.  If generated in experimental or testing conditions these feedback reports are likely to be complete; when instrumented code is distributed to end users predicates are usually sampled infrequently to reduce computational overhead.  Previous experiments \cite{Liblit:2003:BIRPS} have determined that sampling rates of \nicefrac{1}{100} to \nicefrac{1}{1000} are most realistic for deployed use.

Using these reports, CBI analysis identifies from all available predicates the single best predictor of program failure (this step is described in detail in \autoref{sec-scoring}).  It is assumed that this predictor corresponds to one important bug, though other bugs may remain.    This top predictor is recorded, and then all feedback reports where it was true are removed from consideration on the assumption that fixing the corresponding bug will change the behavior of runs in which the predictor originally appeared.  The next best predictor among the remaining reports is then identified, recorded, and removed in the same manner.  This iterative process repeats until either no undiagnosed failed runs remain, or no more failure-predictive predicates can be found.

This process of iterative elimination maps each predictor to a set of program runs.  Ideally each such set corresponds to the expression of a distinct bug; unfortunately this is not always the case.  Due to the statistical nature of the analysis, along with incomplete feedback reports resulting from sparse sampling rates, a single bug could be predicted by several top-ranked predicates, and predictors for less prevalent bugs may not be found at all.

The output of the analysis is a list of predicates that had the highest score during each iteration of the elimination algorithm, as well as a complete list of predicates and their scores before any elimination is performed.  These lists may be used by a programmer to identify areas of the program related to faulty behavior.  Liblit et al.\ employed this method to discover previously unknown bugs in several widely-used applications \cite{Liblit:2003:BIRPS,Liblit:2005:SSBI}.

CBI output can alternately be used as input to an automated analysis tool, such as \textsc{BTrace} \cite{Lal:2006:POPAD}. \textsc{BTrace} finds the shortest control- and dataflow-feasible path in the program that visits a given set of bug predictors.  This analysis allows a programmer to examine the fault-predicting behavior even if the connection to a bug is not easily identifiable, or if the predictors are numerous or complex enough to overwhelm a programmer examining them directly.

\subsection{Scoring Predicates}
\label{sec-scoring}
Identifying the best predictor from a set of predicates is a difficult problem.  A good predictor should be both \emph{sensitive} and \emph{specific}: it should account for many failed runs, without mis-predicting failure in successful ones.  Assigning scores based on sensitivity will result in \emph{super-bug predictors}, which include failures from more than one bug.  Super-bug predictors are highly non-deterministic, since they are not specific to any single cause of failure, and rarely provide useful debugging information.  Scoring predicates based on specificity instead results in \emph{sub-bug predictors}.  A sub-bug predictor accounts for a portion of the failures caused by a bug, but not all.  Unlike super-bug predictors, sub-bug predictors that account for a significant sub-set of failures can be useful in debugging, although perfect predictors are of course preferred.  To balance sensitivity and specificity Liblit et al.\ \cite{Liblit:2005:SSBI} compute a numeric $\Importance$ score corresponding to each predicate, defined as follows.

The truth values of a predicate $p$ from all the runs can be aggregated into four values:

\begin{enumerate}
\item $\obs{S}{p}$ and $\obs{F}{p}$, respectively the number of successful and failed runs in which the value of $p$ was evaluated.
\item $S(p)$ and $F(p)$, respectively the number of successful and failed runs in which the value of $p$ was evaluated and was found to be true.
\end{enumerate}

Using these values, two scores of bug relevance are calculated:
\begin{description}
\item[Sensitivity:] $\log{(F(p))} / \log{(\NumF)}$ where $\NumF$ is the total number of failing runs.  A good predictor must predict a large number of failing runs.
\item[Specificity:] $\Increase(p)$.  The amount by which $p$ being true increases the probability of failure over simply reaching the line where $p$ is defined.  It is computed as follows:

  \begin{equation}
    \label{eqn1}
    \Increase(p) \equiv
    \frac{F(p)}{S(p) + F(p)}
    -
    \frac{\obs{F}{p}}{\obs{S}{p} + \obs{F}{p}}
  \end{equation}

\end{description}

Taking the harmonic mean combines these two scores, selecting out predicates that are both highly sensitive and highly specific:
\begin{equation}
\label{eqn2}
\Importance(p) \equiv
\frac{2}{%
  \frac{1}{\Increase(p)}
  +
  \frac{1}{log(F(p)) / log(\NumF)}}
\end{equation}

The $\Importance$ score is calculated for each predicate, and the top result selected.  After all runs in which the top predicate is true are eliminated scores are recalculated for all remaining predicates in the remaining set of runs.  This process of eliminating runs continues, as described above, until there are no remaining failed runs or no remaining predicates.

\subsection{Expected Benefits of Complex Predicates}
A single predicate can be thought of as partitioning the space of all runs into two subspaces: those satisfying the predicate and those not.  The more closely these partitions match the subspaces where the bug is and is not expressed, the better the predicate is as a bug predictor.  If a bug has a simple cause, and this cause corresponds well to a simple predicate, then a simple analysis is sufficient.  For bugs with more complex causes no perfect predictor will be available among the simple predicates measured, and traditional analysis will produce only super- and sub-bug predictors.

A richer language of predicates can describe more complex shapes within the space of runs.  This allows good predictors for bugs with more complicated causes.  Some bugs may have causes connected to simple predicates, but that no single predicate can accurately predict.  Complex predicates formed from these simpler ones would be more accurate predictors than any component predicate.  Sub-bug predictors and \emph{partial predictors} are two classes of simple predicates that can be combined into more accurate predictors.

A sub-bug predictor is one that correctly partitions some (but not all) expressions of the bug.  In information-retrieval terms, a sub-bug predictor has high precision but low recall.  Sub-bug predictors are useful in identifying a bug because, though they do not predict the bug in a general sense, they are extremely good predictors of some special case where the bug is expressed.  Combining two such predictors with a disjunction will reduce false negatives and result in a predicate that correctly partitions more manifestations of the bug; combine enough special cases in this manner and the resulting predicate will predict the bug in the general case.  It is important to note that the analysis may find a disjunction of predictors of individual bugs as a predictor for the whole set of failures.  The user should keep this in mind while using a disjoined predictor to track down a bug, but it is not as problematic as it seems: for such a disjunction to be high-ranked each component predicate must be a good predictor for a specific bug, providing useful information on all bugs involved.

Partial predictors predict some aspect of a bug that is necessary, but not sufficient, for program failure.  A partial predictor will correctly partition all (or most) expressions of the bug, but would also predict the bug in a large number of runs where it did not occur.  In information-retrieval terms, a partial predictor has high recall but low precision.  Because partial predictors are highly non-deterministic with respect to the bug, they are likely to be outscored by a sub- or super-bug predictor.  Partial predictors can be improved by eliminating false positives - this can be accomplished by taking a conjunction with another predicate that captures another aspect of the bug; the resulting partitioning would more closely match the bug's behavior.  The case study presented in \autoref{sec-exif} describes a bug best predicted by a conjunction involving a partial predictor.

The bug predictors that result from combining simple predicates can be conjoined or disjoined again, eliminating false positives and false negatives to approach a perfect predictor.  This process can continue, eventually finding a good predictor for any bug that can be expressed in terms of the simple predicates measured during the construction of the feedback reports.  Even if some aspect of the bug is uncovered by the simple predicates, it's likely that a sub-bug predictor may still be constructed.  The introduction of complex predicates to CBI analysis greatly increases the number of shapes that can be described within the set of runs, thereby increasing the chances of finding an accurate predictor for a bug.
