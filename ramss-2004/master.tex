\documentclass[10pt,twocolumn]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{ieee}
\usepackage{nicefrac}
\usepackage{url}
\usepackage{xspace}

\usepackage[bookmarks, pdftitle={Public Deployment of Cooperative Bug
  Isolation}, pdfauthor={Ben Liblit, Mayur Naik, Alice X. Zheng, Alex
  Aiken, and Michael I. Jordan}, pdfpagemode=UseOutlines]{hyperref}

\ifthenelse{\isundefined{\pdfoutput}}{}{\usepackage{thumbpdf}}

\newcommand{\evolution}{\texttt{evolution}\xspace}
\newcommand{\gaim}{\texttt{gaim}\xspace}
\newcommand{\gimp}{\texttt{gimp}\xspace}
\newcommand{\gnumeric}{\texttt{gnumeric}\xspace}
\newcommand{\nautilus}{\texttt{nautilus}\xspace}
\newcommand{\rhythmbox}{\texttt{rhythmbox}\xspace}

\pagestyle{empty}

\begin{document}

\title{Public Deployment of Cooperative Bug Isolation}

\makeatletter
\newcommand*{\eecsMark}[0]{\@fnsymbol{2}}
\newcommand*{\statMark}[0]{\@fnsymbol{3}}
\newcommand*{\stanMark}[0]{\@fnsymbol{4}}
\makeatother
\newcommand*{\eecs}[0]{\textsuperscript{\eecsMark}}
\newcommand*{\stat}[0]{\textsuperscript{\statMark}}
\newcommand*{\both}[0]{\textsuperscript{\eecsMark, \statMark}}
\newcommand*{\stan}[0]{\textsuperscript{\stanMark}}

\newcommand{\aupar}{\end{tabular} \\ \begin{tabular}[t]{c}}

\author{%
  Ben Liblit \eecs \and
  Mayur Naik \stan \and
  Alice X.\ Zheng \eecs
  %%
  \aupar
  %%
  Alex Aiken \stan \and
  Michael I.\ Jordan \both
  %%
  \aupar
  %%
  \eecs Department of Electrical \\ Engineering and Computer Science \\
  \stat Department of Statistics \\
  University of California, Berkeley \\
  Berkeley, CA 947201776
  \and
  \stan Computer Science Department \\
  353 Serra Mall \\
  Stanford University \\
  Stanford CA 943059025
}

\maketitle

\thispagestyle{empty}

\begin{abstract}
As part of our work on Cooperative Bug Isolation (CBI) we have undertaken to 
instrument and distribute binaries for a number of large open source projects.
This
public deployment is an important  step towards a large experiment involving (we hope) hundreds or thousands of users that will measure the effectiveness of CBI\@.
This paper describes several of the significant engineering issues that arise in 
instrumenting the source code of realistic applications.  
More information on CBI can be found at \cite{Liblit:CBIP}.
\end{abstract}

\section{Introduction}

Cooperative Bug Isolation (CBI) seeks to leverage the
huge amount of computation done by the end users of software.  By
gathering a little bit of information from every execution of a
program performed by its user community, we should be able to make
inferences automatically about the causes of bugs experienced in the field.

Our approach to CBI is based upon compile-time instrumentation of the
program's source code.  We insert instrumentation into the program to
test a large number of predicates on program values during
execution and to count how many times each predicate is observed to be
true or false.  On termination of the program the list of predicate
counters is uploaded to a central server together with a record of
whether the program terminated successfully or not.  Subsequent
statistical analysis of which predicates are correlated with program
failure can then indicate to engineers which values and what parts of
the program are the sources of crashes; in at least some cases even
the exact line of code that is at fault can be identified \cite{PLDI`03*141,Liblit:2003:SUEBI,Zheng:2003:SDSP}.

We believe that CBI and related research efforts have great potential
to make software development more responsive and efficient by giving
developers accurate data about how software is actually used in
deployment.  However, testing this idea requires significant
experimentation with real, and preferably large, user communities
using real applications.

This paper reports on our experience in preparing for just such an
experiment.  We have instrumented a number of large open source
applications, listed in \autoref{apps}, with a total of about 1.8
million lines of code. We have made these instrumented programs
available to the public and are in the process of collecting feedback
reports.  As a result, we have demonstrated a complete CBI system and
feel comfortable in claiming that our approach is technically
feasible; while aspects of our system could certainly be improved, at
this point all components are good enough to support the deployment of
realistic instrumented applications and the collection of feedback
reports from a large user community.

There are both interesting technical and social problems involved in
the design of a CBI system.  In this paper, we focus on the solutions
to technical problems most likely to be useful to the designers of
similar systems and experiments: dealing with shared libraries,
plugins, and threads.  We also briefly discuss how users interact with
our system, as well as give some static and dynamic measures of the
applications we instrument.

\begin{table*}
  \centering
  \begin{tabular}{lrcccc}
    Application & Lines of Code & Shared Libraries & Plugins    & Threads \\\hline
    \evolution  & 460,912       & \checkmark       & \checkmark & \checkmark \\
    \gaim       & 160,435       &                  & \checkmark & \\
    \gimp       & 650,660       & \checkmark       & \checkmark & \\
    \gnumeric   & 319,137       &                  & \checkmark & \\
    \nautilus   & 124,679       & \checkmark       & \checkmark & \checkmark \\
    \rhythmbox  &  56,442       & \checkmark       &            &
  \end{tabular}
  \caption{Instrumented applications.}
  \label{apps}
\end{table*}


\section{Libraries and Plugins}

Designing a feedback system for CBI is easy for an application that consists of
a single object file.  We simply write out the predicate counters in the
order in which they appear in that file, and that list constitutes a complete
report.

However, as can be seen in \autoref{apps}, most applications involve
multiple object files in the form of shared libraries, plugins, or
both.  Note that this table counts only those shared libraries and
plugins that are part of the source code of the application;
additionally, there may be (and generally will be) other shared
libraries and plugins that are resident only on the end user's
machine.  Thus, the running environment will be a mix of code that has
been instrumented by CBI and code that we have never seen before.
Shared libraries are also interesting because they may be used by
other applications that we have not instrumented.  Thus, not only must
instrumented applications cope with uninstrumented code, but
instrumented code must cope with finding itself in an
uninstrumented application.

An orthogonal set of problems arises from static linking and dynamic
loading.  Our system does not have control over the linker and
cannot assume that object files appear in any particular order.
Plugins may be loaded late and unloaded at any time.  If an
instrumented plugin is about to be unloaded, we must capture its part
of the feedback report immediately, because once it is unloaded its
global predicate counters vanish from the address space and can no
longer be accessed.

Our solution to all of these problems is to make each object file
self-managing, with some initialization code that runs when it is
loaded, and finalization code that runs when it is unloaded.  For
objects which are part of the main application binary, the
initialization code runs early in program execution, before
\texttt{main()}.  The finalization code runs after \texttt{main()}
returns or after \texttt{exit()} is called.  Shared libraries are
similar.  For plugins, the initialization code runs within
\texttt{dlopen()} after the plugin has been mapped into memory.
Plugin finalization code runs within \texttt{dlclose()} just before
the plugin is removed from memory.  Each object file also maintains
its own instrumentation state; in particular, each object file maintains
its own predicate counters.

There is one situation in which we need global knowledge of the loaded
object files.  If the program crashes, we must gather
the predicate counters from each loaded object file for the feedback report.
We maintain a doubly-linked list of loaded object files, and the
initialization/finalization code for each object file adds/removes that
file from this list. The initialization and finalization code uses
facilities provided by the ELF executable format; these are the same
facilities used by C++ code to run constructors and destructors on
global variables.

We have also given some attention to the fact that this global
registry could itself be corrupted by a buggy program.  We maintain a
global counter of the expected size of the global registry.  When
walking the list in a signal handler, we use the counter to decide
when to stop even if we have not reached the end of the list data
structure.  This prevents an infinite loop if a memory error in the
application introduces a cycle into the doubly-linked list.  The
global registry can be damaged in other ways by a misbehaving program,
of course, but avoiding cycles is the most important case to handle.

The complications in checking for a corrupted global registry are just
an example of the general problem that it is not possible to
completely isolate program instrumentation from the program itself in
unsafe languages such as C and C++.  As a result it is necessary to
sanity check feedback reports at the central server and discard any
that are ill-formed.  In practice we do receive ill-formed reports,
but the number is only a tiny fraction of all reports.

\section{Threads}

Our CBI system maintains three kinds of global data that 
need special attention in multithreaded
applications.  In each case thread-safety can hurt performance, 
so we only take this extra care if in fact the application is multithreaded
(i.e., the compiler's command line contains the GCC \texttt{-pthread} flag).

A key feature of our CBI system is that performance can be improved by
sampling the instrumentation code, which is implemented by frequently
skipping over some instrumentation and instead executing a ``fast
path'' with no instrumentation at all.  We use a global \emph{countdown}
to determine how many instrumentation sites to skip before
testing one predicate and recording its result.\footnote{The countdown
is chosen randomly from a geometric distribution with a mean that is
the desired sampling rate.  This is equivalent to, but much more
efficient than, flipping a coin at each instrumentation site to decide
whether to take a sample.}  Thus, in a multi-threaded system, the
global variable holding the next-sample countdown could be a source of
high contention among threads.

The simple solution to this problem is to give each thread its own,
independent countdown variable.  The behavior of the system with per-thread
countdowns is indistinguishable from having a single global countdown,
and in addition we avoid all locking.

Enacting this plan requires compiler support.  We use the
\texttt{\_\_thread} storage qualifier to declare thread-specific
storage.  This is a GCC extension and also requires support from the
POSIX threading runtime, C library, and even the runtime loader.  
We also must alter thread creation so we can initialize the
new thread's global state.  We use the \texttt{--wrap} flag provided
by the GNU linker to replace \texttt{pthread\_create()} with our own
special version.  

The second class of data that must be handled specially in
multi-threaded applications is the predicate counters.  Recall that
the predicate counters keep track of how often a particular predicate
at a particular line of code is observed to be true or false.
Because, for efficiency, we use low sampling rates (say on average
once in every 100 times the line of code associated with the predicate
is executed), predicates are tested rarely and any individual counter
is accessed rarely even by a single thread.  For this reason, we
maintain only one copy of each predicate counter, shared by all
threads.  The critical operation on these counters, an increment by
one, is so basic that every CPU architecture has some way of doing
this atomically without resorting to explicit locking.

Finally, the third class of data that must be protected from concurrent access
is the global registry of compilation units and the report file.
Because these structures are accessed very rarely, we guarantee single 
access by guarding each with its own mutex lock.


\section{User Interaction}

When a user launches an instrumented application, he does not run the
instrumented binary directly.  Instead, we install a wrapper script in
the expected location (e.g. \texttt{/usr/bin/nautilus}) and put the
instrumented binary elsewhere.  The wrapper script has several
responsibilities:  it performs all user interaction
that goes beyond what the underlying application would normally do, and
it collects the raw feedback report from the instrumented
application, packages it for transit, and sends it to the report
collection server along with other information such as program
outcome.  In this way we avoid adding GUI infrastructure and
encrypted networking support to the applications themselves.
Also, the script can be in a different language (e.g., Python)
which has excellent networking and desktop interaction support.

When the wrapper script starts up, it checks whether the user has run
an instrumented version of this application before.  If not, it
presents a first-time opt-in dialog.  The dialog briefly describes the
goals of the project and the consequences of participating or not, and
lets the user decide what to do.  There is also a hyperlink button to
our web site for more information.  This dialog is presented in the
background, however, and the real application launched without waiting
for a reply.  On this first run, the application 
reports no data.  
Once the user has selected yes or no, that preference is
remembered and the first-time opt-in dialog is not shown on subsequent
runs (though it is possible to change the preference later; see below).

Also in the background, the wrapper presents a small status icon in
the GNOME desktop panel notification area.  This icon provides a
visual reminder that an instrumented application is currently running.
It provides a simple popup menu with a toggle to globally disable or
enable sampling.  A second menu item launches the sampler control
panel.  The status icon changes depending on whether sampling is
enabled or disabled globally, and remains present as long as at least
one instrumented application is running.  

Because the wrapper script launches the instrumented binary as a
subprocess, it can also check that subprocess's exit status (either a
result code or a fatal signal), which is included in the report
uploaded to our feedback collection server.
The wrapper script compresses the raw feedback report for transit
using \texttt{gzip}-compatible compression.  This is a huge benefit, as reports
are mostly zeros and compress very well.  The average compression
of the reports we have received is 96\%; \autoref{report-sizes} shows
the range of report sizes we have received by application.  
The largest reports are less than 40K bytes, which can be uploaded
over even a slow modem connection in seconds.

Before submitting a report, the wrapper checks once more whether
sampling is enabled both globally and for this application.  If the
user changed his mind after program launch, this gives a second chance
for stopping an unwanted feedback report from going out.

Because we transmit reports using HTTP requests, every submission
can also have a response from the server.
Ordinarily the collection server does not give any response beyond a
success code.  However, if the server does give a response, the
wrapper script receives it and presents it to the user as an HTML
page.  This feature might be used, for example, if a critical security
issue were found requiring immediate upgrades.  
The HTTP reply can also include a few special reply headers which
update the local sampling configuration on the client.  We have the
ability to promote a different destination URL for future reports,
which may be useful if we need to relocate the collection server.  We
can change the sampling density from its default of $\nicefrac{1}{100}$, which may be
useful if performance problems arise.  We can also issue a ``poison
pill'' which turns off sampling for future runs of the application.
This is intended as a shutoff should the Cooperative Bug
Isolation project be discontinued at some future date (a feature we
learned would be useful from the prior experience of \cite{Elbaum:2003:DISATA}), and it might also
be used to quash future reports from individual misbehaving users.  So
far we have not needed any of these facilities.  

\begin{table}
  \centering
  \begin{tabular}{lrr}
    Application & Min & Max  \\ \hline
    \evolution & 830 & 39863  \\
    \gaim & 1786 & 18772  \\
    \gimp & 15617 & 26304  \\
    \gnumeric & 6661 & 15876  \\
    \nautilus & 3252 & 10638  \\
    \rhythmbox & 897 & 5823 
  \end{tabular}
  \caption{Feedback report sizes in bytes.}
  \label{report-sizes}
\end{table}

\section{Status of the Public Deployment}

We conclude with some discussion of our experience thus far with our public
deployment of the applications listed in \autoref{apps}.

One concern we have is that our approach adds a great deal of new code
to an application; in fact, binaries will often be at least twice as
large as the original, uninstrumented program.  However, the growth in
disk footprint is considerably smaller if one considers the entire
package that comes with a typical large application, and in fact the
executable code is often a relatively small percentage of the total
distribution.  For the applications we have instrumented, downloaded
packages are between 13\% and 49\% larger and the installed footprint
on disk grows between 13\% and 71\%.  The actual application binaries
are between 74\% and 304\% larger than in the original distribution.
Thus far we have received no complaints about package sizes, either
downloaded or as expanded onto disk.

Another potential issue is application performance, but thus far we
have received no complaints about the performance of any of our
instrumented applications.  We use \nicefrac{1}{100} sampling, which
apparently is sparse enough; we probably could have sampled even more
densely for these interactive applications which spend most of their
time waiting for the user to do something.  Note however that 
these applications do have CPU-intensive phases, such as when
\rhythmbox is loading up a library with thousands of music files or
when \gnumeric is recalculating a very large spreadsheet.

Finally, \autoref{reports-per-app} summarizes the current state of
the data for each of our instrumented applications.  In this table,
\emph{count} is the total number of valid feedback reports received so far,
which is broken out into \emph{good} runs, runs that exited with a
non-zero \emph{error} status, and runs that ended in a \emph{crash} due to a
fatal signal.  Applications are ordered by decreasing crash rate;
\rhythmbox crashes 13\% of the time.

There is both good news and bad news in these figures.  The bad news
is that we have not yet received enough executions to carry out
statistically significant analysis of the results.  Based on our
previous experience with studies done ``in the lab'' running
applications on synthetic data to simulate a large user community,
with $\nicefrac{1}{100}$ sampling we need between 10,000 and 20,000
executions with our current methods to achieve accurate analysis of
the results, which is more than ten times the number of runs we have
received to date for any of these applications.\footnote{We began
collecting data from the public distribution in October, 2003.}  Our
situation here reflects an inherent aspect of CBI and similar
approaches, which is that these methods work well only beyond a
certain minimum scale.

The good news is that these applications do crash,
indicating to us that there is potential to improve the state
of the software given enough users participating in CBI.  In addition,
we have enough data to demonstrate that the complete system works, from
instrumenting code through gathering of reports, and we continue to
receive new feedback reports daily.  We are only at the beginning of this
experiment and have not yet invested much effort in attracting users. 
The next step in our experiment will be
to find ways to recruit enough users to test the advantages of CBI
for large user communities of complex applications.


\begin{table}
  \centering
  \begin{tabular}{lrrrrr}
    Application & Count & Good & Error & Crash & Crash Rate \\ \hline
    \rhythmbox & 726 & 598 & 36 & 92 & 0.13 \\
    \gnumeric & 247 & 228 & 2 & 17 & 0.07 \\
    \gaim & 464 & 386 & 62 & 16 & 0.03 \\
    \evolution & 702 & 647 & 39 & 16 & 0.02 \\
    \nautilus & 468 & 426 & 36 & 6 & 0.01 \\
    \gimp & 77 & 77 & 0 & 0 & 0.00
  \end{tabular}
  \caption{Number of runs received.}  
  \label{reports-per-app}
\end{table}

\bibliography{personal,pldi03,ramss}

\end{document}
