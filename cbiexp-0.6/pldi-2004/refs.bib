%%%     author    = "David Coppit",
%%%     author    = "David Coppit",
%%%     date      = "14 November 1996",
%%%     filename  = "refs.bib",
%%%     address   = "Department of Computer Science
%%%                Olsson Hall
%%%                University of Virginia
%%%                Charlottesville, VA 22901
%%%                USA",
%%%     email     = "david at coppit.org",
%%%     keywords  = "software engineering",
%%%     supported = "yes",
%%%     abstract  = "This BibTeX file contains various software
%%%                engineering related resources that I've
%%%                collected. Resources that I have are
%%%                numbered, and my comments appear to the
%%%                right of the numbers. Almost every entry
%%%                has an abstract."
%%%  }
-------------------------------------------------------------------------------

Entry Prototypes:

PhdThesis{,
 key          = "",
 author       = "",
 title        = "",
 address      = "",
 school       = "",
 month        = "",
 year         = "",
 abstract     = "
",
 note         = "",
}

InProceedings{,
 key          = "",
 author       = "",
 title        = "",
 booktitle    = "",
 address      = "",
 month        = "",
 year         = "",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "",
 abstract     = "
",
 note         = "",
}

Article{,
 key     = "",
 author  = "",
 title   = "",
 journal = "",
 month   = "",
 year    = "",
 volume  = "",
 number  = "",
 pages   = "",
 abstract = "
",
 note    = "",
}

Incollection{,
 key       = "",
 author    = "",
 title     = "",
 booktitle = "",
 pages     = "",
 publisher = "",
 editor    = "",
 month     = "",
 year      = "",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
",
 note      = "",
}

Techreport{,
 key         = "",
 author      = "",
 title       = "",
 institution = "",
 month       = "",
 year        = "",
 number      = "",
 address     = "",
 type        = "",
 abstract = "
",
 note        = "",
}

Misc{,
 key    = "",
 author = "",
 title  = "",
 url    = "",
 abstract = "
",
 note   = "URL: {\urlBiBTeX{}}",
}

Book{,
 key       = "",
 author    = "",
 title     = "",
 publisher = "",
 edition   = "",
 month     = "",
 year      = "",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

Inbook{,
 key          = "",
 author       = "",
 title        = "",
 chapter      = "",
 pages        = "",
 publisher    = "",
 year         = "",
 volume       = "",
 series       = "",
 address      = "",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}


------------------------------------------------------------------------------

% Journal abbreviations:
@String{j-cacm  = "Communications of the ACM"}
@String{j-acmcs = "ACM Computing Surveys"}
@String{j-tse   = "IEEE Transactions on Software Engineering"}
@String{j-tsc   = "IEEE Transactions on Computers"}
@String{j-tsem  = "ACM Transactions on Software Engineering and Methodology"}
@String{j-soft  = "IEEE Software"}
@String{j-sig   = "SIGSOFT Software Engineering Notes"}
@String{j-comp  = "IEEE Computer"}
@String{j-jss   = "Journal of Systems and Software"}
@String{j-spe   = "Software Practice and Experience"}

-------------------------------------------------------------------------------

@Book{boehm81a,
 key       = "boehm81a",
 author    = "Barry W. Boehm",
 title     = "Software Engineering Economics",
 publisher = "Englewood Cliffs, New Jersey: Prentice Hall",
 edition   = "",
 month     = "",
 year      = "1981",
 volume    = "",
 series    = "",
 address   = "",
 abstract = "
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{spivey92a,
 key       = "spivey92a",
 author    = "J. M. Spivey",
 title     = "The {Z} Notation: A Reference Manual",
 publisher = "Prentice Hall International Series in Computer Science",
 edition   = "2nd",
 month     = "",
 year      = "1992",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@InProceedings{gorlick94a,
 key          = "gorlick94a",
 author       = "Michael Gorlick and Alex Quilici",
 title        = "Visual Programming-in-the-Large vs. Visual
               Programming-in-the-Small",
 booktitle    = "Proceedings of the 1994 IEEE Symposium on Visual Languages",
 address      = "St. Louis, MO",
 month        = "4--7  " # oct,
 year         = "1994",
 pages        = "137--44",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Visual programming research has largely focused on the issues of visual
programming-in-the-small. However, entirely different concerns arise when one
is programming-in-the-large. We present a visual software engineering
environment that allows users to construct visually programs consisting of
hierarchically organized networks of components that process streams of
arbitrary objects. We discuss the problems that occur when trying to construct
systems consisting of thousands of interconnected components, examine how this
environment deals with some of the problems specific to visual
programming-in-the-large, and show why our initial solutions failed to scale
successfully.  Finally, we argue that a single visual mechanism called
'zooming' addresses these scaling problems and, when suitably augmented, can
also support automatic component discovery and intelligent error correction
(21 Refs.)",
 note         = "",
}

------------------------------------------------------------------------------

1

POP reference, icse18.ps

@InProceedings{sullivan95a,
 key          = "sullivan95a",
 author       = "K. J. Sullivan and J. C. Knight",
 title        = "Experience Assessing an Architectural Approach to Large-scale
                 Systematic Reuse",
 booktitle    = "Proceedings of the 18th International Conference on Software
                 Engineering",
 address      = "Berlin, Germany ",
 month        = "25--30  " # mar,
 year         = "1996",
 pages        = "220--229",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Systematic reuse of large-scale software components promises rapid, low cost
development of high-quality software through the straightforward integration of
existing software assets. To date this promise remains largely unrealized,
owing to technical, managerial, cultural, and legal barriers. One important
technical barrier is architectural mismatch. Recently, several component
integration architectures have been developed that purport to promote
large-scale reuse. Microsoft's OLE technology and associated applications are
representative of this trend. To understand the potential of these
architectures to enable large-scale reuse, we evaluated OLE by using it to
develop a novel fault-tree analysis tool.  Although difficulties remain, the
approach appears to overcome architectural impediments that have hindered some
previous large-scale reuse attempts, to be practical for use in many domains,
and to represent significant progress towards realizing the promise of
large-scale systematic reuse (16 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

2

@Article{hatton94a,
 key     = "hatton94a",
 author  = "Les Hatton and Andy Roberts",
 title   = "How Accurate is Scientific Software?",
 journal = j-tse,
 month   = "",
 year    = "1994",
 volume  = "2",
 number  = "10",
 pages   = "785--797",
 abstract = "
This paper describes some results of what, to the authors' knowledge, is the
largest N-version programming experiment ever performed. The object of this
ongoing four-year study is to attempt to determine just how consistent the
results of scientific computation really are, and, from this, to estimate
accuracy. The experiment is being carried out in a branch of the earth sciences
known as seismic data processing, where 15 or so independently developed large
commercial packages that implement mathematical algorithms from the same or
similar published specifications in the same programming language (Fortran)
have been developed over the last 20 years. The results of processing the same
input dataset, using the same user-specified parameters, for nine of these
packages is reported in this paper. Finally, feedback of obvious flaws was
attempted to reduce the overall disagreement. The results are deeply
disturbing.  Whereas scientists like to think that their code is accurate to
the precision of the arithmetic used, in this study, numerical disagreement
grows at around the rate of 1% in average absolute difference per 4000 fines of
implemented code, and, even worse, the nature of the disagreement is nonrandom.
Furthermore, the seismic data processing industry has better than average
quality standards for its software development with both identifiable quality
assurance functions and substantial test datasets (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

3

@TechReport{murphy95a,
 key         = "murphy95a",
 author      = "Gail C. Murphy and David Notkin and Kevin Sullivan",
 title       = "Software Reflexion Models: Bridging the Gap Between Source
              and High-Level Models",
 institution = "University of Washington",
 month       = "",
 year        = "1995",
 number      = "UW-CSE-TR-95-03-02",
 address     = "Seattle, WA",
 type        = "",
 abstract = "
Software engineers often use high-level models (for instance, box and arrow
sketches) to reason and communicate about an existing software system.  One
problem with high-level models is that they are almost always inaccurate with
respect to the system's source code.  We have developed an approach that helps
an engineer use a high-level model of the structure of an existing software
system as a lens through which to see a model of that system's source code. In
particular, an engineer defines a high-level model and specifies how the model
maps to the source. A tool then computes a software reflexion model that shows
where the engineer's high-level model agrees with and where it differs from a
model of the source. The paper provides a formal characterization of reflexion
models, discusses practical aspects of the approach, and relates experiences of
applying the approach and tools to a number of different systems, The
illustrative example used in the paper describes the application of reflexion
models to NetBSD, an implementation of Unix comprised of 250,000 lines of C
code. In only a few hours, an engineer computed several reflexion models that
provided him with a useful, global overview of the structure of the NetBSD
virtual memory subsystem. The approach has also been applied to aid in the
understanding and experimental reengineering of the Microsoft Excel spreadsheet
product (17 Refs.)
",
 note        = "",
}

------------------------------------------------------------------------------

4

@InProceedings{murphy96a,
 key          = "murphy96a",
 author       = "Gail C. Murphy and David Notkin and Erica S.-C. Lan",
 title        = "An Empirical Study of Static Call Graph Extractors",
 booktitle    = "Proceedings of the 18th International Conference on Software
                 Engineering",
 address      = "Berlin, Germany ",
 month        = "25--30  " # mar,
 year         = "1996",
 pages        = "90--98",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Informally, a call graph represents calls between entities in a given program.
The call graphs that compilers compute to determine the applicability of an
optimization must typically be conservative: a call may be omitted only if it
can never occur an any execution of the program. Numerous software engineering
tools also extract call graphs, with the expectation that they will help
software engineers increase their understanding of a program. The requirements
placed on software engineering tools when computing call graphs are typically
more related than for compilers. For example, some false negatives-calls that
can in fact take place in some execution of the program, but which are omitted
from the call graph-may be acceptable, depending on the understanding task at
hand. In this paper we empirically show a consequence of this spectrum of
requirements by comparing the C call graphs extracted from three software
systems (mapmaker, mosaic, and gee) by five extraction tools (cflow, CIA,
Field, mk-functmap, and rigiparse). A quantitative analysis of the call graphs
extracted for each system shows considerable variation, a result that is
counterintuitive to many experienced software engineers. A qualitative analysis
of these results reveals a number of reasons for this variation:  differing
treatments of macros, function pointers, input formats, etc.  We describe and
discuss the study, sketch the design space, and discuss the impact of our study
on practitioners, tool developers, and researchers (17 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

5

@Article{batory94a,
 key     = "batory94a",
 author  = "Don Batory and Vivek Singhal and Jeff Thomas and Sankar
         Dasari and Bart Geraci and Marty Sirkin",
 title   = "The GenVoca Model of Software-System Generators",
 journal = j-soft,
 month   = sep,
 year    = "1994",
 volume  = "11",
 number  = "5",
 pages   = "89--94",
 abstract = "
An emerging breed of generators synthesize complex software systems from
libraries of reusable components. These generators, called GenVoca generators,
produce high-performance software and offer substantial increases in
productivity. GenVoca is a blueprint for creating domain-specific software
component technologies. Predator and Adage are based directly on GenVoca.
Experimental evidence from the Predator project supports claims of performance
and productivity with this model (7 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

6 (Well-known. Also in _Programming methodology. A collection of articles by
members of IFIP._)

@Article{parnas76a,
 key     = "parnas76a",
 author  = "D. L. Parnas",
 title   = "On the Design and Development of Program Families",
 journal = j-tse,
 month   = mar,
 year    = "1976",
 volume  = "SE-2",
 number  = "1",
 pages   = "1--9",
 abstract = "
Program families are defined (analogously to hardware families) as sets of
programs whose common properties are so extensive that is is advantageous to
study the common properties of the programs before analyzing individual
members. The assumption that, if one is to develop a set of similar programs
over a period of time, one should consider the set as a whole while developing
the first three approaches to the development, is discussed. A conventional
approach called `sequential development' is compared to `stepwise refinement'
and `specification of information hiding modules'. A more detailed comparison
of the two methods is then made. By means of several examples it is
demonstrated that the two methods are based on the same concepts but bring
complementary advantages (16 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

7

@Article{habermann78a,
 key     = "habermann78a",
 author  = "A. N. Habermann",
 title   = "Modularization and Hierarchy in a Family of Operating
         Systems",
 journal = j-cacm,
 month   = may,
 year    = "1976",
 pages   = "266--72",
 abstract = "
This paper describes the design philosophy used in the construction of a family
of operating systems. It is shown that the concepts of module and level do not
coincide in a hierarchy of functions. Family members can share much software as
a result of the implementation of run-time modules at the lowest system level.
(9 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

8 (Excellent)

@Article{brooks87a,
 key     = "brooks87a",
 author  = "Fred Brooks",
 title   = "No Silver Bullet: Essence and Accidents of Software Engineering",
 journal = j-comp,
 month   = apr,
 year    = "1987",
 volume  = "20",
 number  = "4",
 pages   = "10--19",
 abstract = "
The author considers the reasons why there is no single development, in either
technology or in management technique, that by itself promises even one
order-of-magnitude improvement in productivity, reliability, and simplicity of
software. He does this by examining both the nature of the software problem and
the properties of the solutions posed (12 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

9 (Very good)

@Article{boehm76a,
 key     = "boehm76a",
 author  = "B. W. Boehm",
 title   = "Software Engineering",
 journal = j-tsc,
 month   = dec,
 year    = "1976",
 volume  = "",
 number  = "",
 pages   = "1266--41",
 abstract = "
Provides a definition of the term `software engineering' and a survey of the
current state of the art and likely future trends in the field. The survey
covers the technology available in the various phases of the software life
cycle-requirements engineering, design, coding, test and maintenance-and in the
overall area of software management and integrated technology-management
approaches. It is oriented primarily toward discussing the domain of
applicability of techniques (where and when they work), rather than how they
work in detail. To cover the latter, an extensive set of 104 references is
provided (104 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

10

@Article{gibbs94a,
 key     = "gibbs94a",
 author  = "W. Wayt Gibbs",
 title   = "Software's Chronic Crisis",
 journal = "Scientific American (International Edition)",
 month   = sep,
 year    = "1994",
 volume  = "",
 number  = "",
 pages   = "72--81",
 abstract = "
Despite 50 years of progress, the software industry remains years, perhaps
decades, short of the mature engineering discipline needed to meet the demands
of an information age society. Software engineering remains a term of
aspiration. The vast majority of computer code is still handcrafted from raw
programming languages by artisans using techniques they neither measure nor are
able to repeat consistently. The picture is not entirely bleak. Intuition is
slowly yielding to analysis as programmers begin using quantitative
measurements of the quality of the software they produce to improve the way
they produce it. The mathematical foundations of programming are solidifying as
researchers work on ways of expressing program designs in algebraic forms that
make it easier to avoid serious mistakes. Academic computer scientists are
starting to address their failure to produce a solid corps of software
professionals. Perhaps most important, many in the industry are turning their
attention toward inventing the technology and market structures needed to
support interchangeable, reusable software parts (3 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

11 (The big paper on CIA)

@Article{chen90a,
 key     = "chen90a",
 author  = "Yih-Farn Chen and Michael Y. Nishimoto and C V. Ramamoorthy",
 title   = "The {C} Information Abstraction System",
 journal = j-tse,
 month   = mar,
 year    = "1990",
 volume  = "",
 number  = "",
 pages   = "325--34",
 abstract = "
A system for analyzing program structures is described. The system extracts
relational information from C programs according to a conceptual model and
stores the information in a database. It is shown how several interesting
software tasks can be performed by using the relational views. These tasks
include generation of graphical views, subsystem extraction, program layering,
dead code elimination and binding analysis (27 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

12 (See also SIGSOFT Softw. Eng. Notes (USA), SIGSOFT Software Engineering
Notes, vol.14, no.7, p.  86-95)

@InCollection{schwanke93a,
 key       = "schwanke93a",
 author    = "R. W. Schwanke and M. A. Platoff",
 title     = "Cross References are Features",
 booktitle = "Machine learning: from theory to applications. Cooperative
            research at Siemens and MIT",
 pages     = "107--23",
 publisher = "Germany Springer--Verlag (Editors: S. J. Hanson, W. Remmele,
            and R. L. Rivest)",
 editor    = "S.J Hanson and W. Remmele and R. L. Rivest",
 month     = "",
 year      = "1993",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
The authors have built a detailed cross-reference extractor and a conceptual
clustering tool, and are using them to analyze cross-reference graphs for
several kinds of software maintenance problems. Preliminary results suggest
that these methods can help reduce unnecessary recompilations, summarize
complex structure graphs, and improve modularity. The authors plan to develop
interactive techniques that combine the bookkeeping abilities of the computer
with the deep knowledge of the maintainer, to produce even better solutions.
The paper discusses some structural problems that occur frequently in mature
systems, describes the authors feature representation for cross-references,
presents the prototype conceptual clustering algorithm they are using, and
describes how the technology can be used to attack the structural problems.
They include preliminary results of our experiments, and details of some
planned experiments (13 Refs.)
",
 note      = "",
}

------------------------------------------------------------------------------

13 (Large-scale visualization)

@Article{eick92a,
 key     = "eick92a",
 author  = "Stephen G. Eick and Joseph L. Steffen and Eric E. {Sumner Jr.}",
 title   = "Seesoft --- A Tool For Visualizing Line Oriented Software
          Statistics",
 journal = j-tse,
 month   = nov,
 year    = "1992",
 volume  = "",
 number  = "",
 pages   = "957--68",
 abstract = "
The Seesoft software visualization system allows one to analyze up to 50000
lines of code simultaneously by mapping each line of code into a thin row. The
color of each row indicates a statistic of interest, e.g., red rows are those
most recently changed, and blue are those least recently changed. Seesoft
displays data derived from a variety of sources, such as version control
systems that track the age, programmer, and purpose of the code (e.g., control
ISDN lamps, fix bug in call forwarding); static analyses, (e.g., locations
where functions are called); and dynamic analyses (e.g., profiling). By means
of direct manipulation and high interaction graphics, the user can manipulate
this reduced representation of the code in order to find interesting patterns.
Further insight is obtained by using additional windows to display the actual
code. Potential applications for Seesoft include discovery, project management,
code tuning, and analysis of development methodologies (20 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

14

@InProceedings{devanbu94a,
 key          = "devanbu94a",
 author       = "Premkumar T. Devanbu and David S. Rosenblum and Alexander L.
                 Wolf",
 title        = "Automated Construction of Testing and Analysis Tools",
 booktitle    = "Proceedings of the 16th International Conference on Software
                 Engineering",
 address      = "Sorrento, Italy",
 month        = "16--21  " # may,
 year         = "1994",
 pages        = "241--50",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Many software testing and analysis tools manipulate graph representations of
programs, such as abstract syntax trees or abstract semantics graphs.
Hand-crafting such tools in conventional programming languages can be
difficult, error prone, and time consuming. Our approach is to use application
generators targeted for the domain of graph-representation-based testing and
analysis tools. Moreover, we generate the generators themselves, so that the
development of tools based on different languages and/or representations can
also be supported better. In this paper we report on our experiences in
developing a system called Aria that generates testing and analysis tools based
on an abstract semantics graph representation for C and C++ called Reprise.
Aria itself was generated by the Genoa system. We demonstrate the utility of
Aria and, thereby, the power of our approach, by showing Aria's use in the
development of a tool that derives control dependence graphs directly from
Reprise abstract semantics graphs (9 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

15 (Not very good)

@Article{linton84a,
 key     = "linton84a",
 author  = "Mark A. Linton",
 title   = "Implementing Relational Views of Programs",
 journal = "SIGPLAN Notices",
 month   = "",
 year    = "1984",
 volume  = "19",
 number  = "5",
 pages   = "132--40",
 abstract = "
Configurations, versions, call graphs, and slices are all examples of views, or
cross-sections, of programs.  To provide a powerful mechanism for defining,
retrieving, and updating these views, the OMEGA programming system uses a
relational database system to manage all program information. The author has
built a prototype implementation of the OMEGA database system interface. This
implementation includes the design of a relational schema for a Pascal-like
language, a program for taking software stored as text and translating it into
the database representation, and a simple, pointing-oriented user interface.
Initial performance measurements indicate that response is too slow in our
current environment, but that advances in database software technology and
hardware should make response fast enough in the near future.
",
 note    = "",
}

------------------------------------------------------------------------------

16 (Ok paper. Refine.)

@Article{kotik89a,
 key     = "kotik89a",
 author  = "Gordon B. Kotik and Lawrence Z. Markosian",
 title   = "Automating Software Analysis and Testing Using a Program
            Transformation System",
 journal = j-sig,
 month   = dec,
 year    = "1989",
 volume  = "14",
 number  = "8",
 pages   = "75--84",
 abstract = "
The authors describe an approach to software analysis and test generation that
combines several technologies:  object-oriented databases and parsers for
capturing and representing software; pattern languages for writing program
templates and querying and analyzing a database of software; and transformation
rules for automatically generating test cases based on the analysis results,
and for automatically creating program mutants to determine adequacy of
coverage of the test cases. They present a program transformation system,
REFINE, that incorporates these technologies in an open environment for
software analysis and test generation. Next they present concrete examples of
how their approach is being applied to analysis and test generation for C
software.
",
 note    = "",
}

------------------------------------------------------------------------------

17 (Refine. Almost identical to 16.)

@InProceedings{burson90a,
 key          = "burson90a",
 author       = "Scott Burson and Gordon B. Kotik and Lawrence Z. Markosian",
 title        = "A Program Transformation Approach to Automating Software
               Re-engineering",
 booktitle    = "Proceedings. Fourteenth Annual International Computer Software
               and Applications Conference",
 month        = "31 " # oct # "--2 " # nov,
 year         = "1990",
 pages        = "314--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 address      = "Chicago, IL",
 abstract = "
The authors describe a novel approach to software re-engineering that combines
several technologies:  object-oriented databases integrated with parser, for
capturing the software to be re-engineered; specification and pattern languages
for querying and analyzing a database of software; and transformation rules for
automatically generating re-engineered code. The authors then describe REFINE,
an environment for program representation, analysis, and transformation that
provides the tools needed to implement the automation of software maintenance
and re-engineering. The transformational approach is illustrated with examples
taken from actual experience in re-engineering software in C, JCL and NATURAL.
It is concluded that the ability to support automation in modifying large
software systems by using rule-based program transformation is a key innovation
of the present approach that distinguishes it from tools that focus only on
automation of program analysis (8 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

18

@Article{goguen86a,
 key     = "goguen86a",
 author  = "Joseph A. Goguen",
 title   = "Reusing and Interconneting Software Components",
 journal = j-comp,
 month   = feb,
 year    = "1986",
 volume  = "19",
 number  = "2",
 pages   = "16--28",
 abstract = "
The author considers problems, concepts, and approaches relevant to
environments for creating, documenting, and maintaining large software systems.
The goal is to make programming significantly easier, more reliable, and
cost-effective by reusing previous code and programming experience to the
greatest extent possible. Suggestions given include: systematic but limited use
of semantics, by explicitly attaching theories (which give semantics, either
formal or informal) to software components with views (which describe
semantically correct interconnections at component interfaces); use of generic
entities, to maximize reusability; a distinction between horizontal and
vertical composition; use of a library interconnection language to assemble
large programs from existing entities; support for different levels of
formality in both documentation and validation; and facilitation of program
understanding by animating abstract data types and module interfaces. Ada is
used for examples because it has some convenient features, but the proposals
also apply to other languages (42 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

19 (Some random sheet on Rigi)

------------------------------------------------------------------------------

20 (Rigi. Very good paper on reverse engineering concepts. See also
_Understanding software systems using reverse engineering technology:
perspectives from the Rigi project_, Proceedings CASCON '93, p. 2 vol.
xx+1180, 217-26 vol.1)

@InProceedings{muller94a,
 key          = "muller94a",
 author       = "Hausi A. {M\"uller} and Kenny Wong and Scott R. Tilley",
 title        = "Understanding Software Systems Using Reverse Engineering
               Technology",
 booktitle    = "Proceedings of the 62nd Congress of L'Association Canadienne
               Francaise pour l'Avancement des Sciences (ACFAS '94)",
 month        = "16--17 " # may,
 year         = "1994",
 pages        = "41--48",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Montreal, PQ",
 abstract = "
Software engineering research has focused primarily on software construction,
neglecting software maintenance and evolution. Observed is a shift in research
from synthesis to analysis. The process of reverse engineering is introduced
as an aid in program under standing. This process is concerned with the
analysis of existing software systems to make them more understandable for
maintenance, re-engineering, and evolution purposes. Presented is reverse
engineering technology developed as part of the Rigi project. The Rigi approach
involves the identification of software artifacts in the subject system and the
aggregation of these artifacts to form more abstract system representations.
Early industrial experience has shown that software engineers using Rigi can
quickly build mental models from the discovered abstractions that are
compatible with the mental models formed by the maintainers of the underlying
software.
",
 note         = "",
}

------------------------------------------------------------------------------

21 

@InProceedings{muller92a,
 key          = "muller92a",
 author       = "H. A. {M\"uller} and S. R. Tilley and M. A. Orgun and B. D.
               Corrie and N. H. Madhavji",
 title        = "A reverse engineering environment based on spatial and visual
               software interconnection models",
 booktitle    = "Proceedings of the Fifth ACM SIGSOFT Symposium on Software
               Develeopment Environments",
 month        = "9--11 " # dec,
 year         = "1992",
 pages        = "88--98",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "USA",
 abstract = "
Reverse engineering is the process of extracting system abstractions and design
information out of existing software systems. This information can then be used
for subsequent development, maintenance, re-engineering, or reuse purposes.
This process involves the identification of software artifacts in a particular
subject system, and the aggregation of these artifacts to form more abstract
system representations. This paper describes a reverse engineering environment
which uses the spatial and visual information inherent in graphical
representations of software systems to form the basis of a software
interconnection model. This information is displayed and manipulated by the
reverse engineer using an interactive graph editor to build subsystem
structures out of software building blocks (19 Refs.)
",
 note         = "Published as SIGSOFT Software Engineering Notes, Volume 17
               Number 5.",
}

------------------------------------------------------------------------------

22

@Article{canfora92a,
 key     = "canfora92a",
 author  = "Gerardo Canfora and Aniello Cimitile and Ugo de Carlini",
 title   = "A logic-based approach to reverse engineering tools
          production",
 journal = j-tse,
 month   = dec,
 year    = "1992",
 volume  = "18",
 number  = "12",
 pages   = "1053--64",
 abstract = "
Difficulties arising in the use of documents produced by reverse engineering
tools are analyzed. With reference to intermodular data flow analysis for
Pascal software systems, an interactive and evolutionary tool is proposed. The
tool is based on the production of intermodular data flow information by static
analysis of code, its representation in a Prolog program dictionary, and a
Prolog abstractor that allows the specific queries to be answered (44 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

23

@InProceedings{paul92a,
 key          = "paul92a",
 author       = "Santanu Paul and Atul Prakash",
 title        = "Source Code Retrieval Using Program Patterns",
 booktitle    = "Proceedings. Fifth International Workshop on Computer-Aided
               Software Engineering",
 month        = "6--10 " # jul,
 year         = "1992",
 pages        = "92--105",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Montreal, Que., Canada",
 abstract = "
A method for automating the search for codes that have particular structural
features is described. A pattern language is used to specify the pattern of
desired code. The benefits of using a pattern language include a higher level
of abstraction in specifying patterns and the power to specify complex patterns
which are impossible to express in grep type languages. A system that accepts
these patterns and locates code fragments in C source files is described (9
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

24 (CCEL)

@InProceedings{chowdhury95a,
 key          = "chowdhury95a",
 author       = "Anir Chowdhury and Scott Meyers",
 title        = "Facilitating software maintenance by automated detection of
               constraint violations",
 booktitle    = "Proceedings of the 1993 Conference on Software Maintenance",
 month        = "27--30 " # sep,
 year         = "1993",
 pages        = "262--71",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 address      = "Que., Canada",
 abstract = "
CCEL, a language that allows programmers to formally express constraints on
their software systems and to automatically detect violations of these
constraints, is introduced. The power, flexibility, and overall utility of CCEL
are demonstrated by examples showing how it can express real constraints from
real software developers for real systems (26 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

25 (CCEL)

@InProceedings{duby92a,
 key          = "duby92a",
 author       = "Carolyn K. Duby and Scott Meyers and Steven P. Reiss",
 title        = "{CCEL}: A Metalanguage for {C}++",
 booktitle    = "USENIX {C}++ Technical Conference Proceedings",
 month        = "10--13 " # aug,
 year         = "1992",
 pages        = "99--115",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "USENIX Assoc. Berkeley, CA",
 address      = "Portland, OR",
 abstract = "
C++ is an expressive language, but it does not allow software developers to say
all the things about their systems that they need to be able to say. In
particular, it offers no way to express many important constraints on a
system's design, implementation, and presentation. The paper describes CCEL, a
metalanguage for C++ that allows software developers to express constraints on
C++ designs and implementations. It also describes Clean++, a system that
checks C++ code for violations of CCEL constraints. CCEL is designed for
practical, real-world use, and the examples in the paper demonstrate its power
and flexibility (14 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

26

@InProceedings{antonini87a,
 key          = "antonini87a",
 author       = "P. Antonini and P. Benedusi and G. Cantone and A. Cimitile",
 title        = "Maintenance and reverse engineering: low-level design
               documents production and improvement",
 booktitle    = "Proceedings of the Conference on Software Maintenance ---
             1987",
 month        = "21--24 " # sep,
 year         = "1987",
 pages        = "91--100",
 editor       = "",
 organization = "",
 volume       = "",
 publisher    = "IEEE",
 address      = "Austin, TX",
 abstract = "
The authors deal with the production of low-level program design knowledge and
illustrate a system for the improvement of production from scratch of Jackson
or Warnier/Orr documents that are totally consistent with code. An information
abstractor (IA) and the related maintenance methodologies are proposed. The
authors illustrate the rules that allow these IA products to be automatically
transformed into Jackson (JSP) and Warnier/Orr logical diagrams, respectively.
They present a program verification and documentation system founded on the IA,
which supports both production from scratch and structured maintenance (17
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

27

@InProceedings{kontogiannis94a,
 key          = "kontogiannis94a",
 author       = "K. Kontogiannis and R. DeMori and M. Bernstein and E. Merlo",
 title        = "Localization of design concepts in legacy systems",
 booktitle    = "Proceedings. International Conference on Software
               Maintenance",
 address      = "Victoria, BC, Canada",
 month        = "19--23 " # sep,
 year         = "1994",
 pages        = "414--23",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Complete automation of design recovery of large systems is a desirable but
impractical goal due to complexity and size issues, so current research efforts
focus on redocumentation and partial design recovery. Pattern matching lies at
the center of any design recovery system. In the context of a larger project to
develop an integrated reverse engineering environment, we are developing a
framework for performing clone detection, code localization, and plan
recognition. This paper discusses a plan localization and selection strategy
based on a dynamic programming function that records the matching process and
identifies parts of the plan and code fragment that are most `similar'. Program
features used for matching are currently based on data flow, control flow, and
structural properties. The matching model uses a transition network and allows
for the detection of insertions and deletions, and it is targeted for legacy
C-based systems (22 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

28

@InProceedings{miller92a,
 key          = "miller92a",
 author       = "Lawrence Miller",
 title        = "Program Understanding --- Does It Offer Hope for Aging
              Software?",
 booktitle    = "Proceedings of the Seventh Knowledge-Based Software
              Engineering Conference",
 address      = "",
 month        = sep,
 year         = "1992",
 pages        = "238--42",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Two questions are examined: what does it mean to understand a program? What
solutions to the aging software crisis does program understanding offer? One
view is that the aging software problem is a form of support for program
maintenance. Another view is that the problem of applying program understanding
techniques to aging software is one of both extending the life of existing
relics, and mining them for valuable components. Yet another view is that
domain specific cliches are necessary in program understanding and that program
understanding support for software is how programmers can come to understand a
program through cooperative interaction with a knowledge base. The work focuses
on the flexibility provided by natural language interaction (0 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

29 (I have the tech report, which is no longer available; citing the conference
paper instead.)

@InProceedings{sefika96a,
 key          = "sefika96a",
 author       = "Mohlalefi Sefika and Aamod Sane and Roy H. Campbell",
 title        = "Monitoring Compliance of a Software System with its High-Level
                 Design Models",
 booktitle    = "Proceedings of the 18th International Conference on Software
                 Engineering",
 address      = "Berlin, Germany",
 month        = "25--30 " # mar,
 year         = "1996",
 pages        = "387--96",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
As a complex software system evolves, its implementation tends to diverge from
the intended or documented design models. Such undesirable deviation makes the
system hard to understand, modify and maintain. This paper presents a hybrid
computer-assisted approach for confirming that the implementation of a system
maintains its expected design models and rules. Our approach closely
integrates logic-based static analysis and dynamic visualization, providing
multiple code views and perspectives. We show that the hybrid technique helps
determine design-implementation congruence at various levels of abstraction:
concrete rules like coding guidelines, architectural models like design
patterns or connectors, and subjective design principles like low coupling and
high cohesion. The utility of our approach has been demonstrated in the
development of mu Choices, a new multimedia operating system which inherits
many design decisions and guidelines learned from experience in the
construction and maintenance of its predecessor, Choices. 
",
 note         = "",
}

------------------------------------------------------------------------------

30 (Aspect)

@Article{jackson95a,
 key     = "jackson95a",
 author  = "Daniel Jackson",
 title   = "Aspect: detecting bugs with abstract dependences",
 journal = j-tsem,
 month   = apr,
 year    = "1995",
 volume  = "4",
 number  = "2",
 pages   = "109--45",
 abstract = "
Aspect is a static analysis technique for detecting bugs in imperative
programs, consisting of an annotation language and a checking tool. Like a type
declaration, an Aspect annotation of a procedure is a kind of declarative,
partial specification that can be checked efficiently in a modular fashion; but
instead of constraining the types of arguments and results, Aspect
specifications assert dependences that should hold between inputs and outputs.
The checker uses a simple dependence analysis to check code against annotations
and can find bugs automatically that are not detectable by other static means,
especially errors of omission, which are common, but resistant to type
checking.  This article explains the basic scheme and shows how it is
elaborated to handle data abstraction and aliasing (38 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

31

@InProceedings{jackson94a,
 key          = "jackson94a",
 author       = "Daniel Jackson and David A. Ladd",
 title        = "Semantic Diff: A Tool For Summarizing The Effects Of
              Modifications",
 booktitle    = "Proceedings. International Conference on Software
              Maintenance",
 address      = "Victoria, BC, Canada",
 month        = "19--23 " # sep,
 year         = "1994",
 pages        = "243--52",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Describes a tool that takes two versions of a procedure and generates a report
summarizing the semantic differences between them. Unlike existing tools based
on comparison of program dependence graphs, our tool expresses its results in
terms of the observable input-output behavior of the procedure, rather than
its syntactic structure. And because the analysis is truly semantic, it
requires no prior matching of syntactic components, and generates fewer
spurious differences, so that meaning-preserving transformations (such as
renaming local variables) are correctly determined to have no visible effect. A
preliminary experiment on modifications applied to the code of a large
real-time system suggests that the approach is practical (11 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

32

@Article{boehm84a,
 key     = "boehm84a",
 author  = "Barry W. Boehm",
 title   = "Software Engineering Economics",
 journal = j-tse,
 month   = jan,
 year    = "1984",
 volume  = "SE-10",
 number  = "1",
 pages   = "4--21",
 abstract = "
The current state of the art and recent trends in software engineering
economics are summarized. An overview is provided of economic analysis
techniques and their applicability to software engineering and management. The
field of software cost estimation, including the major estimation techniques
available, the state of the art in algorithmic cost models, and the outstanding
research issues in software cost estimation is surveyed (59 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

33

@Article{prietodiaz86a,
 key     = "prietodiaz86a",
 author  = "{Rub\'en} Prieto-{D\'iaz} and James M. Neighbors",
 title   = "Module Interconnection Languages",
 journal = j-jss,
 month   = nov,
 year    = "1986",
 volume  = "6",
 number  = "4",
 pages   = "307--34",
 abstract = "
Module interconnection languages are considered essential tools in the
development of large software systems. Current software development practice
follows the principle of the recursive decomposition of larger problems that
can be grasped, understood, and handled by specialized and usually independent
teams of software engineers. After teams succeed in designing and coding their
respective subsystems, they are faced with different but usually more difficult
issues; how to integrate independently developed subsystems or modules into the
originally planned complete system. Module interconnection languages (MILs)
provide formal grammar constructs for describing the global structure of a
software system and for deciding the various module interconnection
specifications required for its complete assembly. Automatic processing of
these formal descriptions results in a verification of system integrity and
intermodular compatibility. This paper is a survey of MILs that are
specifically designed to support module interconnection and includes brief
descriptions of some software development systems that support module
interconnection (52 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

34

@Article{rasure91a,
 key     = "rasure91a",
 author  = "John R. Rasure and Carla S. Williams",
 title   = "An integrated data flow visual language and
         software development environment",
 journal = "Journal of Visual Languages and Computing",
 month   = sep,
 year    = "1991",
 volume  = "2",
 number  = "3",
 pages   = "217--246",
 abstract = "
The current generation of data flow based visual programming systems is all
too often limited in application. It is the authors' contention that data flow
visual languages, to be more widely accepted for solving a broad range of
problems, need to be more general in their syntax, semantics, translation
schemes, computational model, execution methods and scheduling.  These
capabilities should be accompanied by a development environment that
facilitates information processing extensions needed by the user to solve a
wide range of application-specific problems. They address these issues by
describing and critiquing the Khoros system implemented by the University of
New Mexico, Khoros Group. The Khoros infrastructure consists of several layers
of interacting subsystems. A user interface development system (UIDS) combines
a high-level user interface specification with methods of software development
that are embedded in a code generation tool set. The UIDS is used to create,
install and maintain the fundamental operators for cantata, the visual
programming language component of Khoros (22 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

35

@InProceedings{devanbu92a,
 key          = "devanbu92a",
 author       = "Premkumar T. Devanbu",
 title        = "{GENOA} --- A Customizable, Language- and Front-End
               Independent Code Analyzer",
 booktitle    = "Proceedings of the 14th International Conference on Software
              Engineering",
 address      = "Melbourne, Vic., Australia",
 month        = "11--15 " # may,
 year         = "1992",
 pages        = "307--17",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM New York, NY",
 abstract = "
Programmers working on large software systems spend a great deal of time
examining code and trying to understand it. Code analysis tools (e.g. cross
referencing tools such as CIA and Cscope) can be very helpful in this process.
This paper describes GENOA, an application generator that can produce a whole
range of useful code analysis tools. GENOA is designed to be language- and
front-end independent; it can be interfaced to any front-end for any language
that produces an attributed parse tree, simply by writing an interface
specification. While GENOA programs can perform arbitrary analyses on the parse
tree, the GENOA language has special, compact iteration operators that are
tuned for expressing simple, polynomial time analysis programs. It describes
the system, provides several practical examples, and presents complexity and
expressivity results for the above-mentioned sublanguage of GENOA (21 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

36

@Article{murphy96b,
 key     = "murphy96b",
 author  = "Gail C. Murphy and David Notkin",
 title   = "Lightweight Lexical Source Model Extraction",
 journal = j-tsem,
 month   = jul,
 year    = "1996",
 volume  = "5",
 number  = "3",
 pages   = "262--292",
 abstract = "
Software engineers maintaining an existing software system often depend on the
mechanized extraction of information from system artifacts. Some useful kinds
of information-source models are well-known: call graphs, file dependences,
etc. Predicting every kind of source model that a software engineer may need is
impossible. We have developed a lightweight approach for generating flexible
and tolerant source model extractors from lexical specifications. The approach
is lightweight in that the specifications are relatively small and easy to
write. It is flexible in that there are few constraints on the kinds of
artifacts from which source models are extracted (e.g., we can extract from
source code, structured data files, documentation, etc.). It is tolerant in
that there are few constraints on the condition of the artifacts. For example,
we can extract from source that cannot necessarily be compiled. Our approach
extends the kinds of source models that can be produced from lexical
information while avoiding the constraints and brittleness of most parser-based
approaches. We have developed tools to support this approach and applied the
tools to the extraction of a number of different source models (file
dependences, event interactions, call graphs) from a variety of system
artifacts (C, C++, CLOS, Eiffel, TCL, structured data). We discuss our approach
and describe its application to extract source models not available using
existing systems; for example, we compute the implicitly-invokes relation over
Field tools. We compare and contrast our approach to the conventional lexical
and syntactic approaches of generating source models.
",
 note    = "",
}

------------------------------------------------------------------------------

37

@Article{wong94a,
 key     = "wong94a",
 author  = "Kenny Wong and Scott R. Tilley and Hausi A. {M\"uller} and
         Margaret-Anne D. Storey",
 title   = "Structural Redocumentation: A Case Study",
 journal = j-soft,
 month   = jan,
 year    = "1995",
 volume  = "12",
 number  = "1",
 pages   = "46--54",
 abstract = "
Most software documentation typically describes the program at the algorithm
and data-structure level. For large legacy systems, understanding the system's
architecture is more important. The authors propose a method of reverse
engineering through redocumentation that promises to extend the useful life of
large systems (5 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

38

@Article{meyer85a,
 key     = "meyer85a",
 author  = "Bertrand Meyer",
 title   = "On Formalism in Specifications",
 journal = j-soft,
 month   = jan,
 year    = "1995",
 volume  = "2",
 number  = "1",
 pages   = "6--26",
 abstract = "
A critique of a natural-language specification, followed by presentation of a
mathematical alternative, demonstrates the weakness of natural language and the
strength of formalism in requirements specifications. The author does not
advocate formal specifications as a replacement for natural-language
requirements; rather, he views them as a complement to natural-language
descriptions and, as illustrated by an example, as an aid in improving the
quality of natural-language specifications (16 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

39

@InProceedings{atkinson96a,
 key          = "atkinson96a",
 author       = "Darren C. Atkinson and William G. Griswold",
 title        = "The Design of Whole-Program Analysis Tools",
 booktitle    = "Proceedings of the 18th International Conference on Software
              Engineering",
 address      = "Berlin, Germany",
 month        = "25--30 " # mar,
 year         = "1996",
 pages        = "16--27",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Building efficient tools for understanding large software systems is difficult.
Many existing program understanding tools build control flow and data flow
representations of the program a priori, and therefore may require prohibitive
space and time when analyzing large systems. Since much of these
representations may be unused during an analysis, we construct representations
on demand, not in %advance. Furthermore, some representations, such as the
abstract syntax tree, %may be used infrequently during an analysis. We discard
these representations and recompute them as needed, reducing the overall space
required. Finally, we permit the user to selectively trade off time for
precision and to customize the termination of these costly analyses in order to
provide finer user control. We revised the traditional software architecture
for compilers to provide these features without unnecessarily complicating the
analyses themselves. These techniques have been successfully applied in the
design of a program slicer for the Comprehensive Health Care System (CHCS), a
million line hospital management system written in the MUMPS programming
language (27 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

40

@Misc{devanbuxxa,
 key    = "devanbuxxa",
 author = "Prem Devanbu and Laura Eaves",
 title  = "{GEN}++ --- an analyzer generator for {C}++ programs",
 url    = "http://www.research.att.com/~prem/projects.html#genoa",
 abstract = "
Introduction:
The C++ programming language is becoming increasingly popular within and
without AT&T. C++ supports an object-oriented style of programming, and has
many powerful features that contribute to information hiding, software reuse,
and program maintainability.  Because of the increasing value of C++ software
assets, it is important for developers and managers to have access to tools
that extract information, generate metrics, check coding standards, etc. Tools
such as CIA++ make a significant contribution in this area, but more tools are
needed. Unfortunately, tools for C++ are hard to build, primarily because of
the complexity of processing the source-parsing, type checking, and symbol
resolution for C++ are difficult tasks. In addition, introducing additional
functionality into C++ tools, or tracking the evolution of the C++ language,
involves major effort.\par

In this paper, we describe gen++, a tool generator for C++ 3.0. gen++ was
implemented by attaching the GENOA-GENII portable parse tree querying mechanism
to the Cfront 3.0 compiler. gen++ is designed to reduce the difficulty of
building C++ tools in 3 ways: first, tools are implemented by writing a
specification in a compact domain specific language; second, all tools
generated by gen++ use the Cfront compiler to do parsing, type checking and
symbol resolution, and finally, gen++ masks Cfront's implementation details
from the tool builder. To implement a particular C++ language tool with gen++,
the tool builder simply specifies the operations on the C++ parse tree that are
of interest to her; the specifics of processing the source language, and
traversing the internal data structures of Cfront, are no longer of concern.
",
 note   = "URL: {\urlBiBTeX{http://www.research.att.com/~prem/projects.html#genoa}}",
}

------------------------------------------------------------------------------

41

@Article{ladd95a,
 key     = "ladd95a",
 author  = "David A. Ladd and J. Christopher Ramming",
 title   = "{A}*: A Language for Implementing Language Processors",
 journal = j-tse,
 month   = nov,
 year    = "1995",
 volume  = "21",
 number  = "11",
 pages   = "894--901",
 abstract = "
A* is an experimental language designed to facilitate the creation of
language-processing tools. It is analogous either to an interpreted yacc with
Awk as its statement language, or to a version of Awk which processes programs
rather than records.  A* offers two principal advantages over the combination
of lex, yacc, and C: a high-level interpreted base language and built-in parse
tree construction. A* programmers are thus able to accomplish many useful tasks
with little code. This paper describes the motivation for A*, its design, and
its evolution. Experience with A* is described, and then the paper concludes
with an analysis of that experience (13 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

42

@InProceedings{johnson92a,
 key          = "johnson92a",
 author       = "Bret Johnson and Steve Ornburn and Spencer Rugaber",
 title        = "A Quick Tools Strategy for Program Analysis and Software
               Maintenance",
 booktitle    = "Conference on Software Maintenance 1992",
 address      = "Orlando, FL",
 month        = "9--12 " # nov,
 year         = "1992",
 pages        = "206--13",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A strategy for quickly producing special-purpose tools is described. The
strategy combines existing tools including simple off-the-shelf text processing
tools; rule-based, language-specific analysis tool; and a commercial CASE
(computer-aided software engineering) tool. The strategy has been successfully
used in the context of the maintenance and reverse engineering of a large,
real-time software system used for telephony. The approach is illustrated by
examples that include the generation of cross reference information, calling
trees, run-time stack analysis, and code restructuring (6 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

43

@Article{horwitz86a,
 key     = "horwitz86a",
 author  = "Susan Horwitz and Tim Teitelbaum",
 title   = "Generating Editing Environments Based on Relations and
          Attributes",
 journal = "ACM Transactions on Programming Languages and Systems",
 month   = oct,
 year    = "1986",
 volume  = "8",
 number  = "4",
 pages   = "577--608",
 abstract = "
The ability to generate language-based editors depends on the existence of a
powerful, language-independent model of editing. A model is proposed in which
programs are represented as attributed abstract-syntax trees with an associated
relational database. Relations can depend on the state of the attributed tree,
and attributes can depend on the values in relations, provided there are no
circular dependencies. The power and the limitations of relational operations
are demonstrated with respect to the support of static-semantic checking,
anomaly detection, an interrogation facility, and the ability to define
alternative program displays. The advantages of the hybrid system over both the
purely relational and purely attribute-based systems are presented, and new
algorithms are given for query evaluation and incremental view updating
motivated by the efficiency requirements of interactive editing under the
defined model. A prototype implementation of an editor generator is described,
and suggestions for future research are made (27 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

44

@InProceedings{bischofberger95a,
 key          = "bischofberger95a",
 author       = "Walter R. Bischofberger and Thomas Kofler and Kai-Uwe
               {M\"atzel} and Bruno {Sch\"affer}",
 title        = "Computer Supported Cooperative Software Engineering with
               Beyond-Sniff",
 booktitle    = "Proceedings 1995 Software Engineering Environments",
 address      = "Noordwijkerhout, The Netherlands",
 month        = "5--7 " # apr,
 year         = "1995",
 pages        = "135--43",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Teamwork is a prerequisite for the development of large complex software
systems. In conventional software engineering coordination of teamwork is
achieved by exchanging formal documents and by providing support for keeping
these documents consistent even while several developers are evolving them. In
order to support teamwork more effectively it is important to move the focus
beyond coordination towards cooperation. The goal of the Beyond-Sniff project
is to provide cooperation support in three ways. First, by providing a
terminological and conceptual foundation for the field of cooperative
software engineering. Second, by developing tools for computer supported
cooperative software engineering.  Third, by developing a platform for control
and data integration, a technical prerequisite for computer supported
cooperative software engineering (26 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

45 (this paper defines many of the terms of software reverse engineering)

@Article{chikofsky90a,
 key     = "chikofsky90a",
 author  = "Elliot J. Chikofsky and James H. Cross II",
 title   = "Reverse Engineering and Design Recovery: A Taxonomy",
 journal = j-soft,
 month   = jan,
 year    = "1990",
 volume  = "7",
 number  = "1",
 pages   = "13--17",
 abstract = "
The key to applying computer-aided software engineering to the maintenance and
enhancement of existing systems lies in applying reverse-engineering
approaches. However, there is considerable confusion over the terminology used
in both technical and marketplace discussions. The authors define and relate
six terms: forward engineering, reverse engineering, redocumentation, design
recovery, restructuring, and reengineering. The objective is not to create new
terms but to rationalize the terms already in use. The resulting definitions
apply to the underlying engineering processes, regardless of the degree of
automation applied (3 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

46

@Article{sullivan96a,
 key     = "sullivan96a",
 author  = "Kevin J. Sullivan and Ira J. Kalet and David Notkin",
 title   = "Evaluating the Mediator Method: Prism as a Case Study",
 journal = j-tse,
 month   = aug,
 year    = "1996",
 volume  = "22",
 number  = "8",
 pages   = "563--579",
 abstract = "
A software engineer's confidence in the profitability of a novel design
technique depends to a significant degree on previous demonstrations of its
profitability in practice. Trials of proposed techniques are thus of
considerable value in providing factual bases for evaluation. In this paper we
present our experience with a previously presented design approach as a basis
for evaluating its promise and problems. Specifically, we report on our use of
the mediator method to reconcile tight behavioral integration with ease of
development and evolution of Prism, a system for planning radiation treatments
for cancer patients. Prism is now in routine clinical use in several major
research hospitals. Our work supports two claims. In comparison to more common
design techniques, the mediator approach eases the development and evolution of
integrated systems; and the method can be learned and used profitably by
practicing software engineers.
",
 note    = "",
}

------------------------------------------------------------------------------

47

@Article{griswold95a,
 key     = "griswold95a",
 author  = "William G. Griswold and Darren C. Atkinson",
 title   = "Managing Design Trade-Offs for a Program Understanding and
          Transformation Tool",
 journal = j-jss,
 month   = jul # "--" # aug,
 year    = "1995",
 volume  = "30",
 number  = "1--2",
 pages   = "99--116",
 abstract = "
Software maintenance is costly. Part of the problem is that the repeated
modification of a software system degrades its structure, thus making it
difficult to understand and modify. Semantically rich techniques can help a
software engineer understand or restore the structure of a system, but they
typically use concurrency analysis or dependence analysis on pointers, which
are difficult, at best, to implement efficiently. We believe that less
expensive techniques can be used if the design and domain knowledge of the
software engineer can be used in the analysis. Exploiting this knowledge may
entail iteratively refining a query to find the right information, which
requires a fast, flexible tool. We describe the design and use of a fast,
programmable tool that can perform syntactically oriented text-processing tasks
for use in program understanding and transformation. We take a `traditional'
compiler approach to the problem to provide a tool with the flexibility and
speed of UNIX tools such as AWK, but make structural adjustments to the tool's
design to accommodate interactive analysis of an entire system. Our performance
measurements suggest that this approach can produce results substantially
faster than previous approaches (37 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

48

@InProceedings{sullivan96b,
 key          = "sullivan96b",
 author       = "Kevin J. Sullivan",
 title        = "Rapid development of simple, custom program analysis tools",
 booktitle    = "Proceedings. Fourth Workshop on Program Comprehension",
 address      = "Berlin, Germany",
 month        = "29--31 " # mar,
 year         = "1996",
 pages        = "40--4",
 editor       = "A. Cimitile and H. A. {M\"uller}",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Compiler based program analysis tools can aid program understanding.
Requirements for such tools include practicality and high assurance: Tools have
to be economical to develop and run and applicable to real systems; and
engineers have to have a basis for confidently interpreting tool results. These
requirements can be hard to meet. Sophisticated tools can be costly to build
and use; complexities of real systems can render compiler based tools useless;
and it can be hard to interpret tool results. These problems raise three
questions: Are simple, high assurance tools practical? Are they useful?  What
are the key issues for practicality and assurance in general? To help get
answers, the author has designed a component based architecture for rapid tool
development. He discusses the architecture and its use, and summarizes an
experience using a tool to analyze a real system-a version of Mosaic. Work to
date demonstrates the feasibility of rapidly developing simple, practical high
assurance tools, and sheds light on some of the practicality and assurance
issues (14 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

49 (An example of a well-written paper)

@Article{snelting96a,
 key     = "snelting96a",
 author  = "Gregor Snelting",
 title   = "Reengineering of Configurations Based on Mathematical Concept
          Analysis",
 journal = j-tsem,
 month   = apr,
 year    = "1996",
 volume  = "5",
 number  = "2",
 pages   = "146--89",
 abstract = "
We apply mathematical concept analysis to the problem of reengineering
configurations. Concept analysis will reconstruct a taxonomy of concepts from a
relation between objects and attributes. We use concept analysis to infer
configuration structures from existing source code. Our tool NORA/RECS will
accept source code, where configuration-specific code pieces are controlled by
the preprocessor. The algorithm will compute a so-called concept lattice,
which-when visually displayed-offers remarkable insight into the structure and
properties of possible configurations. The lattice not only displays
fine-grained dependencies between configurations, but also visualizes the
overall quality of configuration structures according to software engineering
principles. In a second step, interferences between configurations can be
analyzed in order to restructure or simplify configurations. Interferences
showing up in the lattice indicate high coupling and low cohesion between
configuration concepts. Source files can then be simplified according to the
lattice structure. Finally, we show how governing expressions can be simplified
by utilizing an isomorphism theorem of mathematical concept analysis (32 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

50

@Article{plessis93a,
 key     = "plessis93a",
 author  = "Annette L. du Plessis",
 title   = "A Method for {CASE} Tool Evaluation",
 journal = "Information and Management",
 month   = aug,
 year    = "1993",
 volume  = "25",
 number  = "2",
 pages   = "93--102",
 abstract = "
An epistemology for CASE that defines the current CASE technologies is
presented. Its influence on traditional software process models is considered,
and its support throughout a typical software development life cycle is
identified. A four stage software process model is used as a basis for
classifying CASE technology. A method for evaluating CASE products that follows
a procedural framework is presented. Five evaluation criteria sets are used
when applying the method to determine the CASE requirements for an organization
(23 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

51 (not too great)

@Article{kitchenham96a,
 key     = "kitchenham96a",
 author  = "Barbara Ann Kitchenham",
 title   = "Evaluating Software Engineering Methods and Tool. 3.
          Selecting an appropriate evaluation method --- practical
          issues",
 journal = j-sig,
 month   = jul,
 year    = "1996",
 volume  = "21",
 number  = "4",
 pages   = "9--12",
 abstract = "
A discussion is given on the DESMET software evaluation methodology. The author
considers some of the practical constraints that can affect the viability of an
evaluation exercise. It is clear from this discussion that there are a large
number of factors that need to be considered when you attempt to evaluate a
method or tool. There are two other major concerns that need to be discussed:
the influence of human factors and sociological issues such as staff motivation
and evaluator expectation; the way in which the different types of evaluation
method need to be organised once you have selected an appropriate method (0
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

52 (good paper)

@InProceedings{grass90a,
 key          = "grass90a",
 author       = "Judith E. Grass and Yih-Farn Chen",
 title        = "The {C}++ Information Abstractor",
 booktitle    = "{USENIX} {C}++ Conference",
 address      = "San Francisco, CA",
 month        = "9--11 " # apr,
 year         = "1990",
 pages        = "265--77",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "USENIX Assoc. Berkeley, CA",
 abstract = "
The C++ Information Abstractor, cia++, builds a database of information
extracted from C++ programs. The database can serve as a foundation for the
rapid development of C++ programming tools. Such tools include tools that
graphically display various views of the program structure, tools that answer
queries about program symbols and relationships, and tools that extract
self-contained components from a large system. Cia++ is a new abstractor
implementation based on the design of the C Information Abstractor. The paper
describes the conceptual model of cia++ and examples of relational views and
applications developed on the C++ program database. It also presents some
aspects of the implementation (8 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

53 (good paper)

@Article{griswold93a,
 key     = "griswold93a",
 author  = "William G. Griswold and David Notkin",
 title   = "Automated Assistance for Program Restructuring",
 journal = j-tsem,
 month   = jul,
 year    = "1993",
 volume  = "2",
 number  = "3",
 pages   = "228--69",
 abstract = "
Maintenance tends to degrade the structure of software, ultimately making
maintenance more costly. By separating structural manipulations from other
maintenance activities, the semantics of a system can be held constant by a
tool, assuring that no errors are introduced by restructuring. To allow the
maintenance team to focus on the aspects of restructuring and maintenance
requiring human judgment, a transformation-based tool can be provided-based on
a model that exploits preserving data flow dependence and control flow
dependence-to automate the repetitive, error-prone, and computationally
demanding aspects of restructuring. A set of automatable transformations is
introduced; their impact on structure is described, and their usefulness is
demonstrated in examples. A model to aid building meaning-preserving
restructuring transformations is described, and its realization in a
functioning prototype tool for restructuring Scheme programs is discussed (57
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

54

@Article{griswold95b,
 key     = "griswold95b",
 author  = "William G. Griswold and David Notkin",
 title   = "Architectural Tradeoffs for a Meaning-Preserving Program
          Restructuring Tool",
 journal = j-tse,
 month   = apr,
 year    = "1995",
 volume  = "21",
 number  = "4",
 pages   = "275--87",
 abstract = "
Maintaining the consistency of multiple program representations in a program
manipulation tool is difficult. I describe a hybrid software architecture for a
meaning-preserving program restructuring tool. Layering is the primary
architectural paradigm, which successively provides increasingly integrated and
unified abstract machines to implement the tool. However, layering does not
provide adequate control over extensibility or the independence of components,
so I also adopt the paradigm of keeping the key program abstractions separate
throughout the layering, providing independent columns of abstract data types.
A pair of columns is integrated by a mapping column that translates elements in
one column's data type into related elements in the other column's data type.
Thus, integration of function and separation of representation can be achieved
simultaneously. This hybrid architecture was crucial in overcoming severe
performance problems that became apparent once the basic tool was completed. By
taking advantage of the independence of the columns and the special
characteristics of meaning-preserving restructuring, it was possible to extend
one representation column of the architecture to the uppermost layer to provide
the required access for efficient updating without compromising independence.
The cost of the extended architecture is that the upper layers are no longer as
simple because they expose operations that only guarantee consistency under
careful usage. However, the structural constraints of the hybrid architecture
and the models for building the more complicated layers minimizes the negative
impact of this tradeoff (36 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

55 (not very good, but the only reference around)

@InProceedings{steffen85a,
 key          = "steffen85a",
 author       = "Joseph L. Steffen",
 title        = "Interactive Examination of a {C} Program with Cscope",
 booktitle    = "Proceedings of the Winter 1995 USENIX Conference",
 address      = "",
 month        = jan,
 year         = "1985",
 pages        = "170--175",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "USENIX Association",
 abstract = "
From the introduction:
Changing an unfamiliar program means learning enough about it to at least find
the code to change. Serious bugs can be introduced because the effects of
changes are not well understood. This is such a daunting task for large
programs that many desirable changes are not made. However changes may be
demanded by users to fix bugs or tailor the program to their needs. Thus we
need tools to help us learn how a program works.\par

...\par

Cscope is an interactive screen-oriented tool designed to meet these goals. It
answers questions like these from a symbol cross-reference that it builds the
first time it is used on the source files. On a later call, cscope rebuilds the
cross-reference only if a source file has changed or the list of source files
is different. When the cross-reference is rebuilt the data for the unchanged
files is copied from the old cross-reference, which makes rebuilding much
faster than the initial build.
",
 note         = "",
}

------------------------------------------------------------------------------

56

@Article{aho79a,
 key     = "aho79a",
 author  = "A. V. Aho and B. W. Kernighan and P. J. Weinberger",
 title   = "Awk --- A Pattern Scanning and Processing Language",
 journal = j-spe,
 month   = apr,
 year    = "1979",
 volume  = "9",
 number  = "4",
 pages   = "267--79",
 abstract = "
This paper describes the design and implementation of awk, a programming
language which searches a set of files for patterns, and performs specified
actions upon records or fields of records which match the patterns. Awk makes
common data selection and transformation operations easy to express (6 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

57

@Article{kontogiannis96a,
 key     = "kontogiannis96a",
 author  = "K. A. Kontogiannis and R. Demori and E. Merlo and M. Galler
          and M. Bernstein",
 title   = "Pattern Matching for Clone and Concept Detection",
 journal = "Automated Software Engineering",
 month   = "",
 year    = "1996",
 volume  = "3",
 number  = "1--2",
 pages   = "77--108",
 abstract = "
A legacy system is an operational, large scale software system that is
maintained beyond its first generation of programmers. It typically represents
a massive economic investment and is critical to the mission of the
organization it serves. As such systems age, they become increasingly complex
and brittle, and hence harder to maintain. They also become even more critical
to the survival of their organization because the business rules encoded within
the system are seldom documented elsewhere. Our research is concerned with
developing a suite of tools to aid the maintainers of legacy systems in
recovering the knowledge embodied within the system. The activities, known
collectively as `program understanding', are essential preludes for several key
processes, including maintenance and design recovery for reengineering. We
present three pattern matching techniques: source code metrics, a dynamic
programming algorithm for finding the best alignment between two code
fragments, and a statistical matching algorithm between abstract code
descriptions represented in an abstract language and actual source code. The
methods are applied to detect instances of code cloning in several moderately
sized production systems including tcsh, bash, and CLIPS. The programmer's
skill and experience are essential elements of our approach. Selection of
particular tools and analysis methods depends on the needs of the particular
task to be accomplished. Integration of the tools provides opportunities for
synergy, allowing the programmer to select the most appropriate tool for a
given task (28 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

58 (This is a 3 page article. No meat.)

@InProceedings{kitchenham92a,
 key          = "kitchenham92a",
 author       = "Barbara Kitchenham",
 title        = "A Methodology for Evaluating Software Engineering Methods and
               Tools",
 booktitle    = "Experimental Software Engineering Issues: Critical Assessment
               and Future Directions. Internation Workshop Proceedings",
 address      = "Dagstuhl Castle, Germany",
 month        = "14--18 " # sep,
 year         = "1992",
 pages        = "121--4",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Germany Springer-Verlag Berlin, Bermany 1993",
 abstract = "
The DESMET project is attempting to develop a methodology for evaluating
software engineering methods and tools.  DESMET supports quantitative and
qualitative assessments. It leads to organisation dependent comparisons using
locally defined measurements (0 Refs.)
",
 note         = "",
}

-----------------------------------------------------------------------------

59

@InProceedings{grass92a,
 key       = "grass92a",
 author    = "Judith E. Grass",
 title     = "Cdiff: A Syntax Directed Differencer for {C}++ Programs",
 booktitle = "USENIX {C}++ Technical Conference Proceedings",
 address   = "",
 month     = aug,
 year      = "1992",
 pages     = "181--93",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "USENIX Association, Berkeley, CA",
 abstract = "
Cdiff is a program that detects syntactically significant changes between
different versions of C++ programs. It is an application implemented within the
framework of the C++ Information Abstractor system and the newest member of the
CIA++toolkit. Because it is built upon this system, Cdiff can support complex
queries about the differences between versions.  The paper discusses why Cdiff
is a useful addition to a C++ programmer's toolkit and its implementation. It
contains examples of Cdiff queries that show how these queries make analyzing
implementation changes easier than is possible using common textual
differencing tools (i.e. diff). Two different releases of the lib/InterViews
library from InterViews 3.0 are used as a test case (9 Refs.)
",
 note      = "",
}

-------------------------------------------------------------------------------

60

@InProceedings{favaro96a,
 key       = "favaro96a",
 author    = "John Favaro",
 title     = "A Comparison of Approaches to Reuse Investment Analysis",
 booktitle = "Proceedings Fourth International Conference on Software
            Reuse",
 address   = "Orlando, FL",
 month     = "23--26 " # apr,
 year      = "1996",
 pages     = "136--45",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "IEEE",
 abstract = "
Software reuse economics has been the subject of vigorous study over the past
few years. Although significant progress has been made in the areas of reuse
metrics and cost estimation, work to date in reuse investment analysis has not
always reflected accepted mainstream financial analysis practices. This paper
compares several approaches that have been described in the reuse literature,
points out known problems and indicates remedies (22 Refs.)
",
 note      = "",
}

-------------------------------------------------------------------------------

61

@Article{kitchenham94a,
 key     = "kitchenham94a",
 author  = "B. A. Kitchenham",
 title   = "Critical Review of Quantitative Assessment",
 journal = "Software Engineering Journal",
 month   = mar,
 year    = "1994",
 volume  = "9",
 number  = "2",
 pages   = "43--53",
 abstract = "
The paper discusses several empirical studies reported in the literature aimed
at evaluating the benefits of using software engineering methods and tools. The
discussion highlights a number of problems associated with the methodology of
the studies. The main problems concerned the difficulty of formulating the
hypothesis to be tested, using surrogate measures, defining a control and
minimising the effect of personalities. Most of these problems are found in
many experimental situations, but the problem associated with the proper
definition of a control group seems to be a particular issue for software
experiments. The paper concludes with some guidelines for improving the
organisation of empirical studies (16 Refs)
",
 note    = "",
}

------------------------------------------------------------------------------

62

@Article{fagan76a,
 key     = "fagan76a",
 author  = "Michael E. Fagan",
 title   = "Design and Code Inspections to Reduce Errors in Program
          Development",
 journal = "{IBM} Systems Journal",
 month   = "",
 year    = "1976",
 volume  = "15",
 number  = "3",
 pages   = "182--211",
 abstract = "
Substantial net improvements in programming quality and productivity have been
obtained through the use of formal inspections of design and of code.
Improvements are made possible by a systematic and efficient design and code
verification process, with well-defined roles for inspection participants. The
manner in which inspection data is categorized and made suitable for process
analysis is an important factor in attaining the improvements. It is shown that
by using inspection results, a mechanism for initial error reduction followed
by ever-improving error rates can be achieved (13 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

63 (has many evaluation criteria for tools)

@Booklet{ieee93a,
 key          = "ieee93a",
 author       = "IEEE",
 title        = "{IEEE} Recommended Practice for the Evaluation and Selection
               of {CASE} Tools",
 howpublished = "Institute of Electrical and Electronics Engineers",
 month        = feb,
 year         = "1993",
 address      = "",
 abstract = "
The evaluation and selection of CASE tools supporting software engineering
processes, including project management processes, predevelopment processes,
development processes, postdevelopment processes, and integral processes, are
addressed. The evaluation and selection processes recommended are based upon
the perspective of a CASE tool user.  Therefore, the evaluation and selection
criteria address only program characteristics visible to the user, such as
program inputs and outputs, program function, user interfaces, and documented
external program interfaces (3 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

64

@Article{boehm88a,
 key     = "boehm88a",
 author  = "Barry W. Boehm",
 title   = "A Spiral Model of Software Development and Enhancement",
 journal = j-comp,
 month   = "",
 year    = "1988",
 volume  = "21",
 number  = "5",
 pages   = "61--72",
 abstract = "
A short description is given of software process models and the issues they
address. An outline is given of the process steps involved in the spiral model,
an evolving risk-driven approach that provides a framework for guiding the
software process, and its application to a software project is shown. A summary
is given of the primary advantages and implications involved in using the
spiral model and the primary difficulties in using it at its current incomplete
level of elaboration (25 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

65

@Article{parnas86a,
 key     = "parnas86a",
 author  = "David Lorge Parnas and Paul C. Clements",
 title   = "A Rational Design Process: How and Why to Fake It",
 journal = j-tse,
 month   = "",
 year    = "1986",
 volume  = "SE-12",
 number  = "2",
 pages   = "251--7",
 abstract = "
Many have sought a software design process that allows a program to be derived
systematically from a precise statement of requirements. It is proposed that,
although designing a real product in that way will not be successful, it is
possible to produce documentation that makes it appear that the software was
designed by such a process. The ideal process and the documentation that it
requires are described. The authors explain why one should attempt to design
according to the ideal process and why one should produce the documentation
that would have been produced by that process. The contents of each of the
required documents are outlined (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

66

@Article{basili75a,
 key     = "basili75a",
 author  = "Victor R. Basili and Albert J. Turner",
 title   = "Iterative Enhancement: A Practical Technique for Software
         Development",
 journal = j-tse,
 month   = "",
 year    = "1975",
 volume  = "SE-1",
 number  = "4",
 pages   = "390--6",
 abstract = "
This paper recommends the iterative enhancement' technique as a practical means
of using a top-down, stepwise refinement approach to software development. This
technique begins with a simple initial implementation of a property chosen
(skeletal) subproject which is followed by the gradual enhancement of
successive implementations in order to build the full implementation. The
development and quantitative analysis of a production compiler for the language
SIMPL-T is used to demonstrate that the application of iterative enhancement to
software development is practical and efficient, encourages the generation of
an easily modifiable product, and facilities reliability (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

67

@InProceedings{daneva96a,
 key       = "daneva96a",
 author    = "Maya Daneva and Radostina Terzieva",
 title     = "Assessing the Potentials of {CASE}-Tools in Software Process
            Improvement: A Benchmarking Study",
 booktitle = "Proceedings of the Fourth International Symposium on
            Assessment of Software Tools",
 address   = "Toronto, Ontario, Canada",
 month     = "22--24 " # may,
 year      = "1996",
 pages     = "104--8",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "",
 abstract = "
CASE tools have been thought as one of the most important means for
implementing the derived quality programs. Two basic questions should be
answered to find the right CASE tool: what attributes the CASE tools should
exhibit and how the existing tools can be ranked according to their capability
to support the quality program goals. We propose our solution based on the
concept of software benchmarking. First, we summarise the QR method, and then
we report on how it was applied in CASE tools assessment. Next we derive
appropriate benchmark sets for CASE tools on the base of the CMM, the standard
ISO 9000 and the MBNQA-criteria. We propose and implement a framework for CASE
tools evaluation and selection (16 Refs.)
",
 note      = "",
}

------------------------------------------------------------------------------

68

@InProceedings{hierholzer96a,
 key          = "hierholzer96a",
 author       = "Andreas Hierholzer and Georg Herzwurm",
 title        = "Methodology of a {CASE}-Tool Assessment",
 booktitle    = "Proceedings of the Fourth International Symposium on
               Assessment of Software Tools",
 address      = "Toronto, Ontario, Canada",
 month        = "22--24 " # may,
 year         = "1996",
 pages        = "97--8",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
This paper investigates the efficiency of international CASE tools, which were
offered on the German market in 1994. In considering a catalogue of more than
400 criteria, which has been developed in cooperation with industry, 17 CASE
tools are evaluated in the scope of a case study. Both the specific
requirements of a CASE user and the common requirements derived from the ISO
9000 are taken into account as well (2 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

69

@InProceedings{etzkorn94a,
 key          = "etzkorn94a",
 author       = "L. H. Etzkorn and C. G. Davis",
 title        = "A Documentation-Related Approach to Object-Oriented Program
               Understanding",
 booktitle    = "Proceedings. IEEE Third Workshop on Program Comprehension",
 address      = "Washington, DC",
 month        = "14--15 " # nov,
 year         = "1994",
 pages        = "39--45",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Object-oriented code is considered to be inherently more reusable than
functional decomposition code; however, object-oriented code can suffer from a
program understanding standpoint since good object-oriented style seems to
require a large number of small methods. Hence code for a particular task may
be scattered widely. Thus good semantics based tools are necessary. This paper
describes an approach to object-oriented code understanding that focuses
largely on informal linguistic aspects of code, such as comments and
identifiers (14 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

70

@Unpublished{reiss95a,
 key             = "reiss95a",
 author           = "Steven P. Reiss and Tony Davis",
 title           = "Experiences Writing Object-Oriented Compiler Front Ends",
 year           = "1995",
 abstract = "
This paper details our methods and experiences in writing two compiler front
ends, one for C++ and one for Object Pascal. We originally started to write a
front end for C++. The implementation was done in C++ using object-oriented
techniques. About nine months after we started work on the C++ front end, we
were asked to implement a compiler for Object Pascal. This was done by reusing
much of the code from the C++ front end to implement a translator from Object
Pascal into Sun Pascal. This work is interesting for the use of object-oriented
techniques for semantic processing in a compiler as an alternative to attribute
grammars, for the experience with reuse and for the technique3s used to parse
and represent a complex language such as C++.
",
 note           = "Unpublished",
 month           = jan,
}

------------------------------------------------------------------------------

71 ("Paper about Sage++ to appear in OONSKI `94" Can't find a reference)

@InProceedings{bodin94a,
 key          = "bodin94a",
 author       = "Francois Bodin and Peter Beckman and Dennis Gannon and Jacob
               Gotwals and Srinivas Narayana and Suresh Srinivas and Beata
               Winnicka",
 title        = "Sage++: An Object-Oriented Toolkit and Class Library for
               Building Fortran and {C}++ Restructuring Tools",
 booktitle    = "Proceedings. OONSKI '94",
 address      = "Oregon",
 month        = "",
 year         = "1994",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 note         = "",
}

------------------------------------------------------------------------------

72 (Paper on grep)

@InBook{aho80a,
 key          = "aho80a",
 author       = "A. V. Aho",
 title        = "Formal Language Theory: Perspectives and Open Problems",
 chapter      = "Pattern Matching in Strings",
 pages        = "325--347",
 publisher    = "Academic Press, New York",
 year         = "1980",
 volume       = "",
 series       = "",
 address      = "",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}

------------------------------------------------------------------------------

73

@InProceedings{ottenstein84a,
 key          = "ottenstein84a",
 author       = "Karl J. Ottenstein and Linda M. Ottenstein",
 title        = "The Program Dependence Graph in a Software Development
               Environment",
 booktitle    = "Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering
                 Symposium on Practical Software Development",
 address      = "Pittsburgh, PA",
 month        = "23--25 " # apr,
 year         = "1984",
 pages        = "177--84",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
The internal program representation chosen for a software development
environment plays a critical role in the nature of that environment. A form
should facilitate implementation and contribute to the responsiveness of the
environment to the user. The program dependence graph (PDG) may be a suitable
internal form. It allows programs to be sliced in linear time for debugging and
for use by language-directed editors. The slices obtained are more accurate
than those obtained with existing methods because I/O is accounted for
correctly and irrelevant statements on multi-statement lines are not displayed.
The PDG may be interpreted in a data driven fashion or may have highly
optimized (including vectorized) code produced from it. It is amenable to
incremental data flow analysis, improving response time to the user in an
interactive environment and facilitating debugging through data flow anomaly
detection. It may also offer a good basis for software complexity metrics,
adding to the completeness of an environment based on it (37 Refs.)
",
 note         = "Published as SIGPLAN Notices, Volume 19, Number 5.",
}

------------------------------------------------------------------------------

74

@Article{weiser84a,
 key     = "weiser84a",
 author  = "Mark Weiser",
 title   = "Program Slicing",
 journal = j-tse,
 month   = "",
 year    = "1984",
 volume  = "SE-10",
 number  = "4",
 pages   = "352--7",
 abstract = "
Program slicing is a method for automatically decomposing programs by analyzing
their data flow and control flow. Starting from a subset of a program's
behavior, slicing reduces that program to a minimal form which still produces
that behavior.  The reduced program, called a slice, is an independent program
guaranteed to represent faithfully the original program within the domain of
the specified subset of behavior. Some properties of slices are presented. In
particular, finding statement-minimal slices is in general unsolvable, but
using data flow analysis is sufficient to find approximate slices.  Potential
applications include automatic slicing tools for debugging and parallel
processing of slices (25 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

75

@InProceedings{batory95a,
 key          = "batory95a",
 author       = "Don Batory and Lou Coglianese and Mark Goodwin and Steve
               Shafer",
 title        = "Creating Reference Architectures: An Example from Avionics",
 booktitle    = "SSR'95: Symposium on Software Reusability",
 address      = "Seattle, WA",
 month        = "28--30 " # apr,
 year         = "1995",
 pages        = "27--37",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 journal      = "",
 publisher    = "",
 abstract = "
ADAGE is a project to define and build a domain-specific software architecture
(DSSA) environment for assisting the development of avionics software. A
central concept of DSSA is the use of software system generators to implement
component-based models of software synthesis in the target domain. We present
the ADAGE component-based model (or reference architecture) for avionics
software synthesis. We explain the modeling procedures used, review our initial
goals, show how component reuse is achieved, and examine what we were (and were
not) able to accomplish. The contributions of our paper are the avionics
reference architecture and the lessons that we learned; both may be beneficial
to others in future modeling efforts (27 Refs.)
",
 note         = "SIGSOFT Software Engineering Notes",
}

------------------------------------------------------------------------------

76

@Article{novak95a,
 key     = "novak95a",
 author  = "Gordon S. {Novak Jr.}",
 title   = "Creation of Views for Reuse of Software with Different Data
          Representations",
 journal = j-tse,
 month   = "",
 year    = "1995",
 volume  = "21",
 number  = "12",
 pages   = "993--1005",
 abstract = "
Software reuse is inhibited by the many different ways in which equivalent data
can be represented. We describe methods by which views can be constructed
semi-automatically to describe how application data types correspond to the
abstract types that are used in numerical generic algorithms. Given such views,
specialized versions of the generic algorithms that operate directly on the
application data can be produced by compilation. This enables reuse of the
generic algorithms for an application with minimal effort. Graphical user
interfaces allow views to be specified easily and rapidly. Algorithms are
presented for deriving, by symbolic algebra, equations that relate the
variables used in the application data to the variables needed for the generic
algorithms. Arbitrary application data structures are allowed. Units of
measurement are converted as needed. These techniques allow reuse of a single
version of a generic algorithm for a variety of possible data representations
and programming languages. These techniques can also be applied in data
conversion and in object-oriented, functional, and transformational programming
(55 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

77 (a well-written paper)

@InProceedings{sagiv96a,
 key          = "sagiv96a",
 author       = "Mooly Sagiv and Thomas Reps and Reinhard Wilhelm",
 title        = "Solving Shape-Analysis Problems in Languages with Destructive
               Updating",
 booktitle    = "Conference Record of POPL '96: The 23rd ACM SIGPLAN-SIGACT
               Symposium on Principles of Programming Languages",
 address      = "St. Petersburg Beach, FL",
 month        = "21--24 " # jan,
 year         = "1996",
 pages        = "16--31",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
This paper concerns the static analysis of programs that perform destructive
updating on heap-allocated storage.  We give an algorithm that conservatively
solves this problem by using a finite shape-graph to approximate the possible
'shapes' that heap-allocated structures in a program can take on.  In contrast
with previous work, our method is even accurate for certain programs that
update cyclic data structures.  For example, our method can determine that when
the input to a program that searches a list and splices in a new element is a
possibly circular list, the output is a possibly circular list. 23 Refs.
",
 note         = "",
}

------------------------------------------------------------------------------

78 (a classic paper)

@Article{parnas72a,
 key     = "parnas72a",
 author  = "D. L. Parnas",
 title   = "On the Criteria to be Used in Decomposing Systems into
          Modules",
 journal = j-cacm,
 month   = dec,
 year    = "1972",
 volume  = "15",
 number  = "12",
 pages   = "1053--8",
 abstract = "
This paper discusses modularization as a mechanism for improving the
flexibility and comprehensibility of a system while allowing the shortening of
its development time. The effectiveness of a `modularization' is dependent upon
the criteria used in dividing the system into modules. A system design problem
is presented and both a conventional and unconventional decomposition are
described. It is shown that the unconventional decompositions have distinct
advantages for the goals outlined. The criteria used in arriving at the
decompositions are discussed. The unconventional decomposition, if implemented
with the conventional assumption that a module consists of one or more
subroutines, will be less efficient in most cases. An alternative approach to
implementation which does not have this effect is sketched (11 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

79

@Article{dennis80a,
 key     = "dennis80a",
 author  = "Jack B. Dennis",
 title   = "Data Flow Supercomputers",
 journal = j-comp,
 month   = nov,
 year    = "1980",
 volume  = "13",
 number  = "11",
 pages   = "48--56",
 abstract = "
Programmability with increased performance? New strategies to attain this goal
include two approaches to data flow architecture: data flow multiprocessors and
the cell block architecture (25 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

80

@Article{waters81a,
 key     = "waters81a",
 author  = "Richard C. Waters",
 title   = "The Programmer's Apprentice: Knowledge Based Program Editing",
 journal = j-tse,
 month   = jan,
 year    = "1982",
 volume  = "SE-8",
 number  = "1",
 pages   = "1--12",
 abstract = "
An initial implementation of an interactive programming assistant system called
the programmer's apprentice (PA) is described. The PA is designed to be midway
between an improved programming methodology and an automatic programming
system. The intention is that the programmer will do the hard parts of design
and implementation while the PA will assist him wherever possible. One of the
major underpinnings of the PA is a representation (called a plan) for programs
which abstracts away from the inessential features of a program, and represents
the basic logical properties of the algorithm explicitly.

The current system is composed of five parts: an analyzer that can construct
the plan for a program; a coder that can create program text corresponding to a
plan; a drawer that can draw a graphical representation of a plan; a library of
plans for common algorithmic fragments; and a plan editor which makes it
possible for a programmer to modify a program by modifying its plan. The
greatest leverage provided by the system comes from the fact that a programmer
can rapidly and accurately build up a program by referring to the fragments in
the library and from the fact that the editor provides commands specifically
designed to facilitate program modification (5 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

81

@Article{weiser82a,
 key     = "weiser82a",
 author  = "Mark Weiser",
 title   = "Programmers Use Slices When Debugging",
 journal = j-cacm,
 month   = jul,
 year    = "1982",
 volume  = "25",
 number  = "7",
 pages   = "446--52",
 abstract = "
Computer programmers break apart large programs into smaller coherent pieces.
Each of these pieces: functions, subroutines, modules, or abstract datatypes,
is usually a contiguous piece of program text. The experiment reported here
shows that programmers also routinely break programs into one kind of coherent
piece which is not contiguous. When debugging unfamiliar programs programmers
use program pieces called slices which are sets of statements related by their
flow of data. The statements in a slice are not necessarily textually
contiguous, but may be scattered through a program (27 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

82 (good paper)

@InCollection{garlan93a,
 key       = "garlan93a",
 author    = "David Garlan and Mary Shaw",
 title     = "An Introduction to Software Architecture",
 booktitle = "Advances in Software Engineering and Knowledge Engineering",
 pages     = "1--40",
 publisher = "World Scientific Publishing Company",
 editor    = "V. Ambriola and G. Tortora",
 month     = "",
 year      = "1993",
 volume    = "1",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
As the size of software systems increases, the algorithms and data structures
of the computation no longer constitute the major design problems. When systems
are constructed from many components, the organization of the overall
system-the software architecture-presents a new set of design problems. This
level of design has been addressed in a number of ways including informal
diagrams and descriptive terms, module interconnection languages, templates and
frameworks for systems that serve the needs of specific domains, and formal
models of component integration mechanisms. We provide an introduction to the
emerging field of software architecture. We begin by considering a number of
common architectural styles upon which many systems are currently based and
show how different styles can be combined in a single design. Then we present
six case studies to illustrate how architectural representations can improve
our understanding of complex software systems. Finally, we survey some of the
outstanding problems in the field, and consider a few of the promising research
directions (44 Refs.)
",
 note      = "Large-scale architecture patterns: pipes and filters,
            layering, black-board systems.",
}

------------------------------------------------------------------------------

83 (good paper)

@Article{parnas79a,
 key     = "parnas79a",
 author  = "David L. Parnas",
 title   = "Designing Software for Ease of Extension and Contraction",
 journal = j-tse,
 month   = mar,
 year    = "1979",
 volume  = "SE-5",
 number  = "2",
 pages   = "128--38",
 abstract = "
Designing software to be extensible and easily contracted is discussed as a
special case of design for change. A number of ways that extension and
contraction problems manifest themselves in current software are explained.
Four steps in the design of software that is more flexible are then discussed.
The most critical step is the design of a software structure called the `uses'
relation. Some criteria for design decisions are given and illustrated using a
small example. It is shown that the identification of minimal subsets and
minimal extensions can lead to software that can be tailored to the needs of a
broad variety of users (21 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

84

@Article{treleaven82a,
 key     = "treleaven82a",
 author  = "Philip C. Treleaven and Richard P. Hopkins and Paul W.
          Rautenbach",
 title   = "Combining Data Flow and Control Flow Computing",
 journal = "The Computer Journal",
 month   = may,
 year    = "1982",
 volume  = "25",
 number  = "2",
 pages   = "207--17",
 abstract = "
A model of program organization for a parallel, data driven computer
architecture is presented which integrates the concepts of pure data flow
computation with those of `multi-thread' control flow computation. In a data
flow organization, data is passed directly from the instruction generating it
to those instructions consuming the data, and the availability of input data
signals an instruction to execute. In a control flow organization, data is
passed indirectly between instructions, via updatable memory cells, and
separate control signals cause an instruction to execute. This program
organization is presented at several levels of abstraction, progressing from
the `Combined' model of computation, through a discussion of the problems of
program representation (e.g. iteration, procedures and resource managers) in
data driven computers, to a logical description of a computer architecture
implementing the organization. The objective in developing this Combined model
is to investigate how the data flow and the control flow concepts interact (26
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

85

@Article{beck91a,
 key     = "beck91a",
 author  = "Micah Beck and Richard Johnson and Keshav Pingali",
 title   = "From Control Flow to Dataflow",
 journal = "Journal of Parallel and Distributed Computing",
 month   = jun,
 year    = "1991",
 volume  = "12",
 number  = "2",
 pages   = "118--29",
 abstract = "
Are imperative languages tied inseparably to the von Neumann model or can they
be implemented in some natural way on dataflow architectures? In the paper, the
authors show how imperative language programs can be translated into dataflow
graphs and executed on a dataflow machine like Monsoon. This translation can
exploit both fine-grain and coarse-grain parallelism in imperative language
programs. More importantly, the authors establish a close connection between
their work and current research in the imperative languages community on data
dependences, control dependences, program dependence graphs, and static single
assignment form. These results suggest that dataflow graphs can serve as an
executable intermediate representation in parallelizing compilers (23 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

86

@Article{moriconi90a,
 key     = "moriconi90a",
 author  = "Mark Moriconi and Timothy C. Winkler",
 title   = "Approximate Reasoning About the Semantic Effects of Program
          Changes",
 journal = j-tse,
 month   = sep,
 year    = "1990",
 volume  = "16",
 number  = "9",
 pages   = "980--92",
 abstract = "
It is pointed out that the incremental cost of a change to a program is often
disproportionately high because of inadequate means of determining the semantic
effects of the change. A practical logical technique for finding the semantic
effects of changes through a direct analysis of the program is presented. The
programming language features considered include parametrized modules,
procedures, and global variables. The logic described is approximate in that
weak (conservative) results sometimes are inferred. Isolating the exact effects
of a change is undecidable in general. The basis for an approximation is a
structural interpretation of the information-flow relationships among program
objects. The approximate inference system is concise, abstract, extensible, and
decidable, giving it significant advantages over the main alternative
formalizations. The authors' implementation of the logic records the
justification for each dependency to facilitate the interpretation of results
(32 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

87

@Article{podgurski90a,
 key     = "podgurski90a",
 author  = "Andy Podgurski and Lori A. Clarke",
 title   = "A Formal Model of Program Dependences and Its Implications for
          Software Testing, Debugging, and Maintenance",
 journal = j-tse,
 month   = sep,
 year    = "1990",
 volume  = "16",
 number  = "9",
 pages   = "965--79",
 abstract = "
A formal, general model of program dependences is presented and used to
evaluate several dependence-based software testing, debugging, and maintenance
techniques. Two generalizations of control and data flow dependence, called
weak and strong syntactic dependence, are introduced and related to a concept
called semantic dependence. Semantic dependence models the ability of a program
statement to affect the execution behavior of other statements. It is shown
that weak syntactic dependence is a necessary but not sufficient condition for
semantic dependence and that strong syntactic dependence is necessary but not
sufficient condition for a restricted form of semantic dependence that is
finitely demonstrated. These results are used to support some proposed uses of
program dependences, to controvert others, and to suggest new uses (28 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

88

@Article{gallagher91a,
 key     = "gallagher91a",
 author  = "Keith Brian Gallagher and James R. Lyle",
 title   = "Using Program Slicing in Software Maintenance",
 journal = j-tse,
 month   = aug,
 year    = "1991",
 volume  = "17",
 number  = "8",
 pages   = "751--61",
 abstract = "
Program slicing is applied to the software maintenance problem by extending the
notion of a program slice (that originally required both a variable and line
number) to a decomposition slice, one that captures all computation on a given
variable, i.e., is independent of line numbers. Using the lattice of single
variable decomposition slices ordered by set inclusion, it is shown how a
slice-based decomposition for programs can be formed. One can then delineate
the effects of a proposed change by isolating those effects in a single
component of the decomposition. This gives maintainers a straightforward
technique for determining those statements and variables which may be modified
in a component and those which may not.  Using the decomposition, a set of
principles to prohibit changes which will interfere with unmodified components
is provided.  These semantically consistent changes can then be merged back
into the original program in linear time (36 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

89

@Article{rugaber90a,
 key     = "rugaber90a",
 author  = "Spencer Rugaber and Stephen B. Ornburn and Richard J.
          LeBlanc",
 title   = "Recognizing Design Decisions in Programs",
 journal = j-soft,
 month   = jan,
 year    = "1990",
 volume  = "7",
 number  = "1",
 pages   = "46--54",
 abstract = "
The authors present a characterization of design decisions that is based on the
analysis of programming constructs. The characterization underlies a framework
for documenting and manipulating design information to facilitate maintenance
and reuse activities. They identify and describe the following categories of
design decisions: composition and decomposition; encapsulation and
interleaving; generalization and specialization; representation; data and
procedures; and function and relation. The authors discuss how to recognize and
represent design decisions (8 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

90

@InProceedings{washburne94a,
 key          = "washburne94a",
 author       = "Ted Washburne and Rolf Stachowitz and John Hawley and Harlan
               Romsdahl",
 title        = "Automatic Classification of Software Modules with
               Probabilistic Neural Networks",
 booktitle    = "1994 IEEE International Conference on Neural Networks. IEEE
               World Congress on Computational Intelligence",
 address      = "Orlando, FL",
 month        = "27 " # jun # "-- 2" # jul,
 year         = "1994",
 pages        = "3894--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Software understanding (SU) requires inspection of code to classify the code
modules into categories (or objects). To date, the comments available with the
code have not been used to extract valuable information. By using standard
information retrieval (IR) methods with neural networks, automatic
classification of the software modules can be done (3 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

91 (good paper)

@Article{goel85a,
 key     = "goel85a",
 author  = "Amrit L. Goel",
 title   = "Software Reliability Models: Assumptions, Limitations, and
          Applicability",
 journal = j-tse,
 month   = dec,
 year    = "1985",
 volume  = "SE-11",
 number  = "12",
 pages   = "1411--23",
 abstract = "
A number of analytical models have been proposed during the past 15 years for
assessing the reliability of a software system. The author presents an overview
of the key modeling approaches, provides a critical analysis of the underlying
assumptions, and assesses the limitations and applicability of those models
during the software development cycle. He also proposes a step-by-step
procedure for fitting a model and illustrates it via an analysis of failure
data from a medium-sized real-time command and control software system (48
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

92 (good paper)

@Article{shaw90a,
 key     = "shaw90a",
 author  = "Mary Shaw",
 title   = "Prospects for an Engineering Discipline of Software",
 journal = j-soft,
 month   = nov,
 year    = "1990",
 volume  = "7",
 number  = "6",
 pages   = "15--24",
 abstract = "
Although software engineering is not yet a true engineering discipline, it has
the potential to become one. Older engineering fields are examined to ascertain
the character that software engineering might have. The current state of
software technology is discussed, covering information processing as an
economic force, the growing role of software in critical applications, the
maturity of development techniques, and the scientific basis for software
engineering practice. Five basic steps that the software engineering profession
must take to become a true engineering discipline are described. They are:
understanding the nature of expertise, recognizing different ways to get
information, encouraging routine practice, expecting professional
specializations, and improving the coupling between science and commercial
practice (16 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

93 (good paper)

@TechReport{vienneau94a,
 key         = "vienneau94a",
 author      = "Robert Vienneau and Roy Senn",
 title       = "A State of the Art Report: Software Design Methods",
 institution = "Kaman Sciences Corporation, Data and Analysis Center for
              Software",
 month       = "",
 year        = "1994",
 number      = "",
 address     = "Griffiss AFB, NY",
 type        = "",
 abstract = "
This state-of-the-art review provides an analysis of the status of software
design methods. It was researched and published as a service of the Data &
Analysis Center for Software (DACS). The DACS is a Department of Defense
Information Analysis Center (IAC) whose mission is to support the development,
testing, validation, distribution and use of software engineering technologies.
The DACS is operated by Kaman Sciences Corporation of Utica, New York under the
sponsorship of the Defense Technical Information Center (DTIC), Alexandria,
Virginia. It is monitored by the U.S. Air Force's Rome Laboratory in Rome, New
York under contract number F30602-92-C-0158.

The topic of software design is an extensive one with a rich history. Views of
software design can range from very focused to those which cover the whole
spectrum of software development. This report attempts to provide readers with
a useful snapshot of software design technology that can be used as a tutorial
for the uninitiated, a starting point for detailed research, or a guide for
those who will be developing software in the future. The report includes
coverage on the nature of design, its evolution, its status, and directions for
the future. Section 2.0 covers the nature and history of design. Section 3.1
covers new paradigms for software development and the role design plays in
these paradigms. Section 3.2 examines programming paradigms. These paradigms do
not derive from development concerns, but do shape the form of program design
and software development paradigms. Section 3.3 discusses a selection of
specific design technologies that may standalone, or may be definable only in
the context of a larger software development methodology. Section 3.4 discusses
software design and its place in future integrated CASE environments. Section
3.5 describes the issues related to software design `In the Large' and how
these issues are being addressed. Section 3.6 discusses the need to enforce
good design within a methodology or software development environment. Finally,
Section 4.0 reflects the authors' perceptions of the state of the art of
software design as indicated by this research, and some ideas are discussed as
to where software design research could lead to from here. Object-Oriented
technology and its influences on software design are covered because this
technology promises to have a large impact on future software development.
",
 note        = "",
}

------------------------------------------------------------------------------

94

@TechReport{vienneau,
 key         = "vienneau",
 author      = "Robert L. Vienneau",
 title       = "The Present Value of Software Maintenance",
 institution = "Kaman Sciences Corporation, Data and Analysis Center for
              Software",
 month       = "",
 year        = "",
 number      = "",
 address     = "Griffiss AFB, NY",
 type        = "",
 abstract = "
Deciding to engage in a software project typically results in incurring costs
and generating revenues over a long time period. Introducing new technology
into the software process can likewise be considered an investment decision.
This paper presents capital budgeting techniques employed among financial
analysts and upper-levelmanagement to evaluate such investment decisions.
Examples are given to illustrate financial analysis techniques based on actual
data reported in the software engineering literature. Under proper time
discounting, the commonly reported effect of modern programming practices to
shift costs to earlier in the lifecycle decreases the net present value of a
project unless resulting gains in quality and productivity more than
compensate. The principal theses of this paper are that software managers
should use these techniques in performing cost analyses and that software
process measurement programs need to be designed with the goal of supporting
such analyses.
",
 note        = "",
}

------------------------------------------------------------------------------

95

@Article{knight93a,
 key     = "knight93a",
 author  = "John C. Knight and E. Ann Myers",
 title   = "An Improved Inspection Technique",
 journal = j-cacm,
 month   = nov,
 year    = "1993",
 volume  = "36",
 number  = "11",
 pages   = "51--61",
 abstract = "
Inspection is one of the most valuable tools the software engineer has
available, but the technology is not being exploited to its full potential. We
define an enhanced inspection technique called phased inspection that addresses
the deficiencies of existing inspection techniques. The most important goal of
phased inspection is rigor, so that engineers can trust the results of a
specific inspection and inspection results are repeatable. We also present
details of a toolset that supports phased inspection by providing the inspector
with as much computer assistance as possible and by checking for compliance
with the required process of phased inspection. Experimental evaluations of
phased inspections lead us to conclude that the goals are being partially
achieved and that further refinement of the checklists used and process
structure will permit further improvements in inspection efficiency (18 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

96

@Article{abowd95a,
 key     = "abowd95a",
 author  = "Gregory D. Abowd and Robert Allen and David Garlan",
 title   = "Formalizing Style to Understand Descriptions of Software
          Architecture",
 journal = j-tsem,
 month   = oct,
 year    = "1995",
 volume  = "4",
 number  = "4",
 pages   = "319--64",
 abstract = "
The software architecture of most systems is usually described informally and
diagrammatically by means of boxes and lines. In order for these descriptions
to be meaningful, the diagrams are understood by interpreting the boxes and
lines in specific, conventionalized ways. The informal, imprecise nature of
these interpretations has a number of limitations. In this article the authors
consider these conventionalized interpretations as architectural styles and
provide a formal framework for their uniform definition. In addition to
providing a template for precisely defining new architectural styles, this
framework allows for analysis within and between different architectural styles
(35 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

97

@Article{diaz93a,
 key     = "diaz93a",
 author  = "{Rub\'en} Prieto-{D\'iaz}",
 title   = "Status Report: Software Reusability",
 journal = j-soft,
 month   = may,
 year    = "1993",
 volume  = "10",
 number  = "3",
 pages   = "61--6",
 abstract = "
It is argued that the problem with software engineering is not a lack of reuse,
but a lack of widespread, systematic reuse.  The reuse research community is
focusing on formalizing reuse because it recognizes that substantial quality
and productivity payoffs will be achieved only if reuse is conducted
systematically and formally. The history of reuse, which is characterized by
this struggle to formalize in a setting where pragmatic problems are the norm
and fast informal solutions usually take precedence, is reviewed. Several reuse
methods are discussed (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

98

@Article{biggerstaff87a,
 key     = "biggerstaff87a",
 author  = "Ted Biggerstaff and Charles Richter",
 title   = "Reusability Framework, Assessment, and Directions",
 journal = j-soft,
 month   = mar,
 year    = "1987",
 volume  = "4",
 number  = "2",
 pages   = "41--9",
 abstract = "
The authors address the question of software reusability from the technology
viewpoint. They examine the technologies, either mature or emerging, that are
available to address reusability, how they work, and how they differ. A
framework is given for classifying the available technologies into those that
are compositioned and those that are generational (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

99

@InCollection{mcilroy69a,
 key       = "mcilroy69a",
 author    = "M. D. McIlroy",
 title     = "Mass Produced Software Components",
 booktitle = "Software Engineering",
 pages     = "138--150",
 publisher = "NATO Science Committee",
 editor    = "P. Naur and B. Randell",
 month     = jan,
 year      = "1969",
 volume    = "1",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
Software components (routines), to be widely applicable to different machines
and users, should be available in families arranged according to precision,
robustness, generality and time-space performance.  Existing sources of
components - manufacturers, software houses, users' groups and algorithm
collections - lack the breadth of interest or coherence of purpose to assemble
more than one or two members of such families, yet software production in the
large would be enormously helped by the availability of spectra of high quality
routines, quite as mechanical design is abetted by the existence of families of
structural shapes, screws or resistors. The talk will examine the kinds of
variability necessary in software components, ways of producing useful
inventories, types of components that are ripe for such standardization, and
methods of instituting pilot production.
",
 note      = "",
}

------------------------------------------------------------------------------

100

@InCollection{neighbors89a,
 key       = "neighbors89a",
 author    = "James M. Neighbors",
 title     = "Draco: A Method for Engineering Reusable Software Systems",
 booktitle = "Software Reusability --- Concepts and Models",
 pages     = "295--319",
 publisher = "IEEE",
 editor    = "Ted J. Biggerstaff and Alan J. Perlis",
 month     = "",
 year      = "1989",
 volume    = "I",
 chapter   = "12",
 series    = "",
 address   = "",
 edition   = "",
 note      = "",
}

------------------------------------------------------------------------------

101

@Article{krueger92a,
 key     = "krueger92a",
 author  = "Charles W. Krueger",
 title   = "Software Reuse",
 journal = j-acmcs,
 month   = jun,
 year    = "1992",
 volume  = "24",
 number  = "2",
 pages   = "131--83",
 abstract = "
This paper surveys the different approaches to software reuse found in the
research literature. It uses a taxonomy to describe and compare the different
approaches and make generalizations about the field of software reuse. The
taxonomy characterizes each reuse approach in terms of its reusable artifacts
and the way these artifacts are abstracted, selected, specialized, and
integrated. This survey will help answer the following questions: What is
software reuse? Why reuse software? What are the different approaches to
reusing software? How effective are the different approaches? What is required
to implement a software reuse technology? Why is software reuse difficult? What
are the open areas for research in software reuse? (105 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

102

@InProceedings{diaz87a,
 key          = "diaz87a",
 author       = "{Rub\'en} Prieto-{D\'iaz}",
 title        = "Domain Analysis for Reusability",
 booktitle    = "Proceedings of COMPSAC 87.",
 address      = "Tokyo, Japan",
 month        = "7--9 " # oct,
 year         = "1987",
 pages        = "23--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Domain analysis is a knowledge-intensive activity for which no methodology or
any kind of formalization is yet available.  Domain analysis is conducted
informally and all reported experiences concentrate on the outcome, not on the
process. The author proposes a model domain analysis process derived from
analyzing some domain analysis cases and two existing approaches. After
decomposition of the activities analyzed, he presents an approach to the domain
analysis process in a set of dataflow diagrams.  The model identifies
intermediate activities and workproducts for which support tools can be
developed. Work is currently being performed to verify the proposed model (18
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

103

@InProceedings{arango89a,
 key          = "arango89a",
 author       = "Guillermo Arango",
 title        = "Domain Analysis --- From Art Form to Engineering Discipline",
 booktitle    = "Proceedings of Fifth International Workshop on Software
               Specification and Design",
 address      = "Pittsburgh, PA",
 month        = "19--20 " # may,
 year         = "1989",
 pages        = "152--9",
 editor       = "Sol Greenspan",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The authors advance a conceptual framework for reusability. They do not offer a
detailed, canonical scheme of how every type of domain analysis is or ought to
be done. A set of principles has been identified providing coherence to a
diverse set of findings about domain analysis. Within this framework the author
has explored techniques for practical domain analysis.  The framework is useful
for comparing different approaches to domain analysis, and can be used for
guidance in developing other instances of methods and representations (43
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

104

@InCollection{lubars91a,
 key       = "lubars91a",
 author    = "Mitchell D. Lubars",
 title     = "Domain Analysis and Domain Engineering in {IDeA}",
 booktitle = "Domain Analysis and Software Systems Modeling",
 pages     = "163--78",
 publisher = "IEEE",
 editor    = "R. Prieto-Diaz and G. Arango",
 month     = "",
 year      = "1991",
 volume    = "1",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
IDeA (Intelligent Design Aid) is a design environment that was developed to
provide several forms of design assistance duling the construction of software
designs [Luba86]. One of IDeA's areas of concentration is in supporting the
reuse of abstract software designs, which are represented in the form of design
schemas [LuHa87]. Before IDeA can provide support for design reuse, a designer
experienced in certain application domains must populate IDeA's design reuse
library with the schemas for those new domains. This is currently done manually
and is an extremely tedious and difficult process. However, methodologies can
be employed to make the job more efficient. The development of reusable design
artifacts has often been referred to as domain analysis [Prie87] and domain
engineering [Aran87] In this paper we distinguish between the processes of
analyzing an application domain for reusability and the actual construction of
the reusable artifacts for that domain. We term the former domain analysis and
the later domain engineering.
",
 note      = "",
}

------------------------------------------------------------------------------

105

@Article{neighbors84a,
 key     = "neighbors84a",
 author  = "James M. Neighbors",
 title   = "The {Draco} Approach to Constructing Software From Reusable
          Components",
 journal = j-tse,
 month   = sep,
 year    = "1984",
 volume  = "SE-10",
 number  = "5",
 pages   = "564--74",
 abstract = "
The author discusses an approach called Draco to the construction of software
systems from reusable software parts. In particular, he is concerned with the
reuse of analysis and design information in addition to programming language
code.  The goal of the work on Draco has been to increase the productivity of
software specialists in the construction of similar systems. The particular
approach taken is to organize reusable software components by problem area or
domain.  Statements of programs in these specialized domains are then optimized
by source-to-source program transformation and refined into other domains. The
problems of maintaining the representational consistency of the developing
program and producing efficient practical programs are discussed. Some examples
from a prototype are also given (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

106

@InProceedings{simos95a,
 key          = "simos95a",
 author       = "Mark A. Simos",
 title        = "Organization Domain Modeling ({ODM}): Formalizing the Core
               Domain Modeling Life Cycle",
 booktitle    = "SSR'95: Symposium on Software Reusability",
 address      = "Seattle, WA",
 month        = "28--30 " # apr,
 year         = "1995",
 pages        = "196--205",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Researchers and practitioners are looking for systematic ways of comparing
domain analysis (DA) methods. Comparisons have often focused on linkage between
DA methods and related technologies such as systems modeling. Less attention
has been paid to comparing DA methods in terms of certain core methodological
issues, including problems of scoping, contextualizing, descriptive vs.
prescriptive modeling, and formalized models of variability. This paper
presents key aspects of organization domain modeling (ODM), a systematic domain
analysis method structured in terms of a core domain modeling life cycle
directly addressing these methodological concerns (24 Refs.)
",
 note         = "SIGSOFT Software Engineering Notes",
}

------------------------------------------------------------------------------

107

@InProceedings{france95a,
 key          = "france95a",
 author       = "Robert B. France and Thomas B. Horton",
 title        = "Applying Domain Analysis and Modeling: an Industrial
               Experience",
 booktitle    = "SSR'95: Symposium on Software Reusability",
 address      = "Seattle, WA",
 month        = "28--30 " # apr,
 year         = "1995",
 pages        = "206--14",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
In this paper, we describe our experience in applying domain analysis within a
company that develops personal electronic devices. We describe how we tailored
the Domain-Specific Software Architecture (DSSA) engineering method to suit our
needs and then present the process and representations that we found most
useful for this situation. The conclusions and lessons learned are useful
because few studies published at this time provide details about applications
of domain engineering in commercial development environments (10 Refs.)
",
 note         = "SIGSOFT Software Engineering Notes " # aug # " 1995",
}

------------------------------------------------------------------------------

108 (Got this from John Knight)

@InCollection{arango91a,
 key       = "arango91a",
 author    = "Guillermo Arango and {Rub\'en} Prieto-{D\'iaz}",
 title     = "Domain Analysis Concepts and Research Directions",
 booktitle = "Domain Analysis and Software Systems Modeling",
 pages     = "9--26",
 publisher = "IEEE",
 editor    = "",
 month     = "",
 year      = "1991",
 volume    = "1",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
Domain analysis is the process of identifying and organizing knowledge about
some class of problems -- the problem domain -- to support the description and
solution of those problems. Domain analysis involves learning. The
problem-solving process that domain analysis is intended to support imposes
particular conceptual structures on each problem domain. The articles in this
volume focus on domain analysis to support reuse-based software specification
and implementation. This context determines the selection of problem domains,
the types of knowledge that must be captured, and the kinds of representations
used to make that knowledge explicit and reusable.
",
 note      = "",
}

------------------------------------------------------------------------------

109 (Intro of paper:)

@InCollection{parnas89a,
 key       = "parnas89a",
 author    = "D. L. Parnas and P. C. Clements and D. M. Weiss",
 title     = "Enhancing Reusability with Information Hiding",
 booktitle = "Software Reusability --- Concepts and Models",
 pages     = "141--57",
 publisher = "IEEE",
 editor    = "Ted J. Biggerstaff and Alan J. Perlis",
 month     = "",
 year      = "1989",
 volume    = "I",
 chapter   = "6",
 series    = "",
 address   = "",
 edition   = "",
 abstract  = "
A number of reasons are commonly cited for not reusing software. Among them
are these:
1. The specifications of the software are either nonexistent or sufficiently
ambiguous so that it is not possible to determine exactly what the software
does without examining all of the source code. However, if the software is
complex enought to make reusability attractive, then it is complex enough to
thwart any effort to specify its behavior in this fashion.
2. The software performs a specialized task that resembles the required task,
but the cost of changing the software to perform the required task is greater
than the cost of writing new software.
3. Although the software to perform the required task may exist, nobody on the
new project knows about it or those who know of its existence don't know how to
find it.
4. Softare that can perform the required task is available, but it is so
general that it is too inefficient for the task.

In this paper we show how a well-known software design principle ameliorates
this situation. The next section discusses the principles that guide our
design. We then present a complete picture of software that is being built
according to these principles. In a concluding section, we discuss the above
problems in reusing software and show how our design supports reusability.
",
 note      = "",
}

------------------------------------------------------------------------------

110 (Part of intro of paper:)

@InCollection{prietodiaz89a,
 key       = "prietodiaz89a",
 author    = "{Rub\'en} Prieto-{D\'iaz}",
 title     = "Classification of Reusable Modules",
 booktitle = "Software Reusability --- Concepts and Models",
 pages     = "99--123",
 publisher = "IEEE",
 editor    = "Ted J. Biggerstaff and Alan J. Perlis",
 month     = "",
 year      = "1989",
 volume    = "I",
 chapter   = "4",
 series    = "",
 address   = "",
 edition   = "",
 abstract  = "
Classification of a collection is central to making code reusability an
attractive approach to software development. If the collection is organized by
attributes related to software requirements, then the pobability of retrieving
nonrelevant components is reduced. This in turn enhances understanding and
adaptation. A classified collection is of no use without a search-and-retrieval
mechnism, and an effective retrieval system requires a well-defined
classification structure. Furthermore, the classification and retrieval system
must help its users discriminate among very similar items in the collection. A
retrieved sample may contain several very similar components differing only in
minor implementation details. The user must select the components that require
the least adaptation effort.

In this chapter we present a model of the code reuse process as a basis of our
work followed by a brief introduction to faceted classification as used in
library science. Faceted schemes, in contrast with hierarchical classification
schemes such as Dewey decimal, are very flexible and well suited for dynamic
collections subject to continuous expansion and change.
",
 note      = "",
}

------------------------------------------------------------------------------

111

@Article{duvall95a,
 key     = "duvall95a",
 author  = "Lorraine M. Duvall",
 title   = "A Study of Software Management: The State of Practice in
          the {U}nited {S}tates and {J}apan",
 journal = j-jss,
 month   = nov,
 year    = "1995",
 volume  = "31",
 number  = "2",
 pages   = "109--24",
 abstract = "
The purpose of the study was to increase our understanding of the problems in
managing software development and the situations in which these problems
occurred-all from the perspective of software managers. The qualitative
research method used during this study was based on grounded theory, a
user-based approach from the social sciences that facilitates the discovery and
definition of generalizations and themes about a complex subject, such as
software development. Thirty-two managers from 14 companies in the United
States and Japan were interviewed. The results of the analysis of the data
collected suggest that many interacting technical and nontechnical factors come
into play. Two major differences between the development contexts of the
managers in the United States and Japan were related to development personnel
and constraints placed on the projects. Similar hardware, software tools, and
software processes were applied to their development efforts. Examination of
the technological aspects of software development showed few distinguishing
characteristics between the practices of the two countries. In contrast,
examination of management and sociological issues provides insight into the
differences, specifically those related to roles managers played with their
people, subcontractors, and customers (43 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

112

@Article{stevens74a,
 key     = "stevens74a",
 author  = "W. P. Stevens and G. J. Myers and L. L. Constantine",
 title   = "Structured Design",
 journal = "{IBM} Systems Journal",
 month   = "",
 year    = "1974",
 volume  = "13",
 number  = "2",
 pages   = "115--39",
 abstract = "
Considerations and techniques are proposed that reduce the complexity of
programs by dividing them into functional modules. This can make it possible to
create complex systems from simple independent reusable modules. Debugging and
modifying programs reconfiguring I/O devices and managing large programming
projects can all be greatly simplified. And, as the module library grows
increasingly sophisticated programs can be implemented less and less new code
(12 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

113

@Article{gargaro87a,
 key     = "gargaro87a",
 author  = "Anthony Gargaro and T. L. Pappas",
 title   = "Reusability Issues and {A}da",
 journal = j-soft,
 month   = jul,
 year    = "1987",
 volume  = "4",
 number  = "4",
 pages   = "43--51",
 abstract = "
Some guidelines are presented for writing reusable code when the methodology
does not address reusability. The focus is on mission-critical computer
resource (MCCR) software. The discussion covers transportability,
orthogonality. Ada run-time system dependencies, and the issue of efficiency
versus reusability (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

114 (From intro)

@InCollection{selby89a,
 key       = "selby89a",
 author    = "Richard W. Selby",
 title     = "Quantitative Studies of Software Reuse",
 booktitle = "Software Reusability --- Concepts and Models",
 pages     = "213--33",
 publisher = "IEEE",
 editor    = "Ted J. Biggerstaff and Alan J. Perlis",
 month     = "",
 year      = "1989",
 volume    = "I",
 chapter   = "11",
 series    = "",
 address   = "",
 edition   = "",
 abstract  = "
The purpose of this paper is to provide a basis for insights into approaches
for supporting software reuse. One method for learning about how software can
be reused is to study development organizations that actively reuse software.
Therefore, the focus of this paper is to examine empirical data from one
particular software development site that reuses software effectively.
Twenty-five software projects of moderate to large size have been selected for
this study from a NASA software production environment. The amount of software
either reused or modified from previous systems averages 32 percent per project
in this environment.
",
 note      = "",
}

------------------------------------------------------------------------------

115

@Article{wohlin94a,
 key     = "wohlin94a",
 author  = "Claes Wohlin and Per Runeson",
 title   = "Certification of Software Components",
 journal = j-tse,
 month   = jun,
 year    = "1994",
 volume  = "20",
 number  = "6",
 pages   = "494--9",
 abstract = "
Reuse is becoming one of the key areas in dealing with the cost and quality of
software systems. An important issue is the reliability of the components,
hence making certification of software components a critical area. The
objective of this article is to try to describe methods that can be used to
certify and measure the ability of software components to fulfil the
reliability requirements placed on them. A usage modelling technique is
presented, which can be used to formulate usage models for components. This
technique will make it possible not only to certify the components, but also to
certify the system containing the components. The usage model describes the
usage from a structural point of view, which is complemented with a profile
describing the expected usage in figures. The failure statistics from the usage
test form the input of a hypothesis certification model, which makes it
possible to certify a specific reliability level with a given degree of
confidence. The certification model is the basis for deciding whether the
component can be accepted, either for storage as a reusable component or for
reuse. It is concluded that the proposed method makes it possible to certify
software components, both when developing for and with reuse (8 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

116

@Article{perry90a,
 key     = "perry90a",
 author  = "Dewayne E. Perry and Gail E. Kaiser",
 title   = "Adequate Testing and Object-Oriented Programming",
 journal = "Journal of Object-Oriented Programming",
 month   = jan # "--" # feb,
 year    = "1990",
 volume  = "2",
 number  = "5",
 pages   = "13--19",
 abstract = "
A flaw has been uncovered in the general wisdom about object-oriented
languages-that `proven' classes can be reused as superclasses without retesting
the inherited code. On the contrary, inherited methods must be retested in most
contexts of reuse in order to meet the standards of adequate testing. The
authors prove this result by applying test adequacy axioms to certain major
features of object-oriented languages-in particular, encapsulation in classes,
overriding of inherited methods and multiple inheritance which pose various
difficulties for adequately testing a program. These results do not indicate
that there is a flaw in the general wisdom that classes promote reuse (which
they in fact do), but some of the attendant assumptions about reuse are
mistaken (that is, those concerning testing.) The authors explain the concepts
of specification- and program-based testing and describe criteria for adequate
testing. They list a set of axioms for test data adequacy developed in the
testing community for program-based testing. They then apply the adequacy
axioms to three features common to many object-oriented programming languages,
and show why the axioms may require inherited code to be retested (31 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

117

@Article{murphy94a,
 key     = "murphy94a",
 author  = "Gail C. Murphy and Paul Townsend and Pok Sze Wong",
 title   = "Experiences with Cluster and Class Testing",
 journal = j-cacm,
 month   = sep,
 year    = "1994",
 volume  = "37",
 number  = "9",
 pages   = "39--47",
 abstract = "
The article describes a testing approach and reports on the use of this
approach to test two new versions of the TRACS system. TRACS is a system
designed for use by a telecommunications company to monitor services used by
its large business customers, to identify network problems prior to service
degradation, and to respond to discovered problems on a proactive basis. The
original version of the TRACS system was tested using system and cluster tests.
Several problems, however, resulted from this approach. Functional dynamic
testing of application-specific clusters was retained in a revised approach to
testing, as well as a tool to support class testing, the Automated Class
Exerciser (ACE) tool used to execute the cluster test case (20 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

118

@Article{jorgensen94a,
 key     = "jorgensen94a",
 author  = "Paul C. Jorgensen and Carl Erickson",
 title   = "Object-Oriented Integration Testing",
 journal = j-cacm,
 month   = sep,
 year    = "1994",
 volume  = "37",
 number  = "9",
 pages   = "30--8",
 abstract = "
Taken together, the implications of traditional testing for object-oriented
integration testing require an appropriate construct for the integration level.
This construct should be compatible with composition, avoid the inappropriate
structure-based goals of traditional integration testing, support the
declarative aspect of object integration, and be clearly distinct from the
unit- and system-level constructs. We postulate five distinct levels of
object-oriented testing: a method, message quiescence, event quiescence, thread
testing, and thread interaction testing. Two new constructs for object-oriented
testing are presented and examples are given of their use: a method/message
path (MM-Path) is a sequence of method executions linked by messages; an atomic
system function (ASF) is an input port event, followed by a set of MM-Paths,
and terminated by an output port event (8 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

119

@InProceedings{shaw95a,
 key          = "shaw95a",
 author       = "Mary Shaw",
 title        = "Architectural Issues in Software Reuse: It's Not Just the
                Functionality, It's the Packaging",
 booktitle    = "Proceedings of the ACM SIGSOFT Symposium on Software
                Reusability",
 address      = "",
 month        = aug,
 year         = "1995",
 pages        = "3--6",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM Press",
 abstract = "
Effective reuse depends not only on finding and reusing components, but also on
the ways those components are combined.  The informal folklore of software
engineering provides a number of diverse styles for organizing software
systems. These styles, or architectures, show how to compose systems from
components; different styles expect different kinds of component packaging and
different kinds of interactions between the components. Unfortunately, these
styles and packaging distinctions are often implicit; as a consequence,
components with appropriate functionality may fail to work together. The paper
surveys common architectural styles, including important packaging and
interaction distinctions, and proposes an approach to the problem of
reconciling architectural mismatches (4 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

120

@Article{carter92a,
 key     = "carter92a",
 author  = "Everett Carter",
 title   = "The Evolution of a {C}++ Programmer",
 journal = "Computer Language",
 month   = aug,
 year    = "1992",
 volume  = "9",
 number  = "8",
 pages   = "53--67",
 abstract = "
About a year ago, I decided it was time to learn object-oriented programming.
Since I knew that C++ was built on C and I had been using C for several years,
I decided that C++ was the logical choice. Over the year, I noticed that my
code went through several distinct evolutionary steps on the way to becoming
truly object oriented. This article explains the steps in this development.

The examples I present are models of chaotic systems. They were chosen because
the code is relatively small, and yet they produce something interesting to
investigate. The early listings are incomplete, to show the stages of
evolution. The final, fully evolved, listings are complete programs. The
demonstration programs send their results to standard output, so they data can
be plotted or analyzed by other programs.
",
 note    = "",
}

------------------------------------------------------------------------------

121

@Article{baratta-perez94a,
 key     = "baratta-perez94a",
 author  = "Grace Baratta-Perez and Richard L. Conn and Charles A.
          Finnell and Thomas J. Walsh",
 title   = "Ada System Dependency Analyzer tool",
 journal = j-comp,
 month   = feb,
 year    = "1994",
 volume  = "27",
 number  = "2",
 pages   = "49--55",
 abstract = "
A major thrust of modern software engineering methods, languages, and tools is
to promote software visibility and to present information about the underlying
software architecture. With large, complex software systems, automated tools
are indispensable for identifying the architectural components, the structure
that interconnects them, and other subtle dependencies. This article describes
the construction of an Ada System Dependency Analyzer (SDA), a software
architecture analysis tool that generates a quantitative snapshot of an Ada
application's software architecture. The SDA can process thousands of Ada
source files during a single run and report on them as a group of files
comprising a single Ada system. Our SDA tool identifies Ada source code
dependencies on COTS products such as operating systems, compilers, the X
Window System, and on routines written in other languages, and can thus predict
software portability and reliability problems. It rapidly and accurately
processes 24,000 lines of code per minute (a time-consuming, if not impossible,
operation if done manually) and has successfully processed more than seven
million lines of code in eight complex systems. Although originally developed
for Ada, our methods and the technology we adopted will let us construct
analogous tools for other programming languages such as C, C++, Cobol; and PL/I
(11 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

122

@InProceedings{boehm92a,
 key          = "boehm92a",
 author       = "Barry W. Boehm and William L. Scherlis",
 title        = "megaprogramming",
 booktitle    = "Proceedings of the DARPA Software Technology Conference",
 address      = "",
 month        = "",
 year         = "1992",
 pages        = "63--82",
 editor       = "",
 organization = "Meridien Corp., Arlington, VA",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
'Megaprogramming' refers to the practice of building and evolving computer
software component by component. Megaprogramming builds on the processes and
technologies of software reuse, software engineering environments, software
architecture engineering, and application generation in order to provide a
component-oriented product-line approach to software development. In the
product line approach, management incentives and technology support can be
structured to favor the aggregate return on investment over a set of related
software products, even when certain portions of the investment -- whose
benefits are realized across the product line -- may be higher than they would
be if products were managed individually.
",
 note         = "",
}

------------------------------------------------------------------------------

123

@Article{garlan95a,
 key     = "garlan95a",
 author  = "David Garlan and Robert Allen and John Ockerbloom",
 title   = "Architectural Mismatch: Why Reuse is So Hard",
 journal = j-soft,
 month   = nov,
 year    = "1995",
 volume  = "12",
 number  = "6",
 pages   = "17--26",
 abstract = "
Many would argue that future breakthroughs in software productivity will
depend on our ability to combine existing pieces of software to produce
new applications.  An important step towards this goal is the development
of new techniques to detect and cope with mismatches in the assembled parts.
Some problems of composition am due to low-level issues of interoperability,
such as mismatches in programming languages or database schemas.  However,
in this paper we highlight a different, and in many ways more pervasive,
class of problem: architectural mismatch.  Specifically, we use our experience
in building a family of software design environments from existing parts to
illustrate a variety of types of mismatch that center around the assumptions
a r-eusable part makes about the structure of the application in which is
to appear.  Based on this experience we show how an architectural view of
the mismatch problem exposes some fundamental, thorny problems for software
composition and suggests possible research avenues needed to solve them.
",
 note    = "",
}

------------------------------------------------------------------------------

124

@Article{kernighan84a,
 key     = "kernighan84a",
 author  = "Brian W. Kernighan",
 title   = "The {U}nix System and Software Reusability",
 journal = j-tse,
 month   = sep,
 year    = "1984",
 volume  = "SE-10",
 number  = "5",
 pages   = "513--18",
 abstract = "
The author describes the facilities of the Unix system that enhance the reuse
of software. The Unix pipe, which makes whole programs building blocks of
larger computational structures, has been the primary reason for the
development of a literature of useful, but specialized programming language
such as C. It has led to high levels of program reuse both by the nature of its
operation and through its effect on programming conventions. The online C
source code for Unix system programs has led to a shared style of programming
in which existing programs are used as models for new programs, allowing the
reuse of ideas, algorithms, and source code. Finally, the Unix system contains
many other reuse enhancing facilities, such as generic facilities for screen
management (curses and termcap) and program generators (lex and yacc) (12
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

125

@Article{knight94a,
 key     = "knight94a",
 author  = "John Knight and Bev Littlewood",
 title   = "Critical Task of Writing Dependable Software",
 journal = j-soft,
 month   = jan,
 year    = "1994",
 volume  = "11",
 number  = "1",
 pages   = "16--20",
 abstract = "
Safety-critical software must perform as desired and should never fail. The
need for dependability stems from the fact that the consequences of failure are
extremely high, usually a threat to human life. To write such systems, most now
agree that we must adopt rigorous techniques, rooted in mathematics (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

126

@Article{betz94a,
 key     = "betz94a",
 author  = "Mark Betz",
 title   = "Interoperable Objects",
 journal = "Dr.\ {D}obb's Journal",
 month   = oct,
 year    = "1994",
 volume  = "19",
 number  = "11",
 pages   = "18--20",
 abstract = "
Distributed-object computing is swiftly shaping up as the next
computer-industry battleground. The author analyzes the specifications and
proposals, ranging from CORBA and SOM/DSOM to COM and OpenDoc. He discusses the
roots of distributed computing and compound documents (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

127

@InProceedings{sullivan97a,
 key          = "sullivan97a",
 author       = "Kevin J. Sullivan",
 title        = "Better, Faster, Cheaper Tools: A Case Study and
               Demonstration",
 booktitle    = "Annual Reliability and Maintainability Symposium 1997
               Proceedings",
 address      = "Philadelphia, PA",
 month        = "13--16 " # jan,
 year         = "1997",
 pages        = "216--19",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
In this paper, the author describes his approach to the rapid construction of
high-quality software tools for engineering, based on the integration of very
large-scale components-in this case, existing, commercial off-the-shelf,
shrink-wrapped application packages (8 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

128

@InProceedings{foy95a,
 key          = "foy95a",
 author       = "Marc Richard-Foy",
 title        = "Safe {A}da Executive: An Executive for {A}da Safety Critical
               Applications",
 booktitle    = "Ada in Europe. Second International Eurospace --- Ada
               Europe Symposium",
 address      = "Frankfurt, Germany",
 month        = "2--6 " # oct,
 year         = "1995",
 pages        = "11--20",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Germany Springer-Verlag Berlin, Germany",
 abstract = "
This paper presents the SAE project (Safe Ada Executive) which deals with the
DO-178 B certification for safety critical systems which use COTS (Commercial
Off The Shelves) software components such as the Ada Run Time System.
Traditionally safety critical systems avoid parallelism or rely on cyclic
dispatcher to achieve determinism. At the opposite this project shows that it
is possible to use the preemptive scheduler of the Ada Run Time System for
safety critical systems. The proposed model of the Safe Ada Executive is
designed to support a predictable subset of the Ada tasking. This real time
executive allows to support applications developed with the RMA (Rate Monotonic
Analysis) methods and an appropriate coding style. We point out this approach
which allows to separate applications from real time executive and to minimize
the certification costs. Anyhow, the SAE project, based on the Ada83 revision
of the language, has a stronger support with the Ada95 revision (6 Refs.)
",
 note         = "",
}

-------------------------------------------------------------------------------

129

@InProceedings{hall88a,
 key          = "hall88a",
 author       = "Fred Hall and Raymond A. Paul and Wendy E. Snow",
 title        = "{R\&M} Engineering for Off-the-shelf Critical Software",
 booktitle    = "Annual Reliability and Maintainability Symposium. 1988
               Proceedings",
 address      = "Los Angeles, CA",
 month        = "26--28 " # jan,
 year         = "1988",
 pages        = "218--26",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE New York, NY",
 abstract = "
The authors present some preliminary guidelines for evaluation of reliability
and maintainability (R&M) and certification of off-the-shelf software
components. These guidelines are established as engineering criteria which must
be achieved in each software component which is selected for reuse (21 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

130

@Article{liddiard94a,
 key     = "liddiard94a",
 author  = "J. Liddiard",
 title   = "Using Commercial Off-the-shelf Software in High Integrity and
          Safety Related Systems",
 journal = "Ada User (Netherlands)",
 month   = jun,
 year    = "1994",
 volume  = "15",
 number  = "2",
 pages   = "95--104",
 abstract = "
The specification, procurement, development, operation and maintenance of
software systems is fraught with risks. Such problems are just one aspect of an
overall `software crisis'.  One attempt to control the software crisis is the
increased use of commercially available off-the-shelf (COTS) software
components. Because COTS software components are marketed as products, there is
an understandable assumption that they are reliable, easy to use and well
behaved. With an increasing use of computer systems in roles which demand high
integrity and reliability, the effect of COTS software components on system
integrity is clearly an issue. This paper reports on an investigation into the
suitability of COTS software components for use in high integrity and safety
related systems. It concludes that COTS software components can constitute a
risk to system integrity, and that such risk is not directly under the control
of the software developer. Measures must therefore be taken to manage and
control the risk of using COTS software components (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

131

@InProceedings{craner88a,
 key          = "craner88a",
 author       = "B. C. Craner",
 title        = "Software Evaluation in the Laboratory --- {A}n Overview",
 booktitle    = "National Conference of Standards Laboratories 1988 Workshop
               and Symposium Technical Presentations: Competitiveness in a
               World Market",
 address      = "",
 month        = aug,
 year         = "1988",
 pages        = "606",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Nat. Conference Stand. Lab Boulder, CO",
 abstract = "
An overview of the steps taken to minimize software error, and maximize
software reliability and maintainability in laboratory software programs is
presented. The discussion is aimed at the measurement and calibration
laboratory arena, where resources committed to software development are not
available to prepare lengthy documentation, and are already minimal (/sup
1///sub 2/-3 programmers). The steps proposed are ways to minimize such waste
and maximize the software development efficiency during specification, resource
planning, coding, testing and validation.  Some explanation of preparedness for
software auditing by internal and external QA agencies are included. Brief
comments are also offered on the decision of whether to use off-the-shelf
applications programs or in-house designs. The test is written for persons not
necessarily familiar with software projects (31 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

132

@InProceedings{butler92a,
 key          = "butler92a",
 author       = "D. M. Butler",
 title        = "Technical Certification of Test Tools/Automatic Test Equipment
               ({ATE})",
 booktitle    = "Conference Record AUTOTESTCON '92. The IEEE Systems Readiness
               Technology Conference",
 address      = "",
 month        = sep,
 year         = "1992",
 pages        = "195--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE New York, NY",
 abstract = "
The authors describe a generic process for the certification of custom-made,
user modified and off-the-shelf tools including automatic test equipment (ATE)
and development tools. A standardized, comprehensive tool certification
procedure was developed. This procedure includes participation with many varied
groups from conception of the tool through final acceptance and its subsequent
use. The major areas of the procedure are addressed.  The use of this
comprehensive tool certification by an independent software quality engineering
group has interdicted tool errors that conceivably could have caused failure in
software/systems tests or allowed faulty deliverable hardware items to pass and
be integrated into final systems. The procedure also mandates correct
configuration control of tool software and hardware. This procedure encourages
total quality management (TQM). It has been shown to inspire better customer
confidence in the test tools and the whole testing process chain (1 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

133

@InProceedings{demko96a,
 key          = "demko96a",
 author       = "E. Demko",
 title        = "Commercial-off-the-Shelf ({COTS}): A Challenge to Military
               Equipment Reliability",
 booktitle    = "Annual Reliability and Maintainability Symposium.  1996
               Proceedings. The International Symposium on Product Quality
               and Integrity",
 address      = "Las Vegas, NV",
 month        = "22--25 " # jan,
 year         = "1996",
 pages        = "7--12",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE New York, NY",
 abstract = "
Commercial-off-the shelf (COTS) military equipment offers the promise of
technology advancement, low cost and reduced acquisition time. Unfortunately,
it also offers the opportunity for a reliability and logistics disaster because
commercial parts, standards, and practices may not meet military requirements.
Additionally, commercial vendors have little or no experience in providing the
kind of technical data required to support military deployment logistics. COTS
hardware is expected to have the following characteristics: low cost; currently
available from multiple suppliers; and built to documented standards in high
volume production with a mature design. The reliability challenge in the next
decade will be to manage COTS to take advantage of the promise and avoid the
disaster. This can be done by carefully selecting the COTS vendors, thoroughly
testing the hardware/software and applying only those analyses and data
requirements (ESS, EQT, verification testing, prediction, FMEA, derating, parts
control and FRACAS/FRB) to ensure reliability performance and logistics
support. This paper offers some guidance in managing COTS program elements and
provides insight into their impact on reliability (1 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

134

@Misc{lions96a,
 key    = "lions96a",
 author = "European Space Agency",
 title  = "Ariane 5: Flight 501 Failure. {R}eport by the Enquiry Board",
 url    = "http://www.esrin.esa.it/htdocs/tidc/Press/Press96/ariane5rep.html",
 note   = "URL: {\urlBiBTeX{http://www.esrin.esa.it/htdocs/tidc/Press/Press96/ariane5rep.html}}",
}

------------------------------------------------------------------------------

135

@InProceedings{mclean92a,
 key          = "mclean92a",
 author       = "J. McLean",
 title        = "New Paradigms for High Assurance Software",
 booktitle    = "Proceedings 1992--1993 ACM SIGSAC New Security Paradigms
               Workshop",
 address      = "Little Compton, RI",
 month        = "22--24 " # sep # " and 3--5 " # aug,
 year         = "1993",
 pages        = "42--7",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM New York, NY",
 abstract = "
We present a new paradigm for the development of trustworthy systems. It
differs from our current paradigm by separating distinct desiderata that are
bundled in the Trusted Computer System Evaluation Criteria, requiring that our
formalisms be tied to real world concerns, requiring a uniform method for
assuring that formalisms are met, replacing a code-then-validate methodology by
a refinement-based methodology, and using composability logic to develop
systems from COTS software (17 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

136

@Article{musa87a,
 key     = "musa87a",
 author  = "D. John Musa",
 title   = "Operational Profiles in Software Reliability Engineering",
 journal = j-soft,
 month   = sep,
 year    = "1987",
 volume  = "10",
 number  = "2",
 pages   = "14--32",
 abstract = "
A systematic approach to organizing the process of determining the operational
profile for guiding software development is presented. The operational profile
is a quantitative characterization of how a system will be used that shows how
to increase productivity and reliability and speed development by allocating
development resources to function on the basis of use.  Using an operational
profile to guide testing ensures that if testing is terminated and the software
is shipped because of schedule constraints, the most-used operations will have
received the most testing and the reliability level will be the maximum that is
practically achievable for the given test time. For guiding regression testing,
it efficiently allocates test cases in accordance with use, so the faults most
likely to be found, of those introduced by changes, are the ones that have the
most effect on reliability (5 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

137

@Article{profeta96a,
 key     = "profeta96a",
 author  = "J. A. Profeta III and N. P. Andrianos and Bing Yu and B. W.
          Johnson and T. A. DeLong and D. Guaspart and D. Jamsck",
 title   = "Safety-critical systems built with {COTS}",
 journal = j-comp,
 month   = nov,
 year    = "1996",
 volume  = "29",
 number  = "11",
 pages   = "54--60",
 abstract = "
In the rail transportation industry competitive pressure has led to the
increased use of COTS (commercial off-the-shelf equipment in safety critical
systems), making it imperative that we extend proven safety techniques to COTS
based systems as well. To this end, we have developed the Vital Framework
(V-Frame), which is used to develop a safety critical platform from COTS
hardware and software. The key technologies in this framework are formal
methods, information redundancy, a proprietary data format, and a concurrent
checking scheme.  Combining these technologies results in a real time,
checkable correctness criterion that is a signature of the application's
algorithm structure and is independent of both the hardware and the operating
system. V-Frame's most significant attribute is that the fail safe properties
of applications do not require the firmware to be correct: the application will
operate in a fail safe (or vital) manner even if there are design faults in the
operating system and/or the hardware fails. This does not mean that the
application does not have to be correctly specified and designed. Formal
methods are appropriate in the design of safety critical COTS systems because a
generic processing environment is analogous to a formal system: it is designed
to apply well defined transformation rules to inputs (7 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

138

@InProceedings{sha96a,
 key          = "sha96a",
 author       = "Lui Sha and R. Rajkumar and M. Gagliardi",
 title        = "Evolving Dependable Real-Time Systems",
 booktitle    = "1996 IEEE Aerospace Applications Conference. Proceedings",
 address      = "Aspen, CO",
 month        = "3--10 " # feb,
 year         = "1996",
 pages        = "335--46",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE New York, NY",
 abstract = "
To keep systems affordable, there is a trend towards using open standard and
commercial off the shelf (COTS) components in the development of dependable
real-time systems. However, the use of COTS also introduces the vendor-driven
upgrade problem that is relatively new to the dependable real-time computing
community. If we refuse to accept the `new and improved' hardware and software
components provided by vendors, then the hope that using COTS components will
help keep the system modern via market forces will be dashed. If we decide to
keep our systems modern, then we have to develop approaches that can introduce
new hardware and software components into deployed systems safely, reliably and
easily, in spite of the inevitable bugs in some of the new COTS components. In
this paper, we give an informal review of the Simplex Architecture, which has
been developed to support safe and reliable online upgrade of dependable
computing systems. This paper is a revision of the SEI technical report: A
Software Architecture for Dependable and Evolvable Industrial Computing
Systems.  CMU/SEI-95-TR-005 (9 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

139

@Article{horowitz84a,
 key     = "horowitz84a",
 author  = "Ellis Horowitz and John B. Munson",
 title   = "An Expansive View of Reusable Software",
 journal = j-tse,
 month   = sep,
 year    = "1984",
 volume  = "SE-10",
 number  = "5",
 pages   = "477--87",
 abstract = "
The concept of reusable software is examined in all of its forms, and the
current state of the art is assessed. In addition to its usual meaning of
reusable code, reusability includes reusable design, various forms of
specification systems, so-called application generators, and systems for
prototyping. Each approach is examined from the perspective of the practicing
engineer, and the work is evaluated in terms of how it may ultimately improve
the development process for large-scale software systems (33 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

140

@Book{pressman92a,
 key       = "pressman92a",
 author    = "Roger S. Pressman",
 title     = "Software Engineering",
 publisher = "McGraw-Hill Inc.",
 edition   = "Third",
 month     = "",
 year      = "1992",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

141

@Book{brockschmidt95a,
 key       = "brockschmidt95a",
 author    = "K. Brockschmidt",
 title     = "Inside {OLE}",
 publisher = "Microsoft Press, Redmond WA",
 edition   = "Second",
 month     = "",
 year      = "1995",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

142

@Book{omg95a,
 key       = "omg95a",
 author    = "",
 title     = "{CORBA}: Architecture and Specification",
 publisher = "Object Management Group, Inc.",
 edition   = "",
 month     = "",
 year      = "1995",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

143

@Article{merlo95a,
 key     = "merlo95a",
 author  = "Ettore Merlo and Pierre-Yves {Gagn\'e} and Jean-Francois Girard 
          and Kostas Kontogiannis and Laurie Hendren and Prakash
          Panangaden and Renato De Mori",
 title   = "Reengineering User Interfaces",
 journal = j-soft,
 month   = jan,
 year    = "1995",
 volume  = "12",
 number  = "1",
 pages   = "64--73",
 abstract = "
Most developers would like to avoid redesigning a system around a new
interface. But turning a character-based interface into a graphical one
requires significant time and resources. The authors describe how this process
can be partially automated, giving the results of their own reverse-engineering
effort (6 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

144

@Article{adler95a,
 key     = "adler95a",
 author  = "Richard M. Adler",
 title   = "Emerging Standards For Component Software",
 journal = j-comp,
 month   = mar,
 year    = "1995",
 volume  = "28",
 number  = "3",
 pages   = "68--77",
 abstract = "
Component software benefits include reusability and interoperability, among
others. What are the similarities and differences between the competing
standards for this new technology, and how will they interoperate?
Object-oriented technology is steadily gaining acceptance for commercial and
custom application development through programming languages such as C++ and
Smalltalk, object oriented CASE tools, databases, and operating systems such as
Next Computer's NextStep. Two emerging technologies, called compound documents
and component software, will likely accelerate the spread of objectoriented
concepts across system-level services, development tools, and application-level
behaviours. Tied closely to the popular client/server architecture for
distributed computing, compound documents and component software define
object-based models that facilitate interactions between independent programs.
These new approaches promise to simplify the design and implementation of
complex software applications and, equally important, simplify human-computer
interactive work models for application end users. Following unfortunate
tradition, major software vendors have developed competing standards to support
and drive compound document and component software technologies. These
incompatible standards specify distinct object models, data storage models, and
application interaction protocols. The incompatibilities have generated
confusion in the market, as independent software vendors, system integrators,
in-house developers, and end users struggle to sort out the standards' relative
merits, weaknesses, and chances for commercial success. Let's take a look now
at the general technical concepts underlying compound documents and component
software. Then we examine the OpenDoc, OLE 2, COM, and CORBA standards being
proposed for these two technologies. Finally, we'll review the work being done
to extend the standards and to achieve interoperability across them (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

145

@Article{coallier94a,
 key     = "coallier94a",
 author  = "Fran\c{c}ois Coallier",
 title   = "How {ISO} 9001 fits into the software world",
 journal = j-soft,
 month   = jan,
 year    = "1994",
 volume  = "11",
 number  = "1",
 pages   = "98--100",
 abstract = "
ISO 9001, together with its companion guide, ISO 9000-3, is a key standard for
many suppliers, but its focus is on the control of manufacturing processes. The
author considers how this fits into software development and how ISO 9001
stacks up to the US National Institute of Standards and Technology's Malcolm
Baldridge Quality Award and the Software Engineering Institutes's Capability
Maturity Model (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

146

@Article{sneed95a,
 key     = "sneed95a",
 author  = "Harry M. Sneed",
 title   = "Planning the Reengineering of Legacy Systems",
 journal = j-soft,
 month   = jan,
 year    = "1995",
 volume  = "12",
 number  = "1",
 pages   = "24--34",
 abstract = "
How can you know if reengineering is cost-effective? If it is preferable to new
development? Or to maintaining the status quo? The author proposes a way to
quantify the costs and prove the benefits of reengineering over other
alternatives and offers some advice on contracting a reengineering project (13
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

147

@InProceedings{deline97a,
 key          = "deline97a",
 author       = "Robert DeLine and Gregory Zelesnik and Mary Shaw",
 title        = "Lessons on Converting Batch Systems to Support Interaction",
 booktitle    = "Proceedings of the 19th International Conference on Software
               Engineering",
 address      = "Boston, Massachusetts",
 month        = "17--23 " # may,
 year         = "1997",
 pages        = "195--204",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 note         = "",
}

------------------------------------------------------------------------------

148

@Article{garlan95b,
 key     = "garlan95b",
 author  = "David Garlan",
 title   = "Research Directions in Software Architecture",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "257--61",
 abstract = "
Although application of good architectural design is becoming increasingly
important to software engineering, much common practice leads to architectural
designs that are informal, ad hoc, unanalyzable, unmaintainable, and
handcrafted. Consequently architectural design is only vaguely understood by
developers; architectural choices are based more on default than on solid
engineering principles; architectural designs cannot be analyzed for
consistency or completeness; architectures are not enforced as a system
evolves; and there are virtually no tools to help architectural designers.
Current research in software architecture attempts to address all of these
issues; among the more active areas are: architecture description languages;
formal underpinnings of software architecture; architectural analysis
techniques; architectural development methods; architecture recovery and
reengineering; architectural codification and guidance; tools and environments
for architectural design; and case studies of architectural design (13 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

149

@Article{wiederhold95a,
 key     = "wiederhold95a",
 author  = "Gio Wiederhold",
 title   = "Mediation in Information Systems",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "265--7",
 abstract = "
Information systems can be defined as computing systems that provide
information for their customers, often in a decision-support role. They are a
large and increasing fraction of modern computing, slowly eclipsing the effort
expended on massive data processing, cyle-intensive scientific computing, and
critical real-time systems. They are also quite visible, since they have a
large number of direct users. However, they do not stand alone.
",
 note    = "",
}

------------------------------------------------------------------------------

150

@Article{manola95a,
 key     = "manola95a",
 author  = "Frank Manola",
 title   = "Interoperability Issues in Large-Scale Distributed Object
          Systems",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "268--70",
 abstract = "
We focus on enterprise-wide client/server systems being developed to support
operational computing within large organizations to illustrate interoperability
issues. Requirements for these systems are not speculative; numerous large
businesses are building systems of this type today. The architecture is
distributed, and divided into three logical layers: applications, shared
services, and data. The layers are only logical groupings; all components
communicate via a common object-oriented messaging backplane. This reflects the
increasing agreement that modeling a system as a distributed collection of
objects provides the appropriate framework for integrating resources in these
environments, and is illustrated by the number of standards activities that are
moving toward adopting, or have already adopted, an object-oriented approach (8
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

151

@Article{heiler95a,
 key     = "heiler95a",
 author  = "Sandra Heiler",
 title   = "Semantic Interoperability",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "271--3",
 abstract = "
Interoperability among components of large-scale, distributed systems is the
ability to exchange services and data with one another. It is based on
agreements between requesters and providers on, for example, message passing
protocols, procedure names, error codes, and argument types. Semantic
interoperability ensures that these exchanges make sense-that the requester and
the provider have a common understanding of the `meanings' of the requested
services and data.  For example, an application that requests customer data
must agree on what a `customer' is, with the application that provides it.
Semantic interoperability is based on agreements on, for example algorithms for
computing requested values, the expected side effects of a re- quested
procedure, or the source or accuracy of requested data elements. The purposes
of this note are to indicate why semantic interoperability is so hard to
achieve, and to suggest that commerical repository technology can provide the
beginnings of help to make it easier
",
 note    = "",
}

------------------------------------------------------------------------------

152

@Article{sutherland95a,
 key     = "sutherland95a",
 author  = "Jeff Sutherland",
 title   = "Business Objects in Corporate Information Systems",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "274--6",
 abstract = "
Reorganization of business processes is most effective when there is a well
understood model of the existing business, an evaluation of alternative future
models against the current business, and when a model-driven approach is used
to realign the business strategy, processes, and technology. A multilayered,
object oriented blueprint of the enterprise can drive the refocusing,
realignment, and reorganization of the business. Current attempts to implement
this process under the rubric of business process reengineering (BPR) have led
to failure in 80% of the attempts, due primarily to soft factors such as lack
of motivation, leadership, and technical expertise. However, even a potentially
successful BPR often runs into technology roadblocks. The author discusses the
limitations of object technology and considers business object repositories (7
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

153

@Article{lewis95a,
 key     = "lewis95a",
 author  = "Ted Lewis",
 title   = "Where is Software Headed? {A} Virtual Roundtable",
 journal = j-comp,
 month   = aug,
 year    = "1995",
 volume  = "28",
 number  = "8",
 pages   = "20--32",
 abstract = "
To find out where software is headed, experts in academia and industry share
their vision of software's future.  It is a snapshot in time of where we have
been and possibly where we are headed. The subjects discussed are: the desktop;
software technology; objects; software agents; software engineering; parallel
software; and the curriculum. The results suggest a strong polarization within
the software community: a chasm exists between academia and industry. It
appears that these two groups share radically different views on where software
is headed. The impression is the heavy emphasis on programming languages,
operating systems and algorithms by the academic group, in contrast to the
clear emphasis on standards and market-leading trends by the industrial group.
Academics worry about evolutionary or incremental changes to already poorly
designed languages and systems, while industrialists race to keep up with
revolutionary changes in everything. Academics are looking for better ideas,
industrialists for better tools. To an industrial person, things are moving
fast-they are revolutionary. To an academic, things are moving too slowly, and
in the wrong direction-they are only evolutionary changes which are slave to an
installed base (4 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

(In the proceedings)
[POP reference]

@InProceedings{sullivan97b,
 key          = "sullivan97b",
 author       = "Kevin J. Sullivan and  Jake Cockrell and  Shengtong Zhang and
               David Coppit",
 title        = "Package-Oriented Programming of Engineering Tools",
 booktitle    = "Proceedings of the 19th International Conference on Software
               Engineering",
 address      = "Boston, Massachusetts",
 month        = "17--23 " # may,
 year         = "1997",
 pages        = "616--617",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
We present an innovative fault-tree analysis tool that we developed using a
novel software architectural style that we call package-oriented programming
(POP). The tool is largely implemented using multiple, tightly integrated,
shrink-wrapped software packages as components. These packages are integrated
using custom-coded integration mediators into a cohesive superstructure that
provides much of the functioning required of many sophisticated modeling and
analysis tools. In addition to serving as a proof of concept, the tool provides
a test-bed for further research on the approach.
 ",
 note         = "",
}

------------------------------------------------------------------------------

154

@Article{redmond-pyle96a,
 key     = "redmond-pyle96a",
 author  = "David Redmond-Pyle",
 title   = "Software Development Methods and Tools: Some Trends and
         Issues",
 journal = "Software Engineering Journal",
 month   = mar,
 year    = "1996",
 volume  = "11",
 number  = "2",
 pages   = "99--103",
 abstract = "
Rapid changes in hardware and software architecture are transforming the nature
of application software systems, leading to upheaval in the methods and tools
used to develop software. The paper offers a brief review of developments and
dilemmas in the state and usage of structured and object-oriented methods, RAD,
GUI design and software process management, and CASE tools and repositories. It
notes an emphasis on technology-specific skills and engineering pragmatism over
software engineering theory (11 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

155

@InProceedings{anjur96a,
 key          = "anjur96a",
 author       = "Vaishnavi Anjur and Yannis E. Ioannidis and Miron Livny",
 title        = "{FROG} and {TURTLE}: Visual Bridges Between Files and
               Object-Oriented Data",
 booktitle    = "Proceedings of the Eighth International Conference on
                 Scientific and Statistical Database Management",
 address      = "Stockholm, Sweden",
 month        = "18--20 " # jun,
 year         = "1996",
 pages        = "76--85",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The problem of translating database objects into a flat format to be written
out in a flat Ascii file or, conversely, translating the contents of a file
into a complex database object arises in several applications. It is especially
important in scientific database applications, where file-based communication
with external programs (e.g., visualization packages or model simulations) is
very common. We introduce Frog, a visual tool that can be used to specify
translations between database objects and flat files, requiring no programming
by the user. The tool can deal with objects of arbitrary complexity, without
the object complexity being directly reflected in the complexity of the
corresponding visual interaction. Based on the visual actions of the user, the
tool stores enough information in a map-file, whose contents are used at
run-time by another tool, Turtle, to translate any chosen database object into
the appropriate file layout. The tool has been developed as part of the ZOO
desktop experiment management environment and has been used by a few
experimental scientists with success (12 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

156

@TechReport{pergande93a,
 key         = "pergande93a",
 author      = "Michael Pergande",
 title       = "Inside {STF}",
 institution = "Forschungszentrum Informatik (FZI)",
 month       = feb,
 year        = "1993",
 number      = "FZI.055.0",
 address     = "Karlsruhe, Germany",
 type        = "",
 abstract = "
This documentation describes the structurer and flattener, a tool for creating
parsers and pretty printers based on OBST. It begins with a description and
picture of the structure of the stf-grammar object, which is generated from a
given stf-grammar in the first phase of the tool's work, and continues with an
explanation how the methods gen_structurer and gen_flattener work on such a
grammar object to finally produce the structurer and flattener for a given
stf-grammar in the second phase of the tool's work. In addition it contains a
very short illustrating example, which shows all the described features. It
concludes with an illustration of the dependency structure of the stf source
files and the generated files including all file names. note: before reading
this document, it is recommended to read the obst-stf man pages.
",
 note        = "",
}

------------------------------------------------------------------------------

157

@TechReport{casais92a,
 key         = "casais92a",
 author      = "Eduardo Casais and Michael Ranft and Bernhard Schiefer and
              Dietmar Theobald and Walter Zimmer",
 title       = "{OBST} --- {A}n Overview",
 institution = "Forschungszentrum Informatik (FZI)",
 month       = jun,
 year        = "1992",
 number      = "FZI.039.1",
 address     = "Karlsruhe, Germany",
 type        = "",
 abstract = "
This paper gives an overview of the OBST persistent object management system.
OBST has been developed by the Forschungszentrum Informatik (FZI) as a contri
bution to the STONE project. 1 The paper summarizes the design objectives and
the current status of OBST, and highlights forthcoming developments with the
system.
",
 note        = "",
}

------------------------------------------------------------------------------

158

@Article{ning94a,
 key     = "ning94a",
 author  = "Jim Q. Ning and Andre Engberts and W. (Voytek) Kozaczynski",
 title   = "Automated Support for Legacy Code Understanding",
 journal = j-cacm,
 month   = may,
 year    = "1994",
 volume  = "37",
 number  = "5",
 pages   = "50--7",
 abstract = "
Many large companies are facing a problem: their legacy systems are inhibiting
their business growth and capacity to change. This problem can be approached in
a number of ways. One way is to simply get rid of a legacy system, replacing it
with a new one developed to meet new requirements. This option is rarely
exercised for many reasons. The article discusses the concept of reusable
component recovery (RCR). Functional components of legacy systems are
recognized, recovered, adapted, and finally reused in new system development.
Since the recovered components are generally much smaller in scale than the
original system, their reusability is higher and they can be more easily
distributed to meet different platform requirements. The drawback of this
approach is that it requires deep analysis and understanding of old code, which
is a difficult, human dependent, and time-consuming task. In this article, we
present our views of how to use software reengineering tools to support deep
analysis and understanding of code necessary for RCR (20 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

159

@Article{leveson86a,
 key     = "leveson86a",
 author  = "Nancy G. Leveson",
 title   = "Software Safety: Why, What and How",
 journal = j-acmcs,
 month   = jun,
 year    = "1986",
 volume  = "18",
 number  = "2",
 pages   = "125--63",
 abstract = "
Software safety issues become important when computers are used to control
real-time, safety-critical processes. This survey attempts to explain why there
is a problem, what the problem is, and what is known about how to solve it.
Since this is a relatively new software research area, emphasis is placed on
delineating the outstanding issues and research topics (118 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

160

@Article{chang87a,
 key     = "chang87a",
 author  = "Shi-Kuo Chang",
 title   = "Visual Languages: A Tutorial and Survey",
 journal = j-soft,
 month   = jan,
 year    = "1987",
 volume  = "4",
 number  = "1",
 pages   = "29--39",
 abstract = "
The author defines four types of visual language: languages that support visual
interaction; visual programming languages; visual information processing
languages; and iconic visual information processing languages. He then examines
each type as regards their characteristics and how they are used. The role of
cognitive aspects in evaluating whether visual languages are adequate for
man-machine interaction is discussed (11 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

161

@Article{cavano87a,
 key     = "cavano87a",
 author  = "Joseph P. Cavano and Frank S. LaMonica",
 title   = "Quality Assurance in Future Development Environments",
 journal = j-soft,
 month   = sep,
 year    = "1987",
 volume  = "4",
 number  = "5",
 pages   = "26--34",
 abstract = "
The nature of software quality assurance (SQA) today is examined, including
coverage inspection, quality control, quality improvement, quality by design,
and quality metrics. The aspects of development environments that impact the
quality issue are discussed. SQA functions, traditionally performed manually or
not at all, that will be integrated in future software development environments
are discussed; they are quality planning, quality specification, data
collection, quality analysis, traceability and change-effect analysis,
automated documentation, and project management (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

162

@Article{gansner93a,
 key     = "gansner93a",
 author  = "Emden R. Gansner and Eleftherios Koutsofios and Stephen C.
          North and Kiem-Phong Vo",
 title   = "A Technique for Drawing Directed Graphs",
 journal = j-tse,
 month   = mar,
 year    = "1993",
 volume  = "19",
 number  = "3",
 pages   = "214--30",
 abstract = "
A four-pass algorithm for drawing directed graphs is presented. The fist pass
finds an optimal rank assignment using a network simplex algorithm. The seconds
pass sets the vertex order within ranks by an iterative heuristic,
incorporating a novel weight function and local transpositions to reduce
crossings.  The third pass finds optimal coordinates for nodes by constructing
and ranking an auxiliary graph. The fourth pass makes splines to draw edges.
The algorithm creates good drawings and is fast (20 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

163

@InProceedings{moore96a,
 key          = "moore96a",
 author       = "Melody M. Moore",
 title        = "Rule-Based Detection for Reverse Engineering User Interfaces",
 booktitle    = "Proceedings of the Third Working Conference on Reverse
               Engineering",
 address      = "Monterey, CA",
 month        = "8--10 " # nov,
 year         = "1996",
 pages        = "42--8",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Reengineering the user interface can be a critical part of the migration of any
large information system.  The paper details experiences with manually reverse
engineering legacy applications to build a model of the user interface
functionality, and to develop a technique for partially automating this
process. The results show that a language-independent set of rules can be used
to detect user interface components from legacy code, and also illustrate
problems that require dynamic analysis to solve them (10 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

164

@InProceedings{neighbors96a,
 key          = "neighbors96a",
 author       = "James M. Neighbors",
 title        = "Finding Reusable Software Components in Large Systems",
 booktitle    = "Proceedings of the Third Working Conference on Reverse
               Engineering",
 address      = "Monterey, CA",
 month        = "8--10 " # nov,
 year         = "1996",
 pages        = "2--10",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The extraction of reusable software components from existing systems is an
attractive idea. The goal of he paper is not to extract a component
automatically, but to identify its tightly coupled region subsystem) for
extraction by hand or knowledge-based system. Much of the author's experience
is anecdotal. His experience with scientific systems differs from much of the
work in reverse engineering that focuses on COBOL systems. Module and data
interconnection was collected from three large cientific systems over a 12
year period from 1980 to 1992. The interconnection data was analyzed in an
attempt to identify subsystems that correspond to domain-specific components.
The difficulties of dealing with large scientific systems and their
organizations are discussed. The failures and successes of various subsystem
analysis methods is discussed. A simple algorithm for the identification of
subsystems is presented. A pattern of object hierarchies of subsystems is
briefly mentioned.  The average subsystem is surprisingly large at 17000
source lines and 35 modules. The concept of a subsystem is informally
validated by developers from subsystem interconnection diagrams. The actual
reusability of these identified components is not assessed (16 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

165

@Article{ambriola88a,
 key     = "ambriola88a",
 author  = "Vincenzo Ambriola and David Notkin",
 title   = "Reasoning About Interactive Systems",
 journal = j-tse,
 month   = feb,
 year    = "1988",
 volume  = "14",
 number  = "2",
 pages   = "272--6",
 abstract = "
Interactive systems have goals and characteristics that differ from those of
batch systems. These differences lead to a need for new techniques, methods,
and tools for manipulating and constructing interactive systems. The difference
in structure between batch and interactive systems. The difference is
considered, focusing on the distinction between command decomposition and
component decomposition. The possible ways of solving a problem using an
interactive system using action paths, which account for the relatively
unconstrained actions of interactive users, are described. It is shown that
interactivity is not an inherent characteristic of a system but rather a
characteristic that depends on the error profile of its users. The requirements
that interaction places on the underlying implementation, specifically the need
for incrementality and integration, are considered. The results are applied to
several existing classes of systems (18 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

166

@Article{haines95a,
 key     = "haines95a",
 author  = "Matthew Haines and Piyush Megrotra and John Van Rosendale",
 title   = "SmartFiles: An {OO} Approach to Data File Interoperability",
 journal = "SIGPLAN Notices",
 month   = "",
 year    = "1995",
 volume  = "30",
 number  = "10",
 pages   = "453--66",
 abstract = "
Data files for scientific and engineering codes typically consist of a series
of raw data values whose description is buried in the programs that interact
with these files. In this situation, making even minor changes in the file
structure or sharing files between programs (interoperability) can only be done
after careful examination of the data files and the I/O statements of the
programs interacting with this file. In short, scientific data files lack
self-description, and other self-describing data techniques are not always
appropriate or useful for scientific data files. By applying an object-oriented
methodology to data files, the authors can add the intelligence required to
improve data interoperability and provide an elegant mechanism for supporting
complex, evolving, or multidisciplinary applications, while still supporting
legacy codes. As a result, scientists and engineers should be able to share
datasets with far greater ease, simplifying multidisciplinary applications and
greatly facilitating remote collaboration between scientists (11 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

167

@Article{markosian94a,
 key     = "markosian94a",
 author  = "Lawrence Markosian and Philip Newcomb and Russell Brand and
          Scott Burson and Ted Kitzmiller",
 title   = "Using an Enabling Technology to Reengineer Legacy Systems",
 journal = j-cacm,
 month   = "",
 year    = "1994",
 volume  = "37",
 number  = "5",
 pages   = "58--70",
 abstract = "
We describe our experience in applying a new enabling technology to automate
reengineering a massive legacy management information system. The enabling
technology supports rapid development of tools for analyzing and systematically
modifying existing systems. Using this technology, we were able to develop and
alpha-test a tool for performing complex, global data flow analysis and
transformation of Cobol programs having 40,000 lines of code in a single
compilation unit. The analyses and transformations that we implemented were
dictated by maintenance practices used by a particular MIS shop, and the
alpha-test results showed that the approach we describe can be cost-effective
even when the cost of tool development must be paid by a single reengineering
project (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

168

@Article{biggerstaff94a,
 key     = "biggerstaff94a",
 author  = "Ted J. Biggerstaff and Bharat G. Mitbander and Dallas E.
          Webster",
 title   = "Program Understanding and the Concept Assignment Problem",
 journal = j-cacm,
 month   = "",
 year    = "1994",
 volume  = "37",
 number  = "5",
 pages   = "72--82",
 abstract = "
A person understands a program when able to explain the program, its structure,
its behavior, its effects on its operational context, and its relationships to
its application domain in terms that are qualitatively different from the
tokens used to construct the source code of the program. When a person tries to
develop an understanding of an unfamiliar program or portion of a program, the
informal, human-oriented expression of computational intent must be created or
reconstructed through a process of analysis, experimentation, guessing, and
crossword puzzle-like assembly. As the informal concepts are discovered and
interrelated concept by concept, they are simultaneously associated with or
assigned to the specific implementation structures within the program (and its
operational context) that are the concrete instances of those concepts. The
problem of discovering these human-oriented concepts and assigning them to
their realizations within a specific program or its context is the concept
assignment problem. In practice, there are several general strategies and
classes of tools that can successfully address this problem. We illustrate some
of these strategies through example scenarios and some classes of tools that
support them through examples of the DESIRE (Design Information Recovery
Environment) suite of tools (8 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

169

@InProceedings{sneed96a,
 key          = "sneed96a",
 author       = "Harry M. Sneed",
 title        = "Encapsulating Legacy Software for Use in Client/Server
               Systems",
 booktitle    = "Proceedings of the Third Working Conference on Reverse
               Engineering",
 address      = "Monterey, CA",
 month        = "8--10 " # nov,
 year         = "1996",
 pages        = "104--19",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
An alternative to migrating legacy software from the mainframe to a
client/server platform is to encapsulate it by wrapping it on the host and
accessing it from the new client programs. Recent technological achievements
such as the OMG's Common Object Request Broker Architecture (CORBA) and IBM's
System Object Model (SOM) have made it possible for distributed objects to
communicate with existing applications on the mainframe. However as described
here, some alterations have to be made to the old programs to make them
accessible. Achieving this promises a new form of coexistence between
monolithic legacy systems on the mainframe and object-oriented, distributed
systems in the network. It also introduces a new mode of software reuse (18
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

@TechReport{casais92b,
 key         = "casais92b",
 author      = "Eduardo Casais and C. Lewerentz",
 title       = "{STONE}: A Short Overview",
 institution = "Forschungszentrum Informatik (FZI)",
 month       = may,
 year        = "1992",
 number      = "FZI.040.1",
 address     = "Karlsruhe, Germany",
 type        = "",
 note        = "",
}

------------------------------------------------------------------------------

170

@Article{karpovich94a,
 key     = "karpovich94a",
 author  = "John F. Karpovich and Andrew S. Grimshaw and James C. French",
 title   = "Extensible File Systems ({ELFS}): An Object-Oriented
          Approach to High Performance File {I/O}",
 journal = "SIGPLAN Notices",
 month   = oct,
 year    = "1994",
 volume  = "29",
 number  = "10",
 pages   = "191--204",
 abstract = "
Scientific applications often manipulate very large sets of persistent data.
Over the past decade, advances in disk storage device performance have
consistently been outpaced by advances in the performance of the rest of the
computer system. As a result, many scientific applications have become
I/O-bound, i.e, their run-times are dominated by the time spent performing I/O
operations.  Consequently, the performance of I/O operations has become
critical for high performance in these applications. The ELFS approach is
designed to address the issue of high performance I/O by treating files as
typed objects. Typed file objects can exploit knowledge about the file
structure and type of data.  Typed file objects can selectively apply
techniques such as prefetching, parallel asynchronous file access, and caching
to improve performance. Also, by typing objects, the interface to the user can
be improved in two ways, First, the interface can be made easier to use by
presenting file operations in a more natural manner to the user. Second, the
interface can allow the user to provide an `oracle' about access patterns, that
can allow the file object to improve performance. By combining these concepts
with the object-oriented paradigm, the goal of the ELFS methodology is to
create flexible, extensible file classes that are easy to use while achieving
high performance. We present the ELFS approach and our experiences with the
design and implementation of two file classes: a two dimensional dense matrix
file class and a multidimensional range searching file class (19 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

@Misc{coppit97a,
 key      = "coppit97a",
 author   = "David Coppit",
 title    = "The {WinBuf} Class",
 url      = "http://www.coppit.org/code/winbuf/index.html",
 abstract = "
The WinBuf class is designed to provide a Windows-based interface for cout and
cin, thereby easing the task of porting non-Windows code. Using the code with
the Microsoft Foundation Classes, you can redefine cout, cin, and cerr to
interact with a window instead of a console-based interface. It can be used
with a few lines at the beginning and end of your code, and a few more files.

This implementation supports buffering of input and output. Right now the input
and output are split into two fields, and maybe I'll fix this later. If you can
suggest any changes or improvements, email me at coppit`at'cs.virginia.edu. For
background, please see DejaNews' archive regarding my initial post to news and
the responses I got.
",
 note     = "URL: {\urlBiBTeX{http://www.coppit.org/code/winbuf/index.html}}",
}

------------------------------------------------------------------------------

@Misc{sullivan97c,
 key    = "sullivan97c",
 author = "Kevin J. Sullivan and Joanne Bechta Dugan and John Knight and
           others",
 year   = "1997",
 title  = "{Galileo}: An Advanced Fault Tree Analysis Tool",
 url    = "http://www.cs.virginia.edu/~ftree/index.html",
 note   = "URL: {\urlBiBTeX{http://www.cs.virginia.edu/~ftree/index.html}}",
}

------------------------------------------------------------------------------

171

@Article{gibbs97a,
 key     = "gibbs97a",
 author  = "W. Wayt Gibbs",
 title   = "Taking Computers to Task",
 journal = "Scientific American",
 month   = jul,
 year    = "1997",
 volume  = "227",
 number  = "1",
 pages   = "82--89",
 abstract = "
Computers exchanging video calls as commonly as e-mail. Three-dimensional
windows that open into virtual worlds instead of virtual scrolls. Machines that
speak and respond to human languages as well as their own. Personal `agent'
programs that haggle for concert tickets, arrange blind dates and winnow useful
information from the chaff of daily news. And everything, from our medical
records to our office files to the contents of our refrigerators,
hypertextually linked via the great global network.\par

These transformations in the way we interact with software--its so-called user
interface--have begun to graduate from idle speculation to working prototypes
and even a few shipped products. It is widely expected that before long they
will replace the flat windows, icons, menus and pointers that for 12 years have
dominated personal computer interfaces. In demos, the new technologies are
inarguably cool, and as Nathan Myhrvold, Microsoft's vice president of
applications and content, observed during his turn at the dais, `'Cool' is a
powerful reason to spend money.' \par

But in the computer industry and in the media that cover it, it has become
common to tout with almost millennial fervor that the changing face of
computers will make them not just more enjoyable but also dramatically more
useful. Historian (and Scientific American columnist) James Burke spoke for
many at the conference when he asserted that `we stand today on the threshold
of an explosion in information technology, the social and economic consequences
of which will make everything that came before look like slow motion.'\par
",
 note    = "",
}

------------------------------------------------------------------------------

172

@Article{arsac79a,
 key     = "arsac79a",
 author  = "Jacques J. Arsac",
 title   = "Syntactic Source to Source Transforms and Program Manipulation",
 journal = j-cacm,
 month   = jan,
 year    = "1979",
 volume  = "22",
 number  = "1",
 pages   = "43--54",
 abstract = "
Syntactic transforms are the source to source program transformations which
preserve the history of computation, and thus do not modify the execution time.
Combined with a small number of primitive semantic transforms, they provide a
powerful tool for program manipulation. A complete catalogue of syntactic
transforms, and its use for solution of a system of program equations, is
given. Examples of derivation of more complex source to source transformations
are also presented. Two case studies illustrate the way in which syntactic and
semantic source to source transformations may be used for development of clear,
simple, and reasonably efficient programs (24 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

173

@Article{joyce86a,
 key     = "joyce86a",
 author  = "Edward J. Joyce",
 title   = "Malfunction 54: Unraveling Deadly Medical Mystery of Computerized
            Accelerator Gone Awry",
 journal = "American Medical News",
 month   = oct,
 year    = "1986",
 volume  = "",
 number  = "",
 pages   = "13--17",
 abstract = "
",
 note    = "",
}

------------------------------------------------------------------------------

174

@InProceedings{parnas94a,
 key          = "parnas94a",
 author       = "David Lorge Parnas",
 title        = "Software Aging",
 booktitle    = "Proceedings of 16th International Conference on Software
                 Engineering",
 address      = "Sorrento, Italy",
 month        = "16--21 " # may,
 year         = "1994",
 pages        = "279--87",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
Programs, like people, get old. We can't prevent aging, but we can understand
its causes, take steps to limits its effects, temporarily reverse some of the
damage it has caused, and prepare for the day when the software is no longer
viable. A sign that the software engineering profession has matured will be
that we lose our preoccupation with the first release and focus on the
long-term health of our products. Researchers and practitioners must change
their perception of the problems of software development. Only then will
software engineering deserve to be called 'engineering' (9 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

175

@InBook{lehman85a,
 key          = "lehman85a",
 author       = "M. M. Lehman",
 title        = "Program Evolution: Processes of Software Change",
 chapter      = "12",
 pages        = "247--274",
 publisher    = "Academic Press",
 year         = "1985",
 volume       = "",
 series       = "",
 address      = "London, UK",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}

------------------------------------------------------------------------------

176

@Article{belady76a,
 key     = "belady76a",
 author  = "L. A. Belady and M. M. Lehman",
 title   = "A Model of Large Program Development",
 journal = "IBM Systems Journal",
 month   = mar,
 year    = "1976",
 volume  = "15",
 number  = "3",
 pages   = "225--252",
 abstract = "
The development of OS/360 system programming is analyzed in the context
of the general effort to further the realization that a program, much as a
mathematical theorem, should and can be provable.  Statistical models are
suggested based on the assumption of programming evolution dynamics.
",
 note    = "",
}

------------------------------------------------------------------------------

177

@Article{bruce84a,
 key     = "bruce84aschneier92a",
 author  = "Robert Bruce",
 title   = "Unwrapping Application Software Packages",
 journal = "Canadian Datasystems",
 month   = mar,
 year    = "1984",
 volume  = "16",
 number  = "3",
 pages   = "72--5",
 abstract = "
Disappointment with purchased business software frequently occurs not because
of the package itself, but as the result of not approaching the 'system' buying
decision logically. The author shows how to do it in a logical fashion (0
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

178

@InProceedings{beck93a,
 key          = "beck93a",
 author       = "Jon Beck and David Eichmann",
 title        = "Program and Interface Slicing for Reverse Engineering",
 booktitle    = "Proceedings of 1993 15th International Conference on Software
                 Engineering",
 address      = "Baltimore, MD",
 month        = "17--21 " # may,
 year         = "1993",
 pages        = "509--18",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
A case is presented for the use of conventional and interface slicing as
enabling mechanisms for numerous reverse engineering and reengineering tasks.
The authors first discuss the applicability of conventional slicing to
algorithm extraction and design recovery at statement-level granularity. They
then present interface slicing and show how it provides similar capabilities at
module-level granularity. Module is a general term for a collection of
subprograms, possibly with information hiding mechanisms: It includes but is
not limited to Ada packages. Component refers to a module in a reuse
repository. A component is thus a code asset of a repository, possibly also
incorporated into a program. Ada is used for the example, as Ada's features
facilitate the types of transformations which are invoked (27 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

179

@Article{walcher94a,
 key     = "walcher94a",
 author  = "Owen Walcher",
 title   = "Reengineering Legacy Systems Using {GUI} and Client/Server
            Technology",
 journal = "OOPS Messenger",
 month   = apr,
 year    = "1994",
 volume  = "5",
 number  = "2",
 pages   = "37--8",
 abstract = "
American Management Systems, Inc. (AMS) is a leader in helping clients
strengthen performance through the creative application of information
technology. AMS has been developing and implementing distributed and
client/server applications for nearly a decade. Recently, AMS established the
AMS Center for Advanced Technologies as an applied research laboratory to
review and make use of the rapidly changing technology for developing
large-scale business systems. AMS began a project to re-engineer their
mainframe-based legacy system, Mobile2000, using a graphical user interface
(GUI) and client/server technology. Mobile2000 is a customer account management
and billing system which supports the cellular telephony industry.  Mobile2000
consists of 200 online and 600 batch/report programs, constituting
approximately 2.2 million lines of COBOL code. The goal is to develop a
production-ready version of Mobile2000 online processes using GUI and object
oriented technologies (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

180

@Article{sullivan92a,
 key     = "sullivan92a",
 author  = "Kevin J. Sullivan and David Notkin",
 title   = "Reconciling Environment Integration and Software Evolution",
 journal = j-tsem,
 month   = jul,
 year    = "1992",
 volume  = "1",
 number  = "3",
 pages   = "229--68",
 abstract = "
Common software design approaches complicate both tool integration and software
evolution when applied in the development of integrated environments. The
authors illustrate this by tracing the evolution of three different designs for
a simple integrated environment as representative changes are made to the
requirements. They present an approach that eases integration and evolution by
preserving tool independence in the face of integration. They design tool
integration relationships as separate components called mediators, and design
tools to implicitly invoke mediators that integrate them. Mediators separate
tools from each other, while implicit invocation allows tools to remain
independent of mediators. To enable the use of the approach on a range of
platforms, they provide a formalized model and requirements for implicit
invocation mechanisms. They apply this model both to analyze existing
mechanisms and in the design of a mechanism for C++ (34 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

181

@Article{trlica97a,
 key     = "trlica97a",
 author  = "Cary Trlica",
 title   = "Software Tools",
 journal = "IEEE Spectrum",
 month   = jan,
 year    = "1997",
 volume  = "34",
 number  = "1",
 pages   = "60--4",
 abstract = "
Many engineering software applications migrated last year from Unix to the
Windows NT operating system. An unaccustomed
camaraderie-interoperability-spread among applications from different vendors
and between different stages of the design process. Meanwhile, the mainstream
of electronic design got a new supreme court: formal verification. In the
course of 1997, the migration from Unix to Windows NT, the growing need for
interoperability and powerful verification tools, and the great HDL debate will
continue. Added to these issues will be the evolution of the desktop computer
into the platform for both engineering and administrative applications, new
approaches to embedded and digital signal processing (DSP) design, and the
impact of the World Wide Web (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

182

@Article{deremer76a,
 key     = "deremer76a",
 author  = "Frank DeRemer and Hans H. Kron",
 title   = "Programming-in-the-large versus programming-in-the-small",
 journal = j-tse,
 month   = jun,
 year    = "1976",
 volume  = "SE-2",
 number  = "2",
 pages   = "80--6",
 abstract = "
Distinguishes the activity of writing large programs from that of writing small
ones. Large programs are systems consisting of many small programs (modules),
usually written by different people (20 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

183

@InProceedings{baker94a,
 key          = "baker94a",
 author       = "Marla J. Baker and Stephen G. Eick",
 title        = "Visualizing Software Systems",
 booktitle    = "Proceedings of 16th International Conference on Software
                 Engineering",
 address      = "Sorrento, Italy",
 month        = "16--21 " # may,
 year         = "1994",
 pages        = "59--67",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
There are many graphical techniques for visualizing software. Unfortunately,
the current techniques do not scale to display large software systems and are
largely unused. We present a method for visualizing statistics associated with
code that is divided hierarchically into subsystems, directories, and files.
Using this technique, we can display the relative sizes of the components in
the system, which components are stable and which are changing, where the new
functionality is being added, and identify error-prone code with many bug
fixes. Using animation, we can display the historical evolution of the code (10
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

184

@Article{paulk95a,
 key     = "paulk95a",
 author  = "Mark C. Paulk",
 title   = "How {ISO 9001} Compares with the {CMM}",
 journal = j-soft,
 month   = jan,
 year    = "1995",
 volume  = "12",
 number  = "1",
 pages   = "74--83",
 abstract = "
Organizations concerned with ISO 9001 certification often question its overlap
with the Software Engineering Institute's Capability Maturity Model (CMM). The
author looks at 20 clauses in ISO 9001 and maps them to practices in the CMM.
The analysis provides answers to some common questions about the two documents
(5 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

185

@Article{ivie77a,
 key     = "ivie77a",
 author  = "Evan L. Ivie",
 title   = "The {P}rogrammer's {W}orkbench --- A Machine for Software
            Development",
 journal = j-cacm,
 month   = oct,
 year    = "1977",
 volume  = "20",
 number  = "10",
 pages   = "746--53",
 abstract = "
It is suggested that there are many situations where it would be advantageous
to separate the program development and maintenance function onto a specialized
computer which is dedicated to that purpose. Such a computer is called a
Programmer's Workbench. The four basic sections of the paper introduce the
subject, outline the general concept, discuss areas where such an approach may
prove beneficial, and describe an operational system utilizing this concept (7
Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

186

@Article{perry92a,
 key     = "perry92a",
 author  = "Dewayne E. Perry and Alexander L. Wolf",
 title   = "Foundations for the Study of Software Architecture",
 journal = j-sig,
 month   = oct,
 year    = "1992",
 volume  = "17",
 number  = "4",
 pages   = "40--52",
 abstract = "
A foundation is built for software architecture. The authors develop an
intuition for software architecture by appealing to several well-established
architectural disciplines. On the basis of this intuition, they present a model
of software architecture that consists of three components: elements, form and
rationale. Elements are either processing, data, or connecting elements. Form
is defined in terms of the properties of, and the relationships among, the
elements-that is, the constraints on the elements. The rationale provides the
underlying basis for the architecture in terms of the system constraints, which
most often derive from the system requirements. The authors discuss the
components of the model in the context of both architectures and architectural
styles and present an extended example to illustrate some important
architecture and style considerations. They conclude by presenting some of the
benefits of the approach to software architecture, summarizing their
contributions, and relating the approach to other current work (23 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

187

@InCollection{garlan92a,
 key       = "garlan92a",
 author    = "David Garlan and Mary Shaw",
 title     = "An Introduction to Software Architecture",
 booktitle = "Advances in Software Engineering and Knowledge Engineering",
 pages     = "1--40",
 publisher = "World Scientific Publishing Co.",
 editor    = "V. Ambriola and G. Tortora",
 month     = "",
 year      = "1992",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
As the size of software systems increases, the algorithms and data structures
of the computation no longer constitute the major design problems. When systems
are constructed from many components, the organization of the overall
system-the software architecture-presents a new set of design problems. This
level of design has been addressed in a number of ways including informal
diagrams and descriptive terms, module interconnection languages, templates and
frameworks for systems that serve the needs of specific domains, and formal
models of component integration mechanisms. We provide an introduction to the
emerging field of software architecture. We begin by considering a number of
common architectural styles upon which many systems are currently based and
show how different styles can be combined in a single design.  Then we present
six case studies to illustrate how architectural representations can improve
our understanding of complex software systems. Finally, we survey some of the
outstanding problems in the field, and consider a few of the promising research
directions (44 Refs.)
",
 note      = "",
}

------------------------------------------------------------------------------

188

@Article{booch86a,
 key     = "booch86a",
 author  = "Grady Booch",
 title   = "Object-Oriented Development",
 journal = j-tse,
 month   = feb,
 year    = "1986",
 volume  = "SE-12",
 number  = "2",
 pages   = "211--21",
 abstract = "
Object-oriented development is a partial-lifecycle software development method
in which the decomposition of a system is based upon the concept of an object.
This method is fundamentally different from traditional functional approaches
to design and serves to help manage the complexity of massive
software-intensive systems. The author examines the process of object-oriented
development as well as the influences upon this approach from advances in
abstraction mechanisms, programming languages, and hardware. The concept of an
object is central to object-oriented development and so the properties of an
object are discussed. The mapping of object-oriented techniques to Ada using a
design case study is considered (33 Refs)
",
 note    = "",
}

------------------------------------------------------------------------------

189

@Article{leveson93a,
 key     = "leveson93a",
 author  = "Nancy G. Leveson and Clark S. Turner",
 title   = "An Investigation of the {Therac-25} Accidents",
 journal = j-comp,
 month   = jul,
 year    = "1993",
 volume  = "26",
 number  = "7",
 pages   = "18--41",
 abstract = "
Between June 1985 and January 1987, the Therac-25 medical electron accelerator
was involved in six massive radiation overdoses. As a result, several people
died and others were seriously injured. A detailed investigation of the factors
involved in the software-related overdoses and attempts by users,
manufacturers, and government agencies to deal with the accidents is presented.
The authors demonstrate the complex nature of accidents and the need to
investigate all aspects of system development and operation in order to prevent
future accidents. The authors also present some lessons learned in terms of
system engineering, software engineering, and government regulation of
safety-critical systems containing software components (10 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

190

@InProceedings{brodman94a,
 key          = "brodman94a",
 author       = "Judith G. Brodman and Donna L. Johnson",
 title        = "What Small Businesses and Small Organizations Say About the
                 {CMM}",
 booktitle    = "Proceedings of 16th International Conference on Software
                 Engineering",
 address      = "Sorrento, Italy",
 month        = "16--21 " # may,
 year         = "1994",
 pages        = "331--40",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The US Air Force sponsored research within the Department of Defense software
development community to determine the applicability of the Software
Engineering Institute's capability maturity model (CMM) for software to small
businesses and small software organizations. The research found that small
businesses are faced not only with a lack of resources and funds required to
implement many of the practices stated in the CMM, but also with the task of
basing their process improvement initiatives on practices that do not apply to
a small business and small software organization. This paper discusses, from
industry's perspective, why small businesses and organizations are experiencing
difficulties implementing CMM-based process improvement programs and how they
are tailoring their approach to the CMM to meet their quality goals (3 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

191

@InProceedings{redwine85a,
 key          = "redwine85a",
 author       = "Samuel T. {Redwine, Jr.} and William E. Riddle",
 title        = "Software Technology Maturation",
 booktitle    = "Proceedings of 8th International Conference on Software
                 Engineering",
 address      = "London, UK",
 month        = "28--30 " # aug,
 year         = "1985",
 pages        = "189--200",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The authors have reviewed the growth and propagation of a variety of software
technologies in an attempt to discover natural characteristics of the process
as well as principles and techniques useful in bringing modern software
technology into widespread use. They have looked at the technology maturation
process, the process by which a piece of technology is first conceived, then
shaped into something usable, and finally 'marketed' to the point that it is
found in the repertoire of a majority of professionals. A major interest is the
time required for technology maturation; the authors' conclusion is that
technology maturation generally takes much longer than popularly thought,
especially for major technology areas. An attempt is made to determine what
actions, if any, can accelerate the maturation of technology, in particular
that part of maturation that has to do with bringing the technology into
widespread use. Observations concerning maturation facilitators and inhibitors
provide the basic thrust of the paper (7 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

192

@Article{schneier92a,
 key     = "schneier92a",
 author  = "Bruce Schneier",
 title   = "Doing it Randomly: Probabilistic Algorithms in Programming",
 journal = "Computer Language Magazine",
 month   = aug,
 year    = "1992",
 volume  = "9",
 number  = "8",
 pages   = "69--71",
 abstract = "
It may seem strange that programming, which has long been a bastion of exact
algorithms behaving in precisely the same manner every time, occasionally turns
to probability to solve some of its more difficult problems. This development
is relatively new. In some people's minds, algorithms should be proveably
correct at all times and for all inputs (as with defect-free programming and
formal methods). Probabilistic algorithms give up this property. There is
always a chance that the algorithm will produce a false result. But this chance
can be made as small as desired. If the chance of the softawre failing is made
smaller than the chance of the hardware failing (or of the user spontaneously
combusting, or whatever), there's little to worry about.
",
 note    = "",
}

------------------------------------------------------------------------------

193

@InProceedings{csaba97a,
 key          = "csaba97a",
 author       = "{L\'aszl\'o} Csaba",
 title        = "Experience with User Interface Reengineering: Transferring
                 {DOS} Panels to {W}indows",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "150--5",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The article describes a method of user interface reengineering used in a
project of turning a large character based archaic DOS application into a
modern Windows application. The program was originally written for mainframe
environments in COBOL. During the conversion the working core had to be left
unchanged. In the applied method the original program-recompiled for Windows
with the I/O calls replaced-is run and remote controlled through a
communications module by the new wrapping application. The new program, the
Artificial User Program, was developed with S-Prog, a highly effective visual
tool. Some parts of the source were generated by Word Basic macros (4 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

194

@InProceedings{pidaparthi97a,
 key          = "pidaparthi97a",
 author       = "Sagar Pidaparthi and Grzegorz Cysewski",
 title        = "Case Study in Migration to Object-Oriented System Structure
                 Using Design Transformation Methods",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "965--79",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
A formal, general model of program dependences is presented and used to
evaluate several dependence-based software testing, debugging, and maintenance
techniques. Two generalizations of control and data flow dependence, called
weak and strong syntactic dependence, are introduced and related to a concept
called semantic dependence. Semantic dependence models the ability of a program
statement to affect the execution behavior of other statements. It is shown
that weak syntactic dependence is a necessary but not sufficient condition for
semantic dependence and that strong syntactic dependence is necessary but not
sufficient condition for a restricted form of semantic dependence that is
finitely demonstrated. These results are used to support some proposed uses of
program dependences, to controvert others, and to suggest new uses (28 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

195

@InProceedings{classen97a,
 key          = "classen97a",
 author       = "Ingo Cla{\ss}en and Klaus Hennig and Ingo Mohr and Michael
                 Schulz",
 title        = "{CUI} to {GUI} Migration: Static Analysis of Character-Based
                 Panels",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "144--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The paper reports on progress in the construction of an integrated tool
environment for the migration of character-based panels of mainframe
applications into graphical user interfaces. It presents an approach for static
analysis of character-based panels that is based on the identification of
dialog entities in panels using generalized descriptions of panel areas and
describes a tool supporting the identification process. The goal of the
approach is to increase the level of automation in the migration process
compared to existing migration tools (13 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

196

@Article{cummins94a,
 key     = "cummins94a",
 author  = "Fred A. Cummins and Mamdouh H. Ibrahim",
 title   = "Wrapping Legacy Applications",
 journal = "First Class --- The Object Management Group Newsletter",
 month   = apr,
 year    = "1994",
 volume  = "4",
 number  = "2",
 pages   = "10, 20",
 abstract = "
EDS has been applying object-oriented programming techniques to practical
business problems since 1985. Early project focused on the use of
object-oriented techniques in conjunction iwth artificial intelligence. More
recently, we have implemented applications using Smalltalk and C++.\par

These applications seldom exist in a vacuum. Typically, they must be interfaced
to legacy systems. These interfaces take the form of an object-oriented
representation of the relevant data and/or functionality of the legacy system.
From the perspective of the object-oriented application developer, the legacy
application is ``wrapped'' in an object-oriented interface.
",
 note    = "",
}

------------------------------------------------------------------------------

197

@InProceedings{jarzabek97a,
 key          = "jarzabek97a",
 author       = "Stan Jarzabek and Irene Woon",
 title        = "Towards a Precise Description of Reverse Engineering Methods
                 and Tools",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "3--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The potential and limitations of reverse engineering techniques is still a
matter of debate and investigation. Both experimental studies and commonsense
tell us that design abstractions are useful in program understanding and
maintenance. In the case of incomplete program documentation, reverse
engineering tools can recover some of the design abstractions from code.
However, it is not clear which design abstractions can and which cannot be
automatically recovered. This can be attributed to the understandable
reluctance of industry to publicize explicit knowledge of this process due to
its enormous commercial value and the fact that reverse engineering is a fairly
new research discipline. As a start to formalizing what we already know about
reverse engineering, we propose a framework for describing and evaluating
reverse engineering methods and tools. First, we build design models for a
source language and for the recovered design. Then, we describe what a given
reverse engineering method or tool achieves as a formal mapping from the source
language design model into the recovered design model. We show use object
recovery scenarios to illustrate the presented concepts (27 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

198

@InProceedings{mendonca97a,
 key          = "mendonca97a",
 author       = "Nabor C. {Mendon\c{c}a} and Jeff Kramer",
 title        = "A Quality-Based Analysis of Architecture Recovery
                 Environments",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "54--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
Architecture recovery is a recent research area which aims at providing reverse
engineering technologies to extract high-level architectural information from
the source code of legacy systems. The authors review the main (only?)
architecture recovery environments proposed thus far.  The environments are
analysed with respect to different quality attributes, and their features and
limitations are discussed. This allows one to highlight problems yet to be
addressed in the area and, for some of them, suggest possible alternatives.
They believe that this analysis is useful for the design of more effective
architecture recovery tools (30 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

199

@InProceedings{tonella97a,
 key          = "tonella97a",
 author       = "P. Tonella and G. Antoniol and R. Fiutem and E. Merlo",
 title        = "Variable Precision Reaching Definitions Analysis for Software
                 Maintenance",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "60--7",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
A flow analyzer can be very helpful in the process of program understanding, by
providing the programmer with different views of the code. As the documentation
is often incomplete or inconsistent, it is extremely useful to extract the
information a programmer may need directly from the code. Program understanding
activities are interactive, thus program analysis tools may be asked for quick
answers by the maintainer. Therefore the control on the trade-off between
accuracy and efficiency should be given to the user.\par

The paper presents an approach to interprocedural reaching definitions flow
analysis based on three levels of precision depending on the sensitivity to the
calling context and the control flow. A lower precision degree produces an
overestimate of the data dependencies in a program. The result is anyhow
conservative (all dependencies which hold are surely reported), and definitely
faster than the more accurate counterparts. A tool supporting reaching
definition analysis in the three variants has been developed. The results on a
test suite show that three orders of magnitude can be gained in execution times
by the less accurate analysis, but 57.4\% extra dependencies are on average
added. The intermediate variant is much more precise (1.6\% extra
dependencies), but gains less in times (one order of magnitude) (22 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

200

@InProceedings{muller97a,
 key          = "muller97a",
 author       = "Bernd {M\"uller} and Rainer Gimnich",
 title        = "Planning {Y}ear 2000 Transformations Using Standard Tools: An
                 Experience Report",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "94--100",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
The Year 2000 transformation is by no means merely a problem of computer
science, involving coding practice or programming-in-the-small.  It turns out
that the Year 2000 problem is mainly a project management problem. To plan and
control a Year 2000 transformation one has to discover the facts on which all
planning is based on. In this paper, we describe a straightforward approach to
gather the facts relevant for a Year 2000 coarse-grained planning. It was done
in a customer engagement at a large German insurance company. The engagement
was done completely at the customer's site using only standard tools, which are
most likely installed in each larger organization (3 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

201

@InProceedings{sneed97a,
 key          = "sneed97a",
 author       = "Harry M. Sneed",
 title        = "Measuring the Performance of a Software Maintenance
                 Department",
 booktitle    = "Proceedings. First Euromicro Conference on Software
                 Maintenance and Reengineering",
 address      = "Berlin, Germany",
 month        = "17--19 " # mar,
 year         = "1997",
 pages        = "119--27",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
All studies indicate that over half of an average data processing user staff is
committed to maintaining existing applications.  However, as opposed to
software development where productivity is measured in terms of lines of code,
function-points, data-points or object-points per person month and quality is
measured in terms of deficiencies and defect rates per test period, there are
no established metrics for measuring the productivity and quality of software
maintenance. This means that over half of an organization's software budget
cannot be accounted for. The costs occur without being able to measure the
benefits obtained. A set of metrics is proposed for helping to remedy this
situation by measuring the productivity and quality of the maintenance service
(20 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{graham95a,
 key       = "graham95a",
 author    = "Ian Graham",
 title     = "Migrating to Object Technology",
 publisher = "Addison Wesley",
 edition   = "",
 month     = "",
 year      = "1995",
 volume    = "",
 series    = "",
 address   = "Reading, Mass.",
 note      = "",
}

------------------------------------------------------------------------------

@Book{neumann95a,
 key       = "neumann95a",
 author    = "Peter G. Neumann",
 title     = "Computer Related Risks",
 publisher = "Addison Wesley",
 edition   = "",
 month     = "",
 year      = "1995",
 volume    = "",
 series    = "",
 address   = "Reading, Mass.",
 note      = "",
}

------------------------------------------------------------------------------

202

@Article{vancamp93a,
 key     = "vancamp93a",
 author  = "Kenneth E. Van Camp",
 title   = "Using Wrappers to Improve Portability of Commercial
            Libraries",
 journal = "C Users Journal",
 month   = jan,
 year    = "1993",
 volume  = "11",
 number  = "1",
 pages   = "35-37, 40",
 abstract = "
You've seen the ads for them: the Ultimate Portability Libraries. One supports
Windows, PenPoint, the Presentation Manager. Another supports, UNIX, MS-DOS,
and VMS. Each on is the ``only'' tool you'll ever need. Promise.\par

Well, maybe not. But you can improve the portability of software that uses
third-party libraries by isolating all library-specific code and writing
wrappers. Most programmers are familiar with wrappers as applied to functions,
but wrappers can also be written for all data structures, constants, and global
variables. In C++, this concept is known as encapsulation, but it works equally
well in C.
",
 note    = "",
}

------------------------------------------------------------------------------

203

@Article{flint97a,
 key     = "flint97a",
 author  = "E. S. Flint",
 title   = "The {COBOL} Jigsaw Puzzle: Fitting Object--Oriented and Legacy
            Applications Together",
 journal = "IBM Systems Journal",
 month   = jan,
 year    = "1997",
 volume  = "36",
 number  = "1",
 pages   = "49-65",
 abstract = "
Object wrappers have been presented as a way to allow legacy applications and
object oriented applications to work together. However, object wrappers do not
always solve the interoperability problem for COBOL legacy applications.  The
paper examines the use of object wrappers and introduces two other types of
wrappers, the procedural wrapper and the combination wrapper, for practical use
with COBOL legacy applications. The main concerns of a developer of an object
oriented application that uses the services of or provides services to a legacy
application are addressed. Examples of ``real world'' COBOL legacy applications
are cited and samples of all three types of wrapper code are provided (5 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

204

@Article{baker97a,
 key     = "baker97a",
 author  = "Larry E. {Baker Jr.}",
 title   = "{C}++ Interfaces for {C}--language Libraries",
 journal = "Dr. Dobb's Journal",
 month   = aug,
 year    = "1997",
 volume  = "22",
 number  = "8",
 pages   = "34, 36-7, 90-1",
 abstract = "
C++ interfaces for C-language libraries Abstract: C++ developers often have to
use legacy C language support libraries even though the styles do not mix, the
paradigms conflict, and the resulting program is as unpleasant to look at as it
is to code. There are usually two alternatives: use the old library, or rewrite
it in C++. Another approach is to develop a C++ wrapper for the underlying C
language API. The author presents a simple C language hash-table library and
the C++ template wrapper that adapted it to a C++ world (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

205

@Article{scherr93a,
 key     = "scherr93a",
 author  = "A. L. Scherr",
 title   = "A New Approach to Business Processes",
 journal = "IBM Systems Journal",
 month   = jan,
 year    = 1993,
 volume  = "32",
 number  = "1",
 pages   = "80-98",
 abstract = "
This paper presents a methodology for analyzing and designing the processes
that an enterprise uses to conduct its business. The methodology builds upon
traditional approaches to business process definition by adding the dimension
of people's accountabilities: their roles, relationships, and agreements. The
approach presented allows for unique insights into customer satisfaciton,
employee empowerment, and quality. It also provides a basis for spanning the
concerns of both business people and information technologists responsible for
providing business process automation.
",
 note    = "",
}

------------------------------------------------------------------------------

206

@Article{lee96a,
 key     = "lee96a",
 author  = "C. H. Lee and R. N. Zobel",
 title   = "Representation of Simulation Model Components for Model Generation
            and a Model Library",
 journal = "Canadian Datasystems",
 month   = mar,
 year    = "1984",
 volume  = "16",
 number  = "3",
 pages   = "72-5",
 abstract = "
Model reuse is one of the main reasons for developing a model library. In
particular, during the process of establishing a model library, two aspects
have to be considered: model generation and database operations; therefore, the
stored model components should have an effective representation method to cope
with the above requirements. In this paper, we propose a representation
methodology including feature design and required query approaches for model
components for the function of both model generation and model storage. In our
proposed model library system, an object-oriented database is working as the
main model repository, and a wrapper front-end is designed to wrap and unwrap
the model components and for transforming messages to the stored objects. We
also explore the developed model representation heuristic potential in model
generation ghrough the functions provided by the object-oriented database and
qualitative query approaches.
",
 note    = "",
}

------------------------------------------------------------------------------

207

@InProceedings{knight97a,
 key          = "knight97a",
 author       = "John C. Knight",
 title        = "Is Information Security an Oxymoron?",
 booktitle    = "COMPASS '97. Are we Making Progress Towards Computer
                 Assurance?",
 address      = "",
 month        = "16--19 " # jun,
 year         = "1997",
 pages        = "158, 120--1",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
Although weaknesses have been demonstrated in some security techniques
(encryption, protocols, mobile code such as Java, etc.), current security
technology is quite strong in many areas. Despite this, information security
has proved difficult to achieve in large modern software systems. Many
problems have been reported in which supposedly secure systems have been
penetrated and in some cases significant damage done. In practice, it appears
that many (perhaps even the majority) of serious security failures are
attributable to software engineering defects in the systems experiencing the
failure. The author discusses the use of wrappers which can deal with
deficiencies in security and considers the software architectural approach (0
Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

208

@Article{reznick95a,
 key     = "reznick95a",
 author  = "Larry Reznick",
 title   = "Hiding {UNIX} Applications in Utility Wrappers",
 journal = "Sys Admin: The Journal for UNIX Systems Administrators",
 month   = sep # "\slash " # oct,
 year    = "1995",
 volume  = "4",
 number  = "5",
 pages   = "68--82",
 abstract = "
On some HP 9000 systems I administered not long ago, a quality assurance group
ran software that tested chip production quality. Each installation of this
software was configured to run a special battery of tests, but configuration
was built in to the software itself, not into a set of data files. Thus, each
software installation on a system testing a different chip was entirely
different from the installation on another system. Data files produced by this
software uniquely identified characteristics of the chip and testing and had
to remain with the software to make sense. This software was never designed to
run in a multiple machine, networked, mass testing environment, as this
company used it.
",
 note    = "",
}

------------------------------------------------------------------------------

209

@Misc{parodi97a,
 key    = "parodi97a",
 author = "John Parodi",
 year   = "1997",
 title  = "Building ``Wrappers'' for Legacy Software Application",
 abstract = "
This paper describes several ``wrapping'' techniques used to provide distributed
access to and control over legacy software. It presents a case study in which
advanced flow control software and DIGITAL ObjectBroker product were used to
integrate existing software applications at the Swiss Telecom PTT. 
",
 url    = "",
 note   = "",
}

------------------------------------------------------------------------------

210

Starts at page 36, but I don't know the end

@Article{winsberg95a,
 key     = "winsberg95a",
 author  = "Paul Winsberg",
 title   = "Legacy Code: Don't Bag It, Wrap It",
 journal = "Datamation",
 month   = may,
 year    = "1995",
 volume  = "41",
 number  = "9",
 pages   = "",
 abstract= "
Moving to client/server computing doesn't mean you have to scrap your
mainframe applications or just settle for screen-scrapers. Application
wrappers give you programming interfaces that let your desktop clients talks
to your existing mainframe apps in a more intelligent way. But if object
programming is your goal, beware: You really can't just wrap a mainframe app
to make it an object.
",
 note    = "",
}

------------------------------------------------------------------------------

211

@Article{aronica96a,
 key     = "aronica96a",
 author  = "Ronald C. Aronica and Donald E. {Rimel Jr.}",
 title   = "Wrapper Your Legacy Systems",
 journal = "Datamation",
 month   = jun,
 year    = "1996",
 volume  = "42",
 number  = "12",
 pages   = "83--88",
 abstract= "
You may be tempted to just bulldoze your legacy apps. Don't. You can integrate
them into your next-generation systems architecture using object-wrapping
techniques.
",
 note    = "",
}

------------------------------------------------------------------------------

212

@Article{kiczales96a,
 key     = "kiczales96a",
 author  = "Gregor Kiczales",
 title   = "Beyond the Black Box: Open Implementations",
 journal = j-soft,
 month   = jan,
 year    = "1996",
 volume  = "13",
 number  = "1",
 pages   = "8--11",
 abstract= "
Encapsulation, informally known as black-box abstraction, is a widely known
and accepted principle. It is a basic tenet of software design, underlying
approaches to portability and reuse. However, many practitioners find
themselves violating it in order to achieve performance requirements in a
practical manner. The gap between theory and practice must be filled. Open
implementation is a controversial new approach that claims to do just
that. The paper provides some ideas to spark further debate on black-box
abstraction (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

213

@Article{wiederhold92a,
 key     = "wiederhold92a",
 author  = "Gio Wiederhold and Peter Wegner and Stefano Ceri",
 title   = "Toward Mega Programming",
 journal = j-cacm,
 month   = nov,
 year    = "1992",
 volume  = "35",
 number  = "11",
 pages   = "89--99",
 abstract= "
Megaprogramming is a technology, for programming with large modules
called megamodules that capture the functionality of services provided
by large organizations like banks, airline reservation systems, and city
transportation systems. Megamodules are internally homogeneous, independently
maintained software systems managed by a community with its own terminology,
goals, knowledge, and programming traditions. Each megamodule describes its
externally accessible data structures and operations and has an internally
consistent behavior. The concepts, terminology, and interpretation paradigm
of a mega-module is called its ontology (30 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

214
This is Dijkstra's Turing Award lecture.

@Article{dijkstra72a,
 key     = "dijkstra72a",
 author  = "Edsger W. Dijkstra",
 title   = "The Humble Programmer",
 journal = j-cacm,
 month   = oct,
 year    = "1972",
 volume  = "15",
 number  = "10",
 pages   = "859--866",
 abstract= "
This article discusses what a programmer is. The authors experiences are
used as a basis for discussion (0 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

215

@Misc{systems95a,
 key    = "systems95a",
 author = "{Systems Techniques, Inc.}",
 title  = "Wrapping Legacy Systems for Reuse: Repackaging vs. Rebuilding",
 url    = "http://www.systecinc.com/white/whitewrp.htm",
 abstract = "
``New and improved!'' always makes you curious. You can't resist the urge
to pick up the product and read the label to find out what's new, what's
improved, and how that improvement might taste, look, feel, or smell. The
proverbial dieter, I am always seeking the easy solution: a fat-free product
that tastes like it was made with real butter. I know I can't have it,
but I keep looking anyway.\par

Information technology (IT) executives aren't any different. They are
looking for ``New and Improved'' software techniques, productivity tools,
development methodologies and hardware platforms that can help them achieve
quick and easy success. Do these products or methods exist? Or, are we
simply putting a new wrapper on an old product, adding some new features,
and calling it ``New and Improved''?
",
 note   = "URL: {\urlBiBTeX{http://www.systecinc.com/white/whitewrp.htm}}",
}

------------------------------------------------------------------------------

216

@InProceedings{perrochon97a,
 key          = "perrochon97a",
 author       = "Louis Perrochon and Gio Wiederhold and Ron Burback",
 title        = "A Compiler for Composition: {CHAIMS}",
 booktitle    = "Proceedings Fifth International Symposium on Assessment of
                 Software Tools and Technologies",
 address      = "Pittsburgh, PA",
 month        = "2--5 " # jun,
 year         = "1997",
 pages        = "44--51",
 editor       = "E. Nahouraii",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "IEEE",
 abstract     = "
CHAIMS supports an innovative paradigm in software engineering: composition.
The CHAIMS programming language focuses solely on integrating so-called
megamodules into new applications. In doing this, CHAIMS exploits existing or
emerging standards for interoperation like CORBA, ActiveX, JavaBeans or DCE.
The approach reduces software development and maintenance costs by actively
supporting autonomy and reuse of megamodules (58 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

217

@InProceedings{papakonstantinou95a,
 key          = "papakonstantinou95a",
 author       = "Yannis Papakonstantinou and Ashish Gupta and Hector
                 Garcia-Molina and Jeffrey Ullman",
 title        = "A Query Translation Scheme for Rapid Implementation of
                 Wrappers",
 booktitle    = "Proceedings of 4th International Conference on Deductive and 
                 Object-Oriented Databases",
 address      = "Singapore",
 month        = "4--7 " # dec,
 year         = "1995",
 pages        = "161--86",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 number       = "",
 publisher    = "Springer-Verlag Berlin, Germany",
 abstract     = "
CHAIMS supports an innovative paradigm in software engineering: composition.
The CHAIMS programming language focuses solely on integrating so-called
megamodules into new applications. In doing this, CHAIMS exploits existing or
emerging standards for interoperation like CORBA, ActiveX, JavaBeans or DCE.
The approach reduces software development and maintenance costs by actively
supporting autonomy and reuse of megamodules (58 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

218

@Article{fromme90a,
 key     = "fromme90a",
 author  = "Brian D. Fromme",
 title   = "{HP Encapsulator}: Bridging the Generation Gap",
 journal = "Hewlett-Packard Journal: Technical Information from the Labratories
            of Hewlett-Packard Company",
 month   = jun,
 year    = "1990",
 volume  = "41",
 number  = "3",
 pages   = "59--68",
 abstract= "
The HP Encapsulator is the tool integration and process specification
facility of the HP SoftBench environment. It allows an HP SoftBench user
to promote existing tools to be fully consistent, integrated HP SoftBench
tools and to tailor the HP SoftBench environment to support a specific
software development process. The HP Encapsulator provides customization
and extension capabilities for automating organization, team, and personal
software development processes using event triggers.
",
 note    = "",
}

------------------------------------------------------------------------------

219

@Article{standish84a,
 key     = "standish84a",
 author  = "Thomas A. Standish",
 title   = "An Essay on Software Reuse",
 journal = j-tse,
 month   = sep,
 year    = "1984",
 volume  = "SE-10",
 number  = "5",
 pages   = "494--7",
 abstract= "
The author discusses briefly some economic incentives for developing effective
software reuse technology and notes that different kinds of software reuse,
such as direct use without modification and reuse of abstract software
modules after refinement, have different technological implications. He
then sketches some problem areas to be addressed if the goal of devising
practical software reuse systems is to be achieved. These include information
retrieval problems and finding effective methods to aid in understanding
how programs work (13 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

220

@InProceedings{mendelsohn97a,
 key          = "mendelsohn97a",
 author       = "Noah Mendelsohn",
 title        = "Operating Systems for Component Software Environments",
 booktitle    = "Proceedings. The Sixth Workshop on Hot Topics in Operating
                 Systems",
 address      = "Cape Cod, MA",
 month        = "5--6 " # may,
 year         = "1997",
 pages        = "49--54",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Although component software has emerged as one of the most significant and
commercially successful technologies of the past few years, few operating
systems (OSs) are designed to host and manage component software effectively.
Components impact OS architectures in the areas of security, process
isolation, code sharing, installation management and user interface design.  A
more radical question is: can effective OSs be built of modular,
interchangeable component parts?  The thesis of this paper is that effective
support of components is a key requirement for OSs of the future (15 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

221

@Article{udell94a,
 key     = "udell94a",
 author  = "Jon Udell",
 title   = "Componentware",
 journal = "Byte",
 month   = may,
 year    = "1994",
 volume  = "19",
 number  = "5",
 pages   = "46--56",
 abstract = "
Object technology failed to deliver on the promise of reuse. Visual Basic's
custom controls succeeded. What role will object-oriented programming play
in the component-software revolution that's now finally under way?
",
 note    = "",
}

------------------------------------------------------------------------------

222

@Article{clements95a,
 key     = "clements95a",
 author  = "Paul C. Clements",
 title   = "From Subroutines to Subsystems: Component-Based Software
           Development",
 journal = "American Programmer",
 month   = "",
 year    = "1995",
 volume  = "8",
 number  = "11",
 pages   = "",
 abstract = "
",
 note    = "",
}

------------------------------------------------------------------------------

223

@InBook{brown96a,
 key          = "brown96a",
 author       = "Alan W. Brown and Kurt C. Wallnau",
 title        = "Engineering of Component-Based Systems",
 chapter      = "",
 pages        = "7--13",
 publisher    = "IEEE",
 year         = "1996",
 volume       = "",
 series       = "",
 address      = "Los Alamitos, CA",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}

------------------------------------------------------------------------------

224

@Book{brooks95a,
 key       = "brooks95a",
 author    = "Fredrick P. Brooks",
 title     = "The Mythical Man-Month: Essays on Software Engineering, 20th
              Anniversary Edition",
 publisher = "Addison Wesley",
 edition   = "Second",
 month     = "",
 year      = "1995",
 volume    = "",
 series    = "",
 address   = "Reading, Mass.",
 note      = "",
}

------------------------------------------------------------------------------

225

@TechReport{johnson75a,
 key         = "johnson75a",
 author      = "S. C. Johnson",
 title       = "{YACC} --- {Y}et Another Compiler-Compiler",
 institution = "Bell Laboratories",
 month       = "",
 year        = "1975",
 number      = "No. 32",
 address     = "Murray Hill, N.J.",
 type        = "Computing Science Technical Report",
 note        = "",
}

------------------------------------------------------------------------------

226

@TechReport{lesk75a,
 key         = "lesk75a",
 author      = "M. E. Lesk and E. Schmidt",
 title       = "Lex --- {A} Lexical Analyzer Generator",
 institution = "Bell Laboratories",
 month       = "",
 year        = "1975",
 number      = "No. 39",
 address     = "Murray Hill, N.J.",
 type        = "Computing Science Technical Report",
 note        = "",
}

------------------------------------------------------------------------------

227

@InProceedings{hassal96a,
 key          = "hassal96a",
 author       = "Jack Hassall and Keith Oulton",
 title        = "Migrating a Legacy System to Object Technology",
 booktitle    = "Systematic Reuse: Issues in Initiating and Improving a Reuse
                 Program. Proceedings of the International Workshop on
                 Systematic Reuse",
 address      = "Liverpool, UK",
 month        = "8--9 " # jan,
 year         = "1996",
 pages        = "92--103",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The open systems movement has made the move to develop applications on, and
migrate applications to, UNIX a steady one.  The distributed applications
movement of the 1990s offers users the prospect of platform-independent
software.  The key enabling technology for this is standardised object
technology.  The idea is that you can create an object and be confident that
it will interoperate with other objects created by you and other people.
For this idea to work, there have to be object standards and wide adoption
of these standards.  The initial set of such standards now exists and they
have the support of the vast majority of the major computer, software
and systems vendors, as well as many of the world's largest end-users.
These standards are the Object Management Group (OMG) CORBA standards.
CORBA stands for Common Object Request Broker Architecture and Specification.
There are 520 OMG members, which makes it the world's largest software
development consortium.  During the last six years, OMG has established a
whole set of distributed applications standards, the most important of which
are how objects are defined (OMG Object Model), how objects communicate
with each other (OMG CORBA), and how objects are realised and maintained
(OMG CORBA services).
",
 note         = "",
}

------------------------------------------------------------------------------

228

@Article{barrett96a,
 key     = "barrett96a",
 author  = "Daniel J. Barrett and Alan Kaplan and Jack C. Wileden",
 title   = "Automated Support for Seamless interoperability in Polylingual
            Software Systems",
 journal = j-sig,
 month   = nov,
 year    = "1996",
 volume  = "21",
 number  = "6",
 pages   = "147--55",
 abstract = "
Interoperability is a fundamental concern in many areas of software
engineering, such as software reuse or infrastructures for software
development environments. Of particular interest to software engineers are
the interoperability problems arising in polylingual software systems. The
defining characteristic of polylingual systems is their focus on uniform
interaction among a set of components written in two or more different
languages. Existing approaches to support interoperability are inadequate
because they lack seamlessness: that is, they generally force software
developers to compensate explicitly for the existence of multiple languages
or the crossing of language boundaries. We first discuss some foundations for
polylingual interoperability, then review and assess existing approaches. We
then outline PolySPIN, an approach in which interoperability can be made
transparent and existing systems can be made to interoperate with no visible
modifications. We also describe PolySPINner, our prototype implementation of a
toolset providing automated support for PolySPIN. We illustrate the advantages
of our approach by applying it to an example problem and comparing PolySPIN's
ease of use with that of an alternative, CORBA-style approach (21 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

229

@Article{sullivan90a,
 key     = "sullivan90a",
 author  = "Kevin J. Sullivan and David Notkin",
 title   = "Reconciling Environment Integration and Component Independence",
 journal = j-sig,
 month   = dec,
 year    = "1990",
 volume  = "15",
 number  = "6",
 pages   = "22--33",
 abstract = "
An approach that eases the design and evolution of integrated environments by
increasing independence among components is presented. The approach combines
mediators, which localize relationships and a general event mechanism, which
increases the independence of components from relationships in which they
participate. To clarify the notion of independence and its relationship to
evolution, the authors analyze four designs for a simple environment. The
first three show how common approaches compromise independence in various
ways. The fourth design demonstrates how the approach presented overcomes
these problems. The event mechanism is specially designed to support
integration and evolution. The authors discuss detailed aspects of mediators
and events by presenting three environments they have built. The approach
has also given significant insights into other related systems (16 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

230

@Article{nierstrasz95a,
 key     = "nierstrasz95a",
 author  = "Oscar Nierstrasz and Theo Dirk Meijler",
 title   = "Research Directions in Software Composition",
 journal = j-acmcs,
 month   = jun,
 year    = "1995",
 volume  = "27",
 number  = "2",
 pages   = "262--7",
 abstract = "
Software composition is the construction of software applications from
components that implement abstractions pertaining to a particular problem
domain. Raising the level of abstraction is a time-honored way of dealing
with complexity, but the real benefit of composable software systems lies
in their increased flexibility: a system built from components should be
easy to recompose to address new requirements. A certain amount of success
has been achieved in some well-understood application domains, as witnessed
by the popularity of user-interface toolkits, fourth-generation languages,
and application generators. A generalization of this success is presented
(1 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

231

@Article{belady77a,
 key     = "belady77a",
 author  = "L. A. Belady and M. M. Lehman",
 title   = "Characteristics of Large Systems",
 journal = "IBM Systems Journal",
 month   = "",
 year    = 1977,
 volume  = "",
 number  = "3",
 pages   = "",
 abstract = "
",
 note    = "",
}

------------------------------------------------------------------------------

232

@Article{wegner96a,
 key     = "wegner96a",
 author  = "Peter Wegner",
 title   = "Interoperability",
 journal = j-acmcs,
 month   = mar,
 year    = "1996",
 volume  = "28",
 number  = "1",
 pages   = "285--7",
 abstract = "
Interoperability is the ability of two or more software components to
cooperate despite differences in language, interface, and execution
platform. It is a scalable form of reusability, being concerned with the
reuse of server resources by clients whose accessing mechanisms may be
plug-incompatible with sockets of the server. Plug compatibility arises most
literally with electrical appliances that require both static compatibility
of shape and dynamic compatibility of voltage and frequency. If there is
no direct match, interoperability of electrical appliances can be achieved
by adapters and transformers. The client-server software paradigm is a plug
and socket paradigm with static compatibility specified by types and dynamic
compatibility by protocols. As with electrical appliances, incompatibility
of software plugs and sockets can be mediated by adapters (8 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

233

@TechReport{chalasani98a,
 key         = "chalasani98a",
 author      = "Prasad Chalasani and Somesh Jha and Kevin Sullivan",
 title       = "An Options Approach to Software Prototyping",
 institution = "Carnegie Mellon University",
 month       = jun,
 year        = "1997",
 number      = "CMU-CS-TR-97-xxx",
 address     = "",
 type        = "Computer Science Technical Report",
 abstract    = "
Prototyping is often used to predict, or reduce the uncertainty over, the
future profitability of a software design choice. Boehm [3] pioneered
the use of techniques from Bayesian decision theory to provide a basis
for making prototyping decisions. However, this approach does not apply
to situations where the software engineer has the flexibility of waiting
for more information before making a prototyping decision. Also, this
framework only assumes uncertainty over one time period, and assumes a
design-choice must be made immediately after prototyping. We propose a more
general multi-period approach that takes into account the flexibility of
being able to postpone the prototyping and design decisions. In particular,
we argue that this flexibility is analogous to the flexibility of exercise
of certain financial instruments called options, and that the value of
the flexibility is the value of the corresponding financial option. The
field of real option theory in finance provides a rigorous framework to
analyze the optimal exercise of such options, and this can be applied to
the prototyping decision problem. Our approach integrates the timing of
prototype decisions and design decisions within a single framework.
",
 note        = "",
}

------------------------------------------------------------------------------

@Book{wall92a,
 key       = "wall92a",
 author    = "Larry Wall and Randal L. Schwartz",
 title     = "Programming Perl",
 publisher = "O'Reilly \& {Associates, Inc.}",
 edition   = "",
 month     = "",
 year      = "1992",
 volume    = "",
 series    = "",
 address   = "Newton, MA",
 note      = "",
}

------------------------------------------------------------------------------

@InProceedings{lea88a,
 key          = "lea88a",
 author       = "Douglas Lea",
 title        = "{libg++}, The {GNU C++} Library",
 booktitle    = "{USENIX} proceedings: {C++} Conference",
 address      = "Denver, Colorado",
 month        = "17--21 " # oct,
 year         = "1988",
 pages        = "243--56",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "USENIX Association",
 abstract = "
The GNU C++ library is a collection of C++ classes and support tools. The
paper describes several general strategies for structuring and designing
GNU C++ library classes, along with an informal taxonomy of library classes
and their implementations.
",
 note         = "",
}

------------------------------------------------------------------------------

234

@Article{caldiera91a,
 key     = "caldiera91a",
 author  = "Gianluigi Caldiera and Victor R. Basili",
 title   = "Identifying and Qualifying Reusable Software Components",
 journal = j-comp,
 month   = feb,
 year    = "1991",
 volume  = "",
 number  = "",
 pages   = "61--70",
 abstract = "
Identification and qualification of reusable software based on software models
and metrics is explored. Software metrics provide a way to automate the
extraction of reusable software components from existing systems, reducing the
amount of code that experts must analyze. Also, models and metrics permit
feedback and improvement to make the extraction process fit a variety of
environments. Some case studies are described to validate the experimental
approach. They deal with only the identification phase and use a very simple
model of a reusable code component, but the results show that automated
techniques can reduce the amount of code that a domain expert needs to
evaluate to identify reusable parts (2 Refs.)  
",
 note    = "",
}

------------------------------------------------------------------------------

235

@InProceedings{burd96a,
 key          = "burd96a",
 author       = "Elizabeth Burd and Malcolm Munro and Clazien Wezeman",
 title        = "Extracting Reusable Modules From Legacy Code: Considering the
                 Issues of Module Granularity",
 booktitle    = "Proceedings of the Third Working Conference on Reverse
               Engineering",
 address      = "Monterey, CA",
 month        = "8--10 " # nov,
 year         = "1996",
 pages        = "189--96",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The paper describes the work of a reverse engineering project. The project is
concerned with the identification of reusable modules of code from legacy
systems. The authors apply a number of existing techniques and methods to
large COBOL programs and attempt to integrate the methods to satisfy the needs
of their industrial sponsors. They have found that the issue of module
granularity is one of the important factors for successful reuse. By
integrating and applying parts of the RECAST method and techniques from the
RE\textsuperscript{2} paradigm, they have identified modules at different
levels of granularity. The paper includes some results and a discussion of the
identification of reuse modules at different levels of granularity (12 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

236

@InProceedings{etzkorn96a,
 key          = "etzkorn96a",
 author       = "L. H. Etzkorn and C. G. Davis and L. L. Bowen and D. B. Etzkorn
                 and L. W. Lewis and B. L. Vinz and J. C. Wolf",
 title        = "A Knowledge-Based Approach to Object-Oriented Legacy Code
                 Reuse",
 booktitle    = "Proceedings of ICECCS '96: 2nd IEEE International Conference
                 on Engineering of Complex Computer Systems (held jointly with
		 6th CSESAW and 4th IEEE RTAW)",
 address      = "Montreal, Que., Canada",
 month        = "21--25 " # oct,
 year         = "1996",
 pages        = "493--6",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Software reuse has been demonstrated to increase productivity, reduce costs
and improve software quality. Most research in the area of extraction of
reusable code from legacy code has concentrated on code created in the
functional decomposition paradigm. However, in recent years much
object-oriented code has been written. This paper describes a knowledge-based
approach to the identification of reusable components in object-oriented
legacy code (17 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

237

@InProceedings{sneed96b,
 key          = "sneed96b",
 author       = "Harry M. Sneed",
 title        = "Object-Oriented {COBOL} Recycling",
 booktitle    = "Proceedings of the Third Working Conference on Reverse
               Engineering",
 address      = "Monterey, CA",
 month        = "8--10 " # nov,
 year         = "1996",
 pages        = "169--78",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
In this paper a tool supported process for extracting objects from existing
COBOL program is described. The process is based on human interaction to
select objects couple with automated slicing techniques to identify all of the
elementary operations wich change the state of the object selected. The object
is redefined within the framework of an Ojbect-COBOL class and the elementary
operations are attached to it as methods. The result is a set of Object-COBOL
classes.
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{vanrossum96a,
 key       = "vanrossum96a",
 author    = "Guido van Rossum",
 title     = "Python Reference Manual",
 publisher = "Stichting Mathematisch Centrum",
 edition   = "",
 month     = "",
 year      = "1996",
 volume    = "",
 series    = "",
 address   = "Amsterdam",
 note      = "",
}

------------------------------------------------------------------------------

238

@Article{medvidovic97a,
 key     = "medvidovic97a",
 author  = "Nenad Medvidovic and Richard N. Taylor",
 title   = "A Framework for Classifying and Comparing Architecture Description
            Languages",
 journal = j-sig,
 month   = nov,
 year    = "1997",
 volume  = "22",
 number  = "6",
 pages   = "60--76",
 abstract = "
Software architectures shift developers' focus from lines-of-code to
coarser-grained architectural elements and their interconnection structure.
Architecture description languages (ADLs) have been proposed as modeling
notations to support architecture-based development.  There is, however,
little consensus in the research community on what an ADL is, what aspects of
an architecture should be modeled in an ADL and which ADL is best suited for a
particular problem.  Furthermore, the distinction is rarely made between ADLs
on the one hand and formal specification, module interconnection, simulation
and programming languages on the other.  This paper attempts to provide an
answer to these questions.  It motivates and presents a definition and a
classification framework for ADLs.  The utility of the definition is
demonstrated by using it to differentiate ADLs from other modeling notations.
The framework is used to classify and compare several existing ADLs (60 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

239

@TechReport{clements96a,
 key         = "clements96a",
 author      = "Paul C. Clements and Linda N. Northrop",
 title       = "Software Architecture: An Executive Overview",
 institution = "Software Engineering Institute",
 month       = "",
 year        = "1996",
 number      = "CMU/SEI-96-TR-003",
 address     = "Pittsburgh, PA",
 type        = "",
 abstract    = "
Software architecture is an area of growing importance to practitioners and
researchers in government, industry, and academia.  Journals and international
workshops are devoted to it. Working groups are formed to study it. Textbooks
are emerging about it.  The government is investing in the development of
software architectures as core products in their own right. Industry is
marketing architectural frameworks such as CORBA.  Why all the interest and
investment?  What is software architecture, and why is it perceived as
providing a solution to the inherent difficulty in designing and developing
large, complex systems? This report will attempt to summarize the concept of
software architecture for an intended audience of mid to senior level
management.  The reader is presumed to have some familiarity with common
software engineering terms and concepts, but not to have a deep background in
the field.  This report is not intended to be overly-scholarly, nor is it
intended to provide the technical depth necessary for practitioners and
technologists.  The intent is to distill some of the technical detail and
provide a high level overview. 
",
 note        = "",
}

------------------------------------------------------------------------------

240

@Article{adolph96a,
 key     = "adolph96a",
 author  = "W. Stephen Adolph",
 title   = "Cash Cow in the Tar Pit: Reengineering a Legacy System",
 journal = j-soft,
 month   = may,
 year    = "1996",
 volume  = "13",
 number  = "3",
 pages   = "41--47",
 abstract= "
Many old systems are still with us and burden us with baggage, but replacing
them, particularly when they are legacy systems, is not as straightforward as
it seems.  The author imparts some of the lessons learned on a
legacy-replacement project.
",
 note    = "",
}

------------------------------------------------------------------------------

241

@Article{yellin94a,
 key     = "yellin94a",
 author  = "Daniel M. Yellin and Robert E. Strom",
 title   = "Interfaces, Protocols, and the Semi-Automatic Construction of
            Software Adaptors",
 journal = "SIGPLAN Notices",
 month   = oct,
 year    = "1994",
 volume  = "29",
 number  = "10",
 pages   = "176--190",
 abstract = "
We show how to augment object-oriented application interfaces with enhanced
specifications that include sequencing constraints called protocols. Protocols
make explicit the relationship between messages (methods) supported by the
application. These relationships are usually only given implicitly, either in
the code or in textual comments.  We define notions of interface compatibility
based upon protocols and show how compatibility can be checked, discovering a
class of errors that cannot be discovered via the type system alone.  We then
define software adaptors that can be used to bridge the difference between
object-oriented applications that have functionally compatible but type
incompatible interfaces.  We discuss what it means for an adaptor to be
well-formed.  Leveraging the information provided by protocols, we show how
adaptors can be automatically generated from a high-level description, called
an interface mapping (24 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

242

@TechReport{keller97a,
 key         = "keller97a",
 author      = "Ralph Keller and Urs {H\"olzle}",
 title       = "Binary Component Adaptation",
 institution = "University of California",
 month       = dec,
 year        = "1997",
 number      = "TRCS97-20",
 address     = "Santa Barbara, CA",
 type        = "",
 abstract    = "
Binary component adaptation (BCA) allows components to be adapted and evolved
in binary form and on-the-fly (during program loading). BCA rewrites component
binaries before (or while) they are loaded, requires no source code access and
guarantees release-to-release compatibility. That is, an adaptation is
guaranteed to be compatible with a new binary release of the component as long
as the new release itself is compatible with clients compiled using the
earlier release. We describe our implementation of BCA for Java and
demonstrate its usefulness by showing how it can solve a number of important
integration and evolution problems. Even though our current implementation was
designed for easy integration with Sun's JDK 1.1 VM rather than for ultimate
speed, measurements show that the load-time overhead introduced by BCA is
small, in the range of one or two seconds. With its flexibility, relative
simple implementation, and low overhead, binary component adaptation could
significantly improve the reusability of Java components.
",
 note        = "",
}

------------------------------------------------------------------------------

243

@InProceedings{beach92a,
 key          = "beach92a",
 author       = "Brian W. Beach",
 title        = "Connecting Software Components with Declarative Glue",
 booktitle    = "Proceedings of the 14th International Conference on Software
                 Engineering",
 address      = "Melbourne, Vic., Australia",
 month        = "11--15 " # may,
 year         = "1992",
 pages        = "120--37",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM New York, NY",
 abstract = "
Bart is a software bus that addresses the problem of maintaining flexibility in
software systems by supporting component independence. Software components can
be built to be independent of the context in which they are used, allowing them
to be reused in many different situations. Component independence also allows a
software system to be extended by adding new components without modifying
existing ones. The connections between software components are described using
SGL, a declarative glue language that defines the relationships between data
models in different components. This glue language is compiled into an
efficient procedural form and, to reduce communication overhead, executed in
the process where the data resides. Bart is a software bus that handles message
transport, data sharing, and data translation. It operates in a distributed
environment and can connect components written in different programming
languages. The author illustrates Bart by showing how it is used to support a
hypertext system (31 Refs.)
",
 note         = "",
}

------------------------------------------------------------------------------

244

@Article{allen97a,
 key     = "allen97a",
 author  = "Robert Allen and David Garlan",
 title   = "A Formal Basis for Architectural Connection",
 journal = j-tsem,
 month   = jul,
 year    = "1997",
 volume  = "6",
 number  = "3",
 pages   = "213--49",
 abstract = "
As software systems become more complex, the overall system structureP or
software architecture becomes a central design problem. An important step
toward an engineering discipline of software is a formal basis for describing
and analyzing these designs. In this article we present a formal approach to
one aspect of architectural design: the interactions among components. The key
idea is to define architectural connectors as explicit semantic entities. These
are specified as a collection of protocols that characterize each of the
participant roles in an interaction and how these roles interact. We illustrate
how this scheme can be used to define a variety of common architectural
connectors. We further provide a formal semantics and show how this leads to a
system in which architectural compatibility can be checked in a way analogous
to type-checking in programming languages.
",
 note    = "",
}

------------------------------------------------------------------------------

245

@Misc{tornabene98a,
 key    = "tornabene98a",
 author = "Catherine Tornabene and Dorothea Beringer and Pankaj Jain and Gio
           Wiederhold",
 title  = "Composition on a Higher Level: {CHAIMS}",
 url    = "",
 abstract = "
Advances in computer networks that support the invocation of remote services
inheterogeneous environments enable new levels of software composition. In
order to manage composition at such a high level we envision a need
for purely compositional languages. The CHAIMS composition language we
introduce here is a megaprogramming language situated above the various
distribution protocols. This allows the assembly of components that are
accessible via differing distribution systems. By breaking up the
traditional CALL statement the CHAIMS language focuses on the asynchronous
composition of large-scale, autonomous modules. Furthermore the language
has the capability to support various optimizations that are specific to
software composition.
",
 note   = "",
}

------------------------------------------------------------------------------

246

@Article{parnas97a,
 key     = "parnas97a",
 author  = "David Lorge Parnas",
 title   = "Software Engineering: An Unconsummated Marriage",
 journal = j-sig,
 month   = nov,
 year    = "1997",
 volume  = "22",
 number  = "6",
 pages   = "40--50",
 abstract = "
When the first conference on ``Software Engineering'' was held, under NATO
sponsorship three decades ago in Munich, the vast majority of Engineers
ignored it. Electrical Engineers were obviously interested in building
computers, but they regarded programming those computers as something to be
done by others, often scientists who wanted the numerical results, or
mathematicians who were interested in numerical methods.  Programming was not
viewed as engineering, but as a trivial task, akin to using a calculator. An
engineer might have to perform such a task in order to get numerical results
needed for some other task, but their real job was the other task.
",
 note    = "",
}

------------------------------------------------------------------------------

247

@InProceedings{fuchs93a,
 key          = "fuchs93a",
 author       = "Norbert Fuchs",
 title        = "Software Engineering Still on the Way to an Engineering
                 Discipline",
 booktitle    = "Experimental Software Engineering Issues: Critical Assessment
                 and Future Directions",
 address      = "",
 month        = sep,
 year         = "1993",
 pages        = "19--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag, Berlin, Germany",
 abstract = "
The paper investigates the impact of experiments on the software engineering
discipline. First, the concept of goals and experiments is transferred to
software engineering as an engineering discipline.  This concept is well known
in other engineering disciplines as a mean to prove hypotheses based on models
of a development process. The impact of experimental software engineering on
the most problematic areas within software engineering is investigated. On the
basis of two examples-management methods and technology transfer-it is
discussed how experimental software engineering can be used to bring software
engineering to an engineering discipline.
",
 note         = "",
}

------------------------------------------------------------------------------

248

@Article{mcconnell98a,
 key     = "mcconnell98a",
 author  = "Steve McConnell",
 title   = "The Art, Science, and Engineering of Software Development",
 journal = j-soft,
 month   = jan # "--" # feb,
 year    = "1998",
 volume  = "15",
 number  = "1",
 pages   = "120, 118--119",
 abstract= "
What is the best way to think of software development? Is it science, art or
craft? Is it something else entirely? People who advocate programming as art
point to the aesthetic aspects of software development and argue that science
does not allow for such inspiration and creative freedom.  People who advocate
programming as science point to many programs' high error rates and argue that
such low reliability is intolerable. The author discusses software development
as an engineering discipline.
",
 note    = "",
}

------------------------------------------------------------------------------

249

@Article{wasserman96a,
 key     = "wasserman96a",
 author  = "Anthony I. Wasserman",
 title   = "Toward a Discipline of Software Engineering",
 journal = j-soft,
 month   = nov,
 year    = "1996",
 volume  = "13",
 number  = "6",
 pages   = "23--31",
 abstract= "
Despite rapid changes in computing and software development, some fundamental
ideas have remained constant. This article describes eight such concepts that
together constitute a viable foundation for a software engineering discipline:
abstraction, analysis and design methods and notations, user interface
prototyping, modularity and architecture, software life cycle and process,
reuse, metrics, and automated support.
",
 note    = "",
}

------------------------------------------------------------------------------

250

@Article{raccoon98a,
 key     = "raccoon98a",
 author  = "L. B. S. Raccoon",
 title   = "Toward a Tradition of Software Engineering",
 journal = j-sig,
 month   = may,
 year    = "1998",
 volume  = "23",
 number  = "3",
 pages   = "105--10",
 abstract = "
Will software engineering last? The author believes that our modern
culture will need more, better, and less expensive software. If developers
continue writing and maintaining programs for many decades to come, and we
continue aspiring to high standards of productivity and quality, then we
should pursue the goal of establishing software engineering as a bona fide
discipline of engineering. If we look primarily at the technical traditions of
software development, it seems clear that software developers have been doing
``engineering'' for a long time, now.  But, if we look primarily at the
corporate and academic traditions of software development, then we clearly
have a long way to go. We lack the maturity and confidence that long standing
academic and corporate traditions bring to a profession. The author believes
that it will take another generation of developers and another twenty to
thirty years for software engineering to become a tradition. If we work at it,
if we produce better programs at lower cost with higher quality, closer to
schedule, if we as individuals pass our best practices to colleagues and
successors, and if we treat software engineering as more than a job title,
then one day we will turn around and realize that we have succeeded. The
National Society of Professional Engineers notwithstanding, we will truly be
the ``software engineers'' that we aspire to be.
",
 note    = "",
}

------------------------------------------------------------------------------

251

@Article{maibaum97a,
 key     = "maibaum97a",
 author  = "TSE Maibaum",
 title   = "What we Teach Software Engineers in the University: Do We Take
            Engineering Seriously?",
 journal = j-sig,
 month   = nov,
 year    = "1997",
 volume  = "22",
 number  = "6",
 pages   = "40--50",
 abstract = "
Software engineering, as a discipline in its own right, is reaching the end of
its third decade. As such, we might legitimately be expecting that software
engineering curricula should reflect some level of maturity, both with respect
to the conception of software as an artefact and with respect to the
incorporation of effective engineering principles. I argue that this is simply
not the case, and further that the trend may well be the opposite of what
engineers from traditional disciplines might expect.
",
 note    = "",
}

------------------------------------------------------------------------------

252

@Article{parnas98a,
 key     = "parnas98a",
 author  = "David Lorge Parnas",
 title   = "Successful Software Engineering Research",
 journal = j-sig,
 month   = may,
 year    = "1998",
 volume  = "23",
 number  = "3",
 pages   = "64--8",
 abstract = "
Rumination about what makes research successful is a strong indication that a
researcher will not continue to do successful research. Nonetheless, the
invitation to publish a short article in SEN on the occasion of being honoured
by receiving SIGSOFT's ``Outstanding Research Award'' has led me to reflect on
what I have done. I have been active in research on software design for more
than 35 yeaars; perhaps this is the time to pause and look back. I also want
to look forward; I have some concerns about the direction obeing taken by many
researchers in the software community and would like to offer them my
(possibly unwelcome) advice.
",
 note    = "",
}

------------------------------------------------------------------------------

253

@Article{kapor91a,
 key     = "kapor91a",
 author  = "Mitchell Kapor",
 title   = "A Software Design Manifesto",
 journal = "Dr. Dobb's Journal",
 month   = jan,
 year    = "1991",
 volume  = "16",
 number  = "1",
 pages   = "62, 64--7",
 abstract = "
If software design is to be a profession in its own right, then there must be
professional training which develops the consciousness and skills central to
the profession. Training in software design is distinguished from computer
science, software engineering, and computer programming in that its principal
focus is on the training of professional practitioners whose work it is to
create usable computer-based artifacts, that is, software programs. The
emphasis on developing this specific professional competency distinguishes
software design on the one hand from computer science, which seeks to train
scientists in a theoretical discipline, and on the other, from engineering,
which focuses almost exclusively on the construction of the internals of
computer programs and, from the design point of view, gives short shrift to
consideration of use and users.
",
 note    = "",
}

------------------------------------------------------------------------------

254

@InProceedings{basili93a,
 key          = "basili93a",
 author       = "Victor R. Basili",
 title        = "The Experimental Paradigm in Software Engineering",
 booktitle    = "Experimental Software Engineering Issues: Critical Assessment
                 and Future Directions",
 address      = "",
 month        = sep,
 year         = "1993",
 pages        = "3--12",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag, Berlin, Germany",
 abstract = "
The author concentrates on three primary characteristics of software and
software engineering: its inherent complexity, the lack of well defined
primitives or components of the discipline, and the fact that software is
developed, not produced. This combination makes software something quite
different than anything dealt with before. He discusses research paradigms,
approaches to experimentation and the relationship between software research
and software development in practice.
",
 note         = "",
}

------------------------------------------------------------------------------

255

@Article{jackson98a,
 key     = "jackson98a",
 author  = "Michael Jackson",
 title   = "Will There Ever Be Software Engineering?",
 journal = j-soft,
 month   = jan # "--" # feb,
 year    = "1998",
 volume  = "15",
 number  = "1",
 pages   = "36--39",
 abstract= "
In his opening essay, Ed Yourdon forecasts both a happy and an unhappy future.
His bright future promises challenging projects, exciting technologies,
innovative applications, giant salaries, and lucrative stock options. His
gloomy future warns of US federal and state government departments unable to
solve their Year 2000 problem, a business revolt against expensive and
troublesome software that delivers no apparent economic benefits, and a
consequent drying up of money to buy new releases of COTS software or to
finance new software development. The good news and the bad news are
essentially commercial. But the dark cloud of Yourdon's bad future offers a
silver lining: if it comes to pass, there might be demands for certification
and licensing of software professionals and for a formal approach to software
development.  In a word, we might be expected to become serious software
engineers. We won't, of course. Yourdon is confident that we have learned a
lot about soft-ware processes, methods, and techniques; he says that we have a
vast body of knowledge in requirements, risk management, metrics, testing, and
quality assurance incorporated into the SEI Capability Maturity Model. So
that's all right, then.  We don't really need software engineering in the
narrow academic sense.
",
 note    = "",
}

------------------------------------------------------------------------------

256

@Article{jackson94b,
 key     = "jackson94b",
 author  = "Michael Jackson",
 title   = "Problems, Methods and Specialisation",
 journal = "Software Engineering Journal",
 month   = nov,
 year    = "1994",
 volume  = "9",
 number  = "6",
 pages   = "249--55",
 abstract = "
Software engineering is not a discipline; it is an aspiration, as yet
unachieved. Many approaches have been proposed, including reusable components,
formal methods, structured methods and architectural studies.  These
approaches chiefly emphasise the engineering product; the solution rather than
the problem it solves. An approach to understanding and classifying software
development problems in terms of problem frames is suggested. In addition to
such general approaches, specialisation is essential; the established branches
of engineering are all specialisations.  Some specialisations have arisen in
software development, notably in compiler construction and software for
personal computers.
",
 note    = "",
}

------------------------------------------------------------------------------

257

@Article{goldberg86a,
 key     = "goldberg86a",
 author  = "R. Goldberg",
 title   = "Software Engineering: An Emerging Discipline",
 journal = "IBM Systems Journal",
 month   = "",
 year    = 1986,
 volume  = "25",
 number  = "3--4",
 pages   = "334-53",
 abstract = "
Software engineering is an emerging discipline whose goal is to produce
reliable software products in a cost-effective manner.  This discipline is
evolving rapidly as the challenges faced by its practitioners keep extending
their skills. This paper gives a quick tour of the main ideas and thrusts that
have driven software engineering in its first 25 years and attempts to look
ahead at the next set of advances.
",
 note    = "",
}

------------------------------------------------------------------------------

258

@Article{rugaber98a,
 key     = "rugaber98a",
 author  = "Spencer Rugaber",
 title   = "Restoring a Legacy: Lessons Learned",
 journal = j-soft,
 month   = jul # "--" # aug,
 year    = "1998",
 volume  = "15",
 number  = "4",
 pages   = "28--33",
 abstract= "
Rebuilding a legacy system has some parallels to the restoration of a work of
art.  The authors draw upon this comparison to illustrate the challenges they
faced in redesigning a telephony system.  The restoration involved far more
than updating the code, the development team also had to understand the
existing architecture, add new functionality, and develop a long-term hardware
migration plan.  The technical and managerial lessons learned should prove
valuable to those involved in similar projects.
",
 note    = "",
}

------------------------------------------------------------------------------

259

@Article{tucker97a,
 key     = "tucker97a",
 author  = "Michael Jay Tucker",
 title   = "Bridge Your Legacy Systems to the Web",
 journal = "Datamation",
 month   = mar,
 year    = "1997",
 volume  = "43",
 number  = "",
 pages   = "114--121",
 abstract= "
Two types of middleware --- message-oriented middleware (MOM) and object
request brokers (ORBs)---are competing to become the network standard for
linking your legacy systms to the Web. Which type makes sense for you?
",
 note    = "",
}

------------------------------------------------------------------------------

260

@Article{schreiber95a,
 key     = "schreiber95a",
 author  = "Richard Schreiber",
 title   = "Glue Enterprisewide Apps Together",
 journal = "Datamation",
 month   = aug,
 year    = "1995",
 volume  = "41",
 number  = "15",
 pages   = "41--43",
 abstract= "
Suppose you had to build and run a set of business-critical applications
across an enterprise, and you have the typical hodgepodge of multiple
platforms, differing databases and file systems, immovable existing
applications that must coordinate with new applications, and a mix of LAN and
WAN protocols. ``A complex and challenging opportunity,'' you say to your
staff, but a little voice inside says, ``How about just plain ugly!''
",
 note    = "",
}

------------------------------------------------------------------------------

261

@Article{schreiber95b,
 key     = "schreiber95b",
 author  = "Richard Schreiber",
 title   = "Middleware Demystified",
 journal = "Datamation",
 month   = apr,
 year    = "1995",
 volume  = "41",
 number  = "",
 pages   = "41--45",
 abstract= "
Gag! Middleware! In the first of a three-part series, we make sense of a
confusing set of software that, when properly used, can be the key to
economical client/server development.
",
 note    = "",
}

------------------------------------------------------------------------------

262

@Article{schreiber95c,
 key     = "schreiber95c",
 author  = "Richard Schreiber",
 title   = "Workflow Imposes Order on Transaction Processing",
 journal = "Datamation",
 month   = jul,
 year    = "1995",
 volume  = "41",
 number  = "",
 pages   = "57--60",
 abstract= "
Reduce the complexity of constantly changing business-critical applications
with middleware, a traffic cop between the different functions of large
applications.
",
 note    = "",
}

------------------------------------------------------------------------------

263

@Article{yourdon98a,
 key     = "yourdon98a",
 author  = "Ed Yourdon",
 title   = "A Tale of Two Futures",
 journal = j-soft,
 month   = jan # "--" # feb,
 year    = "1998",
 volume  = "15",
 number  = "1",
 pages   = "23--29",
 abstract= "
Will the coming years bring the dawn of software's golden age or an
ever-worsening nightmare of disasters and cutbacks? The author explains why
either outcome, or both, are possible.  He discusses improvements such as new
application development tools and advanced hardware technology.  He then
focuses on the Year 2000 problem in contrast. 
",
 note    = "",
}

------------------------------------------------------------------------------

264

@Article{murphy97a,
 key     = "murphy97a",
 author  = "Gail C. Murphy",
 title   = "Reengineering with Reflexion Models: A Case Study",
 journal = j-comp,
 month   = aug,
 year    = "1997",
 volume  = "30",
 number  = "8",
 pages   = "29--36",
 abstract = "
Reengineering large and complex software systems is often very costly. The
article presents a reverse engineering technique and relates how a Microsoft
engineer used it to aid an experimental reengineering of Excel-a product that
comprises about 1.2 million lines of C code. The reflexion technique is
designed to be lightweight and iterative. To use it, the user first defines a
high-level structural model, then extracts a map of the source code and uses a
set of computation tools to compare the two models. The approach lets software
engineers effectively validate their high-level reasoning with information from
the source code. The engineer in this case study-a developer with 10-plus years
at Microsoft-specified and computed an initial reflexion model of Excel in a
day and then spent four weeks iteratively refining it. He estimated that
gaining the same degree of familiarity with the Excel source code might have
taken up to two years with other available approaches. On the basis of this
experience, the authors believe that the reflexion technique has practical
applications.
",
 note    = "",
}

------------------------------------------------------------------------------

265

@Article{lehman98a,
 key     = "lehman98a",
 author  = "M. M. Lehman",
 title   = "Software's Future: Managing Evolution",
 journal = j-soft,
 month   = jan # "--" # feb,
 year    = "1998",
 volume  = "15",
 number  = "1",
 pages   = "40--44",
 abstract= "
In his essay, Ed Yourdon expresses, justifies, and leaves unresolved two
well-founded questions: What is the future of software? What does the future
hold for the software professional? His prognosis is evasive, incomplete and
unsatisfying: the future will be good for some, not so for others.\par
Given Yourdon's extensive experience in the real world of computer usage, as
proven by the problems he has observed, it is easy to see why he feels that
softawre's future is uncertain. But he does not point to a solution to this
uncertainty, nore doe he indicate what can be done to achieve the best possible
outcome for software professionals. More importantly, Yourdon's analysis does
not indicate what should be done to ensure the security, well being, and
survival of society, which depends increasingly on software.
",
 note    = "",
}

------------------------------------------------------------------------------

266

@InProceedings{baniassad98a,
 key          = "baniassad98a",
 author       = "Elisa L. A. Baniassad and Gail C. Murphy",
 title        = "Conceptual Module Querying for Software Engineering",
 booktitle    = "Proceedings of the 1998 International Conference on Software
                 Engineering",
 address      = "Kyoto, Japan",
 month        = "19--25 " # apr,
 year         = "1998",
 pages        = "64--73",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE Comput. Soc, Los Alamitos, CA",
 abstract = "
Many tools have been built to analyze source. Most of these tools do not
adequately support reengineering activities because they do not allow a
software engineer to simultaneously perform queries about both the existing and
the desired source structure. This paper introduces the conceptual module
approach that overcomes this limitation. A conceptual module is a set of lines
of source that are treated as a logical unit.  We show how the approach
simplifies the gathering of source information for reengineering tasks, and
describe how a tool to support the approach was built as a front-end to
existing source analysis tools.
",
 note         = "",
}

------------------------------------------------------------------------------

267

@Article{voas98a,
 key     = "voas98a",
 author  = "Jeffrey Voas",
 title   = "Maintaining Component-Based Systems",
 journal = j-soft,
 month   = jul # "--" # aug,
 year    = "1998",
 volume  = "15",
 number  = "4",
 pages   = "22--7",
 abstract= "
As we continue to move toward component-based software engineering, software
development will become more like traditional manufacturing: developers will
code less and design and integrate more.  The author argues that to reap the
benefits of component-based development: reduced time to market, more user
choice, and lower costs, we must rethink our software maintenance strategies.
He gives a wide-ranging overview of the maintenance challenges raised by
component-based development. 
",
 note    = "",
}

------------------------------------------------------------------------------

268

@InCollection{sutherland91a,
 key       = "sutherland91a",
 author    = "Ivan Sutherland",
 title     = "Technology and Courage",
 booktitle = "Perspectives on Computer Science",
 pages     = "",
 publisher = "ACM Press and Addison-Wesley Publishing Co.",
 editor    = "Anita K. Jones",
 month     = "",
 year      = "1991",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
",
 note      = "",
}

------------------------------------------------------------------------------

269

@Article{bower95a,
 key     = "bower95a",
 author  = "Joseph L. Bower and Clayton M. Christensen",
 title   = "Disruptive Technologies: Catching the Wave",
 journal = "Harvard Business Review",
 month   = jan # "\slash" # feb,
 year    = "1995",
 volume  = "73",
 number  = "1",
 pages   = "43--53",
 abstract= "
One of the most consisten patterns in business is the failure of leading
companies to stay at the top of their industries when technologies or markets
change.
",
 note    = "",
}

------------------------------------------------------------------------------

@Book{aho86a,
 key       = "aho86a",
 author    = "Alfred V. Aho and Ravi Sethi and Jeffrey D. Ullman",
 title     = "Compilers---Principles, Techniques, and Tools",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1986",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

270

@Article{loy93a,
 key     = "loy93a",
 author  = "Patrick Loy",
 title   = "The method won't save you (but it can help)",
 journal = j-sig,
 month   = jan,
 year    = "1993",
 volume  = "18",
 number  = "1",
 pages   = "30--4",
 abstract = "
There is no single 'quick fix' to the problem of improving the software
development process. Most software development organizations desperately need
an improvement plan that is comprehensive in scope, encompassing not only
methods and tools, but every aspect of the development environment.  The
orderly, disciplined incorporation of a method can play a central part in such
a plan by encouraging and reinforcing skills and habits that are essential for
the plan to be effective.  Three of the most important of these skills are
discussed: model-building, unambiguous communication, and standardizing
procedures. 
",
 note    = "",
}

------------------------------------------------------------------------------

271

@Article{caplinskas97a,
 key     = "caplinskas97a",
 author  = "Albertas {\vCaplinskas}",
 title   = "Software System Engineering: Analysis of the Discipline",
 journal = "Informatica",
 month   = "",
 year    = "1997",
 volume  = "8",
 number  = "1",
 pages   = "57--82",
 abstract = "
Software system engineering has not yet developed an engineering science for
its discipline. On the other hand, a lot of fundamental concepts, shared
methods, techniques, patterns for structuring software systems, and languages
for documenting design decisions has been accumulated over the years. To
analyse and systematise the accumulated ideas is the main challenge for
computer scientists today. The main objective of this paper is to analyse
software system engineering both as a discipline and as an engineering science.
A special attention is paid to conceptual modelling formalisms used in software
system engineering. 
",
 note    = "",
}

------------------------------------------------------------------------------

272

@Article{ramamoorthy84a,
 key     = "ramamoorthy84a",
 author  = "C. V. Ramamoorthy and Atul Prakash and Wei-Tek Tsai and Yutaka
            Usuda",
 title   = "Software Engineering: Problems and Perspectives",
 journal = j-comp,
 month   = oct,
 year    = "1984",
 volume  = "17",
 number  = "10",
 pages   = "191--209",
 abstract = "
The authors discuss some important aspects of software engineering, noting past
accomplishments and speculating on trends and future needs in this important
area. In particular, they discuss the software life cycle; requirements and
specifications; software design; software maintenance; software quality
assurance; software reusability; and rapid prototyping.
",
 note    = "",
}

------------------------------------------------------------------------------

273

@Article{tichy93a,
 key     = "tichy93a",
 author  = "Walter F. Tichy and Nico Habermann and Lutz Prechelt",
 title   = "Future Directions in Software Engineering",
 journal = j-sig,
 month   = jan,
 year    = "1993",
 volume  = "18",
 number  = "1",
 pages   = "35--48",
 abstract = "
The intent of the workshop was to bring together leading scientists for
identifying promising directions for future research in Software Engineering.
The motivation for the wworkshop was the realization that Software Engineering
research was not in good shape, with the present emphasis on management and
risk control diverting attention from hard, technical subjects.\par

After week-long, intensive discussions of a great number of issues, the
following topics were seen as most crucial for progress:\par

1. Developing the are of software architecture as a foundation of Software
Engineering;\par
2. Learning to master evolving systems;\par
3. Building a scientific basis for Software Engineering;\par
4. Emphasizing sience and engineering know-how when educating
practitioners.\par

Formal methods, domain specific knowledge, special purpose languages, and
reuse were seen as important approaches, but not as solutions or ends in
themselves. There was also a fair amount of introspection on proper method,
evaluation, and experimentation in Software Engineering research.\par

This report contains participants' position statements and a record of the
discussions. The editors hope that it may help make research in Software
Engineering an exciting and thriving endeavor once again.
",
 note    = "",
}

------------------------------------------------------------------------------

274

@Article{andrews86a,
 key     = "andrews86a",
 author  = "Derek Andrews",
 title   = "Overview of Software Engineering",
 journal = "Data Processing",
 month   = mar,
 year    = "1986",
 volume  = "28",
 number  = "2",
 pages   = "64--78",
 abstract = "
It is nearly 17 years since the unofficial birth of 'software engineering' as
a discipline.  Mathematics can now be applied to the task of computer
programming-turning it into a science. The techniques developed over the
period have now matured to an extent where they can be applied in the 'real
world', rather than just in an academic environment.  Mathematics can be used
to specify a computer system at a very high level of abstraction, the
resultant specification being the computing equivalent of an engineer's
blueprint. Such a specification can be used as the basis for developing a
computer system, allowing each design decision to be documented and a clear
development path charted; thus making maintenance and improvements easier. The
exercise of writing a formal specification frequently reveals misconceptions
in the minds of both the developer and his customer. Further, the existence of
such a specification will allow the verification of the developed
software-mathematical proof of correctness.  The development of computer
software using formal methods is reviewed and some indication of future trends
given. How the ideas of software engineering can be applied today is also
discussed. 
",
 note    = "",
}

------------------------------------------------------------------------------

275

@InProceedings{shimizu98a,
 key          = "shimizu98a",
 author       = "Yoko Shimizu and Noboru Fujimaki and Masayuki Hirayama",
 title        = "A Systematic Approach to Domain-Oriented Software Development",
 booktitle    = "Proceedings of the 1998 International Conference on
                 Software Engineering",
 address      = "Kyoto, Japan",
 month        = "19--25 " # apr,
 year         = "1998",
 pages        = "499--502",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE Comput. Soc, Los Alamitos, CA",
 abstract = "
We describe our experience with domain-oriented software development in the
domain of automatic teller machine applications. We systematically proceeded
with development in four phases: domain analysis, domain formalization, domain
facility building, and product development. In this development, we built
domain facilities consisting of a domain framework and domain CASE (Computer
Aided Software Engineering) tools, then employed them for application
development. The framework shared about 4\% of the application, and the
remaining 96\% was generated by the CASE tools automatically. Our approach was
found to realize effective reuse of design and implementation and to enable
domain-oriented development in large domains. 
",
 note         = "",
}

------------------------------------------------------------------------------

276

@Article{horowitz85a,
 key     = "horowitz85a",
 author  = "Ellis Horowitz and Alfons Kemper and Balaji Narasimhan",
 title   = "A Survey of Application Generators",
 journal = j-soft,
 month   = jan,
 year    = "1985",
 volume  = "2",
 number  = "1",
 pages   = "40-54",
 abstract= "
The authors examine closely some of the existing application generators to see
precisely what facilities they offer and in what form.  They describe the
basic components that most of the existing systems share.  Then they
hypothesize a generic application generator.  Using this generic system, they
discuss the language features it offers, and write several programs as
demonstrations.  Finally, the authors contrast application generators to
general-purpose programming languages and discuss how programming languages
can be extended to support the development of data-intensive applications. 
",
 note    = "",
}

------------------------------------------------------------------------------

277

@Article{dikel97a,
 key     = "dikel97a",
 author  = "David Dikel and David Kane and Steve Ornburn and Jim Wilson",
 title   = "Applying Software Product-Line Architecture",
 journal = j-comp,
 month   = aug,
 year    = "1997",
 volume  = "30",
 number  = "8",
 pages   = "49--55",
 abstract = "
Software product-line architecture is a powerful way to control the risks and
take advantages of the opportunities of complex customer requirements, business
constraints, and technology, but its success depends on more than technical
excellence.
",
 note    = "",
}

------------------------------------------------------------------------------

278

@Article{raccoon97a,
 key     = "raccoon97a",
 author  = "L. B. S. Raccoon",
 title   = "Fifty Years of Progress in Software Engineering",
 journal = j-sig,
 month   = jan,
 year    = "1997",
 volume  = "22",
 number  = "1",
 pages   = "88--104",
 abstract = "
I describe a new outlook on the history of Software Engineering. I portray
large-scale structures within Software Engineering to give a better
understanding of the flow of history. I use these large-scale structures to
reveal the steady, ongoing evolution of concepts, and show how they relate to
the myriad whorls and eddies of change. I also have four smaller, more specific
purposes in writing this paper. First I want to point out that old ideas do not
die. Though the Waterfall model may not describe the whole truth, it describes
an interesting structure that occurs in many well-defined projects and it will
continue to describe this truth for a long time to come. I expect the Waterfall
model will live on for the next one hundred years and more. Second, I want to
show that the Chaos model, Chaos life cycle, Complexity Gap, and Chaos strategy
are part of the natural evolution of Software Engineering. The Chaos model and
strategy supersede, but do not contradict, the Waterfall and Spiral models, and
the Stepwise Refinement strategy. They are more up to date because they express
contemporary issues more effectively, and fit our contemporary situations
better. The Chaos model, life cycle, and strategy are equally as important but
not better than, other concepts. Third, I compare the Chaos model, life cycle,
and strategy to other models, life cycles, and strategies. Fourth, I make a few
predictions about the next ten years of Software Engineering. The large-scale
structures described in this history provide a stronger base for understanding
how software engineering will evolve in the future. 
",
 note    = "",
}

------------------------------------------------------------------------------

279

@Article{leibfried92a,
 key     = "leibfried92a",
 author  = "T. F. Leibfried, Jr. and R. B. MacDonald",
 title   = "Where is software engineering in the technical spectrum?",
 journal = "International Journal of Engineering Education",
 month   = "",
 year    = "1992",
 volume  = "8",
 number  = "6",
 pages   = "419--26",
 abstract = "
With the meteoric rise in the capabilities of computer hardware there is
general agreement that software as a discipline has not kept pace.  The need
for some sort of software design discipline is generally acknowledged but its
precise definition is a matter of dispute. Indeed there is some question as to
whether 'software engineering' is indeed a valid engineering discipline at all.
The historical roots of other engineering disciplines are traced and a parallel
is drawn to clarify the proper perspectives on 'software engineering'. A set of
criteria is constructed and a view of software engineering vis-a-vis computer
science is presented with a system criticality in mind. The type of academic
and professional-level education programs that would be required to support a
software engineering discipline are examined.
",
 note    = "",
}

------------------------------------------------------------------------------

280

@InProceedings{biggerstaff93a,
 key          = "biggerstaff93a",
 author       = "Ted J. Biggerstaff",
 title        = "Directions in Software Development and Maintenance",
 booktitle    = "1993 Conference on Software Maintenance",
 address      = "Quebec, Canada",
 month        = "27--30 " # sep,
 year         = "1993",
 pages        = "2--10",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Development environments are entering a period of dramatic change. A major
component of this change is a reorientation toward domain driven environments
including an integration of domain-oriented support tools. This reorientation
will bring about a decline in the role of conventional programming languages
and at the same time force an evolution toward more abstract programming
representations more abstract in the sense that most of the implementation
details (as they are currently known) will be abstracted away. Thus,
development is moving farther away from conventional software engineering
models and closer to the problem. 
",
 note         = "",
}

------------------------------------------------------------------------------

281

@InProceedings{coppit98b,
 key          = "coppit98b",
 author       = "David Coppit and Kevin J. Sullivan",
 title        = "Formal Specification in Collaborative Design of Critical
                 Software Tools",
 booktitle    = "Proceedings Third IEEE International High-Assurance Systems
                 Engineering Symposium",
 address      = "Washington, D.C.",
 month        = "13--14 " # nov,
 year         = "1998",
 pages        = "13--20",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Engineers use software tools to model and analyze designs for critical
systems. Because important design decisions are based on tool results, tools
must provide valid modeling constructs; engineers must understand them to
validate their models; and tools must implement these constructs without major
error. Such tools thus demand careful conceptual and software design. One
aspect of such design is the use of rigorous specification and design
techniques. This paper contributes a case study on the use of such techniques
in the collaborative development of a dynamic fault tree analysis tool. The
collaboration involved software engineering researchers knowledgable about
software specification and design and reliability engineering researchers
expert in fault tree analysis. Our work revealed conceptual and implementation
errors in an earlier version of the tool. Our study supports the position that
there is a need for rigorous software specification and design in developing
novel analysis tools, and that collaboration between software engineers and
domain experts is feasible and profitable.
",
 note         = "",
}

------------------------------------------------------------------------------

282

@InProceedings{manian98a,
 key          = "manian98a",
 author       = "Ragavan Manian and Joanne Bechta Dugan and David Coppit and
                 Kevin Sullivan",
 title        = "Combining Various Solution Techniques for Dynamic Fault Tree
                 Analysis of Computer Systems",
 booktitle    = "Proceedings Third IEEE International High-Assurance Systems
                 Engineering Symposium",
 address      = "Washington, D.C.",
 month        = "13--14 " # nov,
 year         = "1998",
 pages        = "21--28",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Fault trees provide a conceptually simple modeling framework to represent
system-level reliability in terms of interactions between component
reliabilities. DIFtree [1] effectively combines the best static fault tree
solution technique (Binary Decision diagrams) with Markov solution techniques
for dynamic fault trees. DIFtree includes advanced techniques for modeling
coverage; coverage modeling has been shown to be critical to the analysis of
fault tolerant computer systems. DIFtree is based on a divide-and-conquer
technique for modularizing the system level fault tree into independent
sub-trees; different solution techniques can be used for sub-trees. In this
paper we extend the DIFtree analysis capability to model several different
distributions of time to failure, including fixed probabilities (no time
component), exponential (constant hazard rate), Weibull (time varying hazard
rate), and log normal. Our approach extends both the BDD and Markov analytical
approaches and incorporates simulation as well.
",
 note         = "",
}

------------------------------------------------------------------------------

283

@InProceedings{weiss98a,
 key          = "weiss98a",
 author       = "David M. Weiss",
 title        = "Commonality Analysis: A Systematic Process for Defining
                 Families",
 booktitle    = "Development and Evolution of Software Architectures for
                 Product Families. Second International ESPRIT ARES
                 Workshop Proceedings.",
 address      = "Las Palmas de Gran Canaria, Spain",
 month        = "26--27 " # feb,
 year         = "1998",
 pages        = "214--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The success of family-oriented software development processes depends on how
well software engineers can predict the family members that will be needed.
Commonality analysis is an analytical technique for deciding what the members
of a family should be.  It is in use at Lucent Technologies as part of a
domain engineering process known as family-oriented abstraction,
specification, and translation (FAST).  Lucent software developers have
performed commonality analyses on more than 20 families; results have been
sufficiently encouraging that the analysis process is rapidly undergoing
institutionalization.
",
 note         = "",
}

------------------------------------------------------------------------------

284

@Article{bryant92a,
 key     = "bryant92a",
 author  = "Randal E. Bryant",
 title   = "Symbolic Boolean Manipulation with Ordered Binary-Decision
            Diagrams",
 journal = j-acmcs,
 month   = sep,
 year    = "1992",
 volume  = "24",
 number  = "3",
 pages   = "293--318",
 abstract = "
Ordered binary-decision diagrams (OBDDs) represent Boolean functions as
directed acyclic graphs. They form a canonical representation, making testing
of functional properties such as satisfiability and equivalence
straightforward. A number of operations on Boolean functions can be implemented
as graph algorithms on OBDD data structures. Using OBDDs, a wide variety of
problems can be solved through symbolic analysis. First, the possible
variations in system parameters and operating conditions are encoded with
Boolean variables. Then the system is evaluated for all variations by a
sequence of OBDD operations. Researchers have thus solved a number of problems
in digital-system design, finite-state-system analysis, artificial intelligence
and mathematical logic.  The author describes the OBDD data structure and
surveys a number of applications that have been solved by OBDD-based symbolic
analysis.
",
 note    = "",
}

------------------------------------------------------------------------------

285

@Article{vinoski96a,
 key     = "vinoski96a",
 author  = "Steve Vinoski",
 title   = "{CORBA}: Integrating Diverse Applications Within Distributed
            Heterogeneous Environments",
 journal = "IEEE Communications",
 month   = feb,
 year    = "1997",
 volume  = "35",
 number  = "2",
 pages   = "46--55",
 abstract = "
Large computer networks such as corporate intranets and the Internet are
inherently heterogeneous due to such factors as increasingly rapid
technological change, engineering trade-offs, accumulation of legacy systems
over time, and varying system costs.  Unfortunately, such heterogeneity makes
the development and maintenance of applications that make the best use of such
networks difficult. The Common Object Request Broker Architecture specification
created by the Object Management Group provides a stable model for distributed
object-oriented systems that helps developers cope with heterogeneity and
inevitable change. Applications written to the CORBA standard are abstracted
away from underlying networking protocols and transports, instead relying on
object request brokers to provide a fast and flexible communication and object
activation substrated. The abstractions provided by CORBA ORBs are currently
serving as the basis for applications in a wide variety of problem domains,
including telecommunications, finance, medicine, and manufacturing, running on
platforms ranging from mainframes down to test and measurement equipment. This
article first provides an overview of the Object Management Architecture, then
describes in detail the CORBA component of that architecture, and concludes
with a description of the OMG organization along with some of its current and
future work.
",
 note    = "",
}

------------------------------------------------------------------------------

@Misc{famoos99,
 key    = "famoos99",
 author = "",
 year   = "1999",
 title  = "{SCG/FAMOOS}",
 url    = "http://iamwww.unibe.ch/~famoos/",
 note   = "URL: {\urlBiBTeX{http://iamwww.unibe.ch/~famoos/}}",
}

------------------------------------------------------------------------------

@TechReport{stevens98,
 key         = "stevens98keller97a",
 author      = "Perdita Stevens and Rob Pooley",
 title       = "Software Reengineering Patterns",
 institution = "Department of Computer Science, Edinburgh University",
 month       = mar,
 year        = "1998",
 number      = "ECS-CSG-40-98",
 address     = "Edinburgh, United Kingdom",
 type        = "",
 abstract    = "
The problem of reengineering of legacy systems, in the widest sense, is widely
recognised as one of the most significant challenges facing software
engineers.  So-called legacy systems are normally, but not necessarily, large
systems built in an era before encapsulation and componentisation were
regarded as fundamental tenets of design.  Through a gradual process of
accretion and change, they have become devoid of useful structure.  This makes
them hard, expensive or impossible to modify in order to meet changes in the
business processes.  Legacy systems, whilst often essential to the running of
an organisation, also inhibit change in that organisation.  The problems of
legacy systems are not limited to any one kind of organisation: large
corporations and SMEs both suffer.  Moreover, there seems no reason to be
confident that today's new systems are not also tomorrow's legacy systems.
The problem of reengineering legacy systems is probably here to stay.  In this
paper we introduce the idea of software reengineering patterns, which adapt
the ideas of design patterns to identify lessons in successful reengineering
projects and to make these lessons available to new projects.  This is done in
the context of component based reengineering, which has been the focus of
considerable hope in the reengineering community, but which has delivered
limited successes so far.  These ideas are developed in terms of some
introductory examples taken from real projects.
",
 note        = "URL: {\urlBiBTeX{http://iamwww.unibe.ch/~famoos/}}",
 url         = "http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-40-98.ps.gz",
}

------------------------------------------------------------------------------

286

@Misc{brockschmidt96a,
 key    = "brockschmidt96a",
 author = "Kraig Brockschmidt",
 title  = "What {OLE} Is Really About",
 url    = "http://www.microsoft.com/oledev/olecom/aboutole.htm",
 note   = "URL: {\urlBiBTeX{http://www.microsoft.com/oledev/olecom/aboutole.htm}}",
}

------------------------------------------------------------------------------

@Book{baetjer98a,
 key       = "baetjer98a",
 author    = "Howard Baetjer",
 title     = "Software As Capital: An Economic Perspective on Software
                 Engineering ",
 publisher = "IEEE Computer Society Press",
 edition   = "",
 month     = "",
 year      = "1998",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

287

@InProceedings{harrison93a,
 key          = "harrison93a",
 author       = "William Harrison and Harold Ossher",
 title        = "Subject-Oriented Programming (A Critique of Pure Objects)",
 booktitle    = "Proceedings of the \mbox{OOPSLA}~'93 Conference on
                 Object-oriented Programming Systems, Languages and
                 Applications",
 address      = "",
 month        = oct,
 year         = "1993",
 pages        = "411--28",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE Comput. Soc, Los Alamitos, CA",
 abstract = "
Object-Oriented technology is often described in terms of an interwoven troika
of themes: encapsulation, polymorphism, and inheritance.  But these themes are
firmly tied with the concept of identity.  If object-oriented technology is to
be successfully scaled from the development of independent applications to
development of integrated suites of applications, it must relax its emphasis
on the object.  The technology must recognize more directly that a
multiplicity of subjective views delocalizes the concept of object, and must
emphasize more the binding concept of identity to tie them together.  The
authors explore this shift to a style of object-oriented technology that
emphasizes the subjective views: Subject-Oriented Programming. 
",
 note         = "",
}

------------------------------------------------------------------------------

289

@InProceedings{tarr99a,
 key          = "tarr99a",
 author       = "Peri Tarr and Harold L. Ossher and William H. Harrison and
                 Stanley M. {Sutton, Jr.}",
 title        = "{N} Degrees of Separation: Multi-Dimentional Separation of
                 Concerns",
 booktitle    = "Proceedings of the ~21st~ International Conference on
                 Software Engineering",
 address      = "",
 month        = may,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Done well, separation of concerns can provide many software engineering
benefits, including reduced complexity, improved reusability, and simpler
evolution. The choice of boundaries for separate concerns depends on both
requirements on the sytsem and on the kind(s) of decomposition and composition
a given formalism supports. The predominant methodologies and formalisms
availalbe, however, support only orthogonal separations of concerns, along
single dimensions of composition and decomposition. These characteristics
lead to a number of well-known and difficult problems.\par

This paper describes a new paradigm for modeling and implementing software
artifacts, one that permits separation of overlapping concerns along multiple
dimensions of composition and decomposition. This approach addresses numerous
problems throughout the software lifecycle in achieving well-engineered,
evolvable, flexible software artifacts and traceability across artifacts.
",
 note         = "",
}

------------------------------------------------------------------------------

290

@InProceedings{griswold99a,
 key          = "griswold99a",
 author       = "William G. Griswold",
 title        = "Coping With Software Change Using Information Transparency",
 booktitle    = "Proceedings of the ~21st~ International Conference on
                 Software Engineering",
 address      = "",
 month        = may,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Designers are often unsuccessful in designing for change using traditional
modularity techniques. A complementary modularity technique called information
transparency can improve a designer's ability to simplify changes by exposing
the interdependence of dispersed program elements that must be changed
together for correctness. Information transparency represents modules via
similarity and architecture, rather than locality and abstraction. With these,
a programmer can create locality with a software tool, easing change in much
the same way as traditional modularity.  When combined with information
hiding, then, more complex module structures can be represented. Information
transparency techniques include naming conventions, formatting style, and
ordering of code in a file. Transparency can be increased by better matching
tool capabilities and programming style. We discuss applications of
information transparency and introduce design principles for software
designers and tool designers. 
",
 note         = "",
}

------------------------------------------------------------------------------

291

@Misc{cardone99a,
 key    = "cardone99a",
 author = "Richard Cardone",
 title  = "On the Relationship of Aspect-Oriented Programming and GenVoca",
 url    = "http://www.cs.utexas.edu/users/richcar/aopwisr.html",
 note   = "URL: {\urlBiBTeX{http://www.cs.utexas.edu/users/richcar/aopwisr.html}}",
}

------------------------------------------------------------------------------

292

@TechReport{hursch95a,
 key         = "hursch95a",
 author       = "Walter {H\"ursch} and Cristina Videira Lopes",
 title        = "Separation of Concerns",
 institution = "College of Computer Science, Northeastern University",
 month       = feb # "24",
 year        = "1995",
 number      = "NU-CCS-95-03",
 address     = "Boston, Massachusetts",
 type        = "",
 abstract    = "
This paper identifies and analyzes the emergence of a new paradigm in software
engineering, called ``separation of concerns'', which tries to formally
separate the basic algorithm from special purpose concerns such as
synchronization, real-time constraints, and location control. This separation
allows for the locality of different kinds of information in the programs,
making them easier to write, understand, and modify. We identify the major
concerns existing in today's software applications, and analyze recent
proposals in the literature that address single concerns. Furthermore, we
study the commonalities of these proposals and discuss how they can be used to
achieve separation of concerns in general. Finally, we address the problem of
composition of several separated concerns. 
",
 note        = "URL: {\urlBiBTeX{ftp://www.ccs.neu.edu/pub/people/crista/papers/separation.ps}}",
 url         = "ftp://www.ccs.neu.edu/pub/people/crista/papers/separation.ps",
}

------------------------------------------------------------------------------

293

@InProceedings{hayden97a,
 key          = "hayden97a",
 author       = "Mark Hayden and Robbert van Renesse",
 title        = "Coping With Software Change Using Information Transparency",
 booktitle    = "Proceedings. The Sixth IEEE International Symposium
                 on High Performance Distributed Computing",
 address      = "Portland, OR",
 month        = "5--8 " # aug,
 year         = "1997",
 pages        = "169--77",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Layering of communication protocols offers many well-known advantages but
typically leads to performance inefficiencies.  We present a model for
layering, and point out where the performance problems occur in stacks of
layers using this model.  We then investigate the common execution paths in
these stacks and how to identify them.  These paths are optimized using three
techniques: optimizing the computation, compressing protocol headers, and
delaying processing.  All of the optimizations can be automated in a compiler
with the help of minor annotations by the protocol designer.  We describe the
performance that we obtain after implementing the optimizations by hand on a
full-scale system.
",
 note         = "",
}

------------------------------------------------------------------------------

294

@InProceedings{reps96a,
 key          = "reps96a",
 author       = "Thomas Reps and Todd Turnidge",
 title        = "Program Specialization Via Program Slicing",
 booktitle    = "Proceedings of the Dagstuhl Seminar on Partial Evaluation",
 address      = "Schloss Dagstuhl, Wadern, Germany",
 month        = "12--16 " # feb,
 year         = "1996",
 pages        = "409--429",
 editor       = "O. Danvy and R. Glueck and P. Thiemann",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag, New York, NY",
 abstract = "
This paper concerns the use of program slicing to perform a certain kind of
program-specialization operation. We show that the specialization operation
that slicing performs is different from the specialization operations
performed by algorithms for partial evaluation, supercompilation, bifurcation,
and deforestation. To study the relationship between slicing and these
operations in a simplified setting, we consider the problem of slicing
functional programs. We identify two different goals for what we mean by
``slicing a functional program'' and give algorithms that correspond to each of
them.
",
 note         = "URL: {\urlBiBTeX{http://www.cs.wisc.edu/wpis/papers/dagstuhl96.ps}}",
 url          = "http://www.cs.wisc.edu/wpis/papers/dagstuhl96.ps",
}

------------------------------------------------------------------------------

295

@InProceedings{heldal97a,
 key          = "heldal97a",
 author       = "Rogardt Heldal and John Hughes",
 title        = "Partial Evaluation and Separate Compilation",
 booktitle    = "Symposium on Partial Evaluation and Semantics-Based Program
                 Manipulation",
 address      = "Amsterdam",
 month        = "12--13 " # jun,
 year         = "1997",
 pages        = "1--11",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM SIGPLAN",
 abstract = "
Hitherto all partial evaluators have processed a complete program to produce a
complete residual program. We are interested in treating programs as
collections of modules which can be processed independently - separate partial
evaluation. In this paper we assume that the original program is processed in
its entirety, and show how to specialise it to the static data bit-by-bit,
generating a different module for each bit. When the program to be specialised
is an interpreter, this corresponds to specialising it to one module of its
object language at a time: each module of the object language gives rise to one
module of the residual program. 
",
 note         = "",
}

------------------------------------------------------------------------------

296

@InProceedings{consel96a,
 key          = "consel96a",
 author       = "Charles Consel and Luke Hornof and {Fran\c{c}ois No\"el} and 
                 Jacques {Noy\'e} and Nicolae Volanschi",
 title        = "A uniform approach for compile-time and run-time
                 specialization",
 booktitle    = "Partial Evaluation. International Seminar.",
 address      = "Dagstuhl Castle, Germany",
 month        = "12-16 " # feb,
 year         = "1996",
 pages        = "54--72",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag, Berlin, Germany",
 abstract = "
As partial evaluation gets more mature, it is now possible to use this program
transformation technique to tackle realistic languages and real-size
application programs.  However, this evolution raises a number of critical
issues that need to be addressed before the approach becomes truly practical.
First of all, most existing partial evaluators have been developed based on the
assumption that they could process any kind of application program. This
attempt to develop universal partial evaluators does not address some critical
needs of real-size application programs. Furthermore, as partial evaluators
treat richer and richer languages, their size and complexity increase
drastically. This increasing complexity reveals the need to enhance design
principles. Finally, exclusively specializing programs at compile time
seriously limits the applicability of partial evaluation since a large class of
invariants in real-size programs are not known until run time and therefore
cannot be taken into account. In this paper, we propose design principles and
techniques to deal with each of these issues. By defining an architecture for a
partial evaluator and its essential components, we are able to tackle a rich
language like C without compromising the design and the structure of the
resulting implementation. By designing a partial evaluator targeted towards a
specific application area, namely system software, we have developed a system
capable of treating realistic programs. Because our approach to designing a
partial evaluator clearly separates preprocessing and processing aspects, we
are able to introduce run-time specialization in our partial evaluation system
as a new way of exploiting information produced by the preprocessing phase.
",
 note         = "",
}

------------------------------------------------------------------------------

297

@Article{jones96a,
 key     = "jones96a",
 author  = "Neil D. Jones",
 title   = "An Intoduction to Partial Evaluation",
 journal = j-acmcs,
 month   = sep,
 year    = "1996",
 volume  = "28",
 number  = "3",
 pages   = "480--503",
 abstract = "
Partial evaluation provides a unifying paradigm for a broad spectrum of work in
program optimization, compiling, interpretation and the generation of automatic
program generators (Bjorner et al., 1987; Ershov, 1992; and Jones et al.,
1993). It is a program optimization technique, perhaps better called program
specialization, closely related to but different from Jarring and Scherlis'
(1986) staging transformations. It emphasizes, in comparison with Burstall and
Darlington (1977) and Jarring and Scherlis (1986) and other program
transformation work, full automation and the generation of program generators
as well as transforming single programs. Much partial evaluation work to date
has concerned automatic compiler generation from an interpretive definition of
a programming language, but it also has important applications in scientific
computing, logic programming, metaprogramming, and expert systems.
",
 note    = "",
}

------------------------------------------------------------------------------

298

@Article{batory93a,
 key     = "batory93a",
 author  = "Don Batory and Vivek Singhal and Marty Sirkin and Jeff Thomas",
 title   = "Scalable Software Libraries",
 journal = j-sig,
 month   = dec,
 year    = "1993",
 volume  = "18",
 number  = "5",
 pages   = "191--9",
 abstract = "
Many software libraries (e.g., the Booch C++ Components, libg++, NIHCL, COOL)
provide components (classes) that implement data structures. Each component is
written by hand and represents a unique combination of features (e.g.
concurrency, data structure, memory allocation algorithms) that distinguishes
it from other components. We argue that this way of building data structure
component libraries is inherently unscalable. Libraries should not enumerate
complex components with numerous features; rather, libraries should take a
minimalist approach: they should provide only primitive building blocks and be
accompanied by generators that can combine these blocks to yield complex custom
data structures. In this paper, we describe a prototype data structure
generator and the building blocks that populate its library. We also present
preliminary experimental results which suggest that this approach does not
compromise programmer productivity nor the run-time performance of generated
data structures. 
",
 note    = "",
}

------------------------------------------------------------------------------

299

@Misc{batory98a,
 key    = "batory98a",
 author = "Don Batory",
 title  = "Product-Line Architectures",
 url    = "ftp://ftp.cs.utexas.edu/pub/predator/stja.ps",
 abstract = "
Today's software design methodologies are aimed at one-of-a-kind applications,
designs are expressed in terms of objects and classes, and software must be
coded manually. We argue that future software development will be very
different and will center around product-line architectures (i.e., designs for
families of related applications), refinements (a generalization of today's
components), and software plug-and-play (a codeless form of programming)
",
 note   = "URL: {\urlBiBTeX{ftp://ftp.cs.utexas.edu/pub/predator/stja.ps}}",
}

------------------------------------------------------------------------------

300

@Article{batory97a,
 key     = "batory97a",
 author  = "Don Batory and Jeff Thomas",
 title   = "{P2}: A Lightweight {DBMS} Generator",
 journal = "Journal of Intelligent Information Systems",
 month   = sep # "--" # oct,
 year    = "1997",
 volume  = "9",
 number  = "2",
 pages   = "107--23",
 abstract = "
A lightweight database system (LWDB) is a high-performance,
application-specific DBMS. It differs from a general-purpose (heavyweight) DBMS
in that it omits one or more features and specializes the implementation of its
features to maximize performance. Although heavyweight monolithic and
extensible DBMSs might be able to emulate LWDB capabilities, they cannot match
LWDB performance. We describe P2, a generator of lightweight DBMSs, and explain
how it was used to reengineer a hand-coded, highly-tuned LWDB used in a
production system compiler (LEAPS). We present results that show P2-generated
LWDBs reduced the development time and code size of LEAPS by a factor of three
and that the generated LWDBs executed substantially faster than versions built
by hand or that use an extensible heavyweight DBMS. 
",
 note    = "",
}

------------------------------------------------------------------------------

301

@TechReport{thomas95a,
 key         = "thomas95a",
 author      = "Jeff Thomas and Don Batory",
 title       = "{P2}: An Extensible Lightweight {DBMS}",
 institution = "Department of Computer Sciences, University of Texas",
 month       = feb,
 year        = "1995",
 number      = "TR-95-04",
 address     = "Austin, Texas",
 type        = "",
 abstract    = "
A lightweight database system (LWDB) is a high-performance,
application-specific DBMS. It differs from a general-purpose (heavyweight) DBMS
in that it omits one or more features and specializes the implementation of its
features to maximize performance. Although heavyweight monolithic and
extensible DBMSs might be able to emulate LWDB capabilities, they cannot match
LWDB performance. \par

In this paper, we explore LWDB applications, systems, and implementation
techniques. We describe P2, an extensible lightweight DBMS, and explain how it
was used to reengineer a hand-coded, highly-tuned LWDB used in a production
system compiler (LEAPS). We present results that show P2-generated LWDBs for
LEAPS executes substantially faster than versions built by hand or that use an
extensible heavyweight DBMS. 
",
 note        = "URL: {\urlBiBTeX{ftp://ftp.cs.utexas.edu/pub/predator/tr-95-04.ps.Z}}",
 url         = "ftp://ftp.cs.utexas.edu/pub/predator/tr-95-04.ps.Z",
}

------------------------------------------------------------------------------

302

@Article{batory97b,
 key     = "batory97b",
 author  = "Don Batory and Bart J. Geraci",
 title   = "Composition Validation and Subjectivity in GenVoca Generators",
 journal = j-tse,
 month   = feb,
 year    = "1997",
 volume  = "23",
 number  = "2",
 pages   = "67--82",
 abstract = "
GenVoca generators synthesize software systems by composing components from
reuse libraries. GenVoca components are designed to export and import
standardized interfaces, and thus be plug-compatible, interchangeable, and
interoperable with other components. In this paper, we examine two different
but important issues in software system synthesis. First, not all syntactically
correct compositions of components are semantically correct. We present simple,
efficient, and domain-independent algorithms for validating compositions of
GenVoca components. Second, components that export and import immutable
interfaces are too restrictive for software system synthesis. We show that the
interfaces and bodies of GenVoca components are subjective, i.e., they mutate
and enlarge upon instantiation. This mutability enables software systems with
customized interfaces to be composed from components with ``standardized''
interfaces.
",
 note    = "",
}

------------------------------------------------------------------------------

303

@InProceedings{batory98b,
 key          = "batory98b",
 author       = "Don Batory and Bernie Lofaso and Yannis Smaragdakis",
 title        = "{JTS}: Tools for Implementing Domain-Specific Languages",
 booktitle    = "Proceedings Fifth International Conference on Software Reuse",
 address      = "Victoria, BC, Canada",
 month        = "2--5 " # jun,
 year         = "1998",
 pages        = "143--53",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The Jakarta Tool Suite (JTS) aims to reduce substantially the cost of generator
development by providing domain-independent tools for creating domain-specific
languages and component-based generators called GenVoca generators. JTS is a
set of precompiler-compiler tools for extending industrial programming
languages (e.g., Java) with domain-specific constructs. JTS is itself a GenVoca
generator where precompilers for JTS-extended languages are constructed from
components. 
",
 note         = "",
}

------------------------------------------------------------------------------

304

@InProceedings{kiczales97a,
 key          = "kiczales97a",
 author       = "Gregor Kiczales and John Lamping and Cristina Videira
                 Lopes and Chris Maeda and Anurag Mendhekar and Gail
                 Murphy",
 title        = "Open Implementation Design Guidelines",
 booktitle    = "Proceedings of the 19th International Conference on Software
                 Engineering",
 address      = "Boston, Massachusetts",
 month        = "17--23 " # may,
 year         = "1997",
 pages        = "481--90",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
Designing reusable software modules can be extremely difficult. The design must
be balanced between being general enough to address the needs of a wide range
of clients and being focused enough to truly satisfy the requirements of each
specific client. One area where it can be particularly difficult to strike this
balance is in the implementation strategy of the module. The problem is that
general-purpose implementation strategies, tuned for a wide range of clients,
aren't necessarily optimal for each specific client---this is especially an
issue for modules that are intended to be reusable and yet provide
high-performance.  An examination of existing software systems shows that an
increasingly important technique for handling this problem is to design the
module's interface in such a way that the client can assist or participate in
the selection of the module's implementation strategy. We call this approach
open implementation.\par

When designing the interface to a module that allows its clients some control
over its implementation strategy, it is important to retain, as much as
possible, the advantages of traditional closed implementation modules. This
paper explores issues in the design of interfaces to open implementation
modules. We identify key design choices, and present guidelines for deciding
which choices are likely to work best in particular situations.
 ",
 note         = "",
}

------------------------------------------------------------------------------

305

@InProceedings{maeda97a,
 key          = "maeda97a",
 author       = "Chris Maeda and Arthur Lee and Gail Murphy and Gregor
                 Kiczales",
 title        = "Open Implementation Analysis and Design",
 booktitle    = "Proceedings of the Symposium on Software Reusability",
 address      = "",
 month        = may,
 year         = "1997",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
This paper describes a methodology for designing Open Implementations --
software modules that can adapt or change their internals to accommodate the
needs of different clients.  Analysis techniques are used for capturing domain
knowledge, user requirements, and domain properties that influence the module's
eventual implementation. Design techniques are used for determining and
refining the interfaces by which clients control the modules implementation
strategies. The methodology has evolved over the past two years in several
pilot projects.
 ",
 note         = "",
}

------------------------------------------------------------------------------

306

@InProceedings{kiczales97b,
 key          = "kiczales97b",
 author       = "Gregor Kiczales and John Lamping and Anurag Mendhekar and
                 Chris Maeda and Cristina Videira Lopes and Jean-Marc
                 Loingtier and John Irwin",
 title        = "Aspect-Oriented Programming",
 booktitle    = "Proceedings of the European Conference on Object-Oriented
                 Programming (ECOOP)",
 address      = "Finland",
 month        = jun,
 year         = "1997",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract     = "
We have found many programming problems for which neither procedural nor
object-oriented programming techniques are sufficient to clearly capture some
of the important design decisions the program must implement. This forces the
implementation of those design decisions to be scattered through-out the code,
resulting in ``tangled'' code that is excessively difficult to develop and
maintain. We present an analysis of why certain design decisions have been so
difficult to clearly capture in actual code. We call the properties these
decisions address aspects, and show that the reason they have been hard to
capture is that they cross-cut the system's basic functionality. We present the
basis for a new programming technique, called aspect-oriented programming, that
makes it possible to clearly express programs involving such aspects, including
appropriate isolation, composition and re-use of the aspect code. The
discussion is rooted in systems we have built using aspect-oriented
programming.
 ",
 note         = "",
}

------------------------------------------------------------------------------

307

@TechReport{keller97b,
 key         = "keller97b",
 author      = "Ralph Keller and Urs {H\"olzle}",
 title       = "Supporting the Integration and Evolution of Components Through
                Binary Component Adaptation",
 institution = "Department of Computer Science, University of California",
 month       = sep,
 year        = "1997",
 number      = "TRCS97-15",
 address     = "Santa Barbara, California",
 type        = "",
 abstract    = "
Object-oriented components are hard to integrate if developed independently of
each other, and difficult to evolve without affecting existing clients,
particularly with widely distributed components that have thousands of reusers.
We propose binary component adaptation (BCA), a new solution that allows
components to be adapted and evolved in binary form and on-the-fly (during
program load ing). Binary component adaptation rewrites component binaries
before (or while) they are loaded and is significantly more flexible than
previous approaches. Also, BCA requires no source code access and guarantees
release-to-release compatibility. That is, an adaptation is guaranteed to be
compatible with a new binary release of the component as long as the new
release itself is compatible with clients compiled using the earlier release.
We show how binary component adaptation can solve a number of important
integration and evolution problems and discuss how it can be implemented for
JVM class files (e.g., Java programs). We believe that binary component
adaptation could significantly improve the integration and evolution of
software components, especially in a relatively uncoordinated and fast-evolving
environment such as the Internet. 
",
 note        = "URL: {\urlBiBTeX{http://www.cs.ucsb.edu/oocsb/papers/bca.html}}",
 url         = "http://www.cs.ucsb.edu/oocsb/papers/bca.html",
}

------------------------------------------------------------------------------

308

@InProceedings{keller98a,
 key          = "keller98a",
 author       = "Ralph Keller and Urs {H\"olzle}",
 title        = "Binary Component Adaptation",
 booktitle    = "ECOOP'98 - Object-Oriented Programming. 12th European
                 Conference. Proceedings",
 address      = "Brussels, Belgium",
 month        = "20--24 " # jul,
 year         = "1998",
 pages        = "307-29",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract     = "
Binary component adaptation (BCA) allows components to be adapted and evolved
in binary form and on-the-fly (during program loading). BCA rewrites component
binaries before (or while) they are loaded, requires no source code access and
guarantees release-to-release compatibility. That is, an adaptation is
guaranteed to be compatible with a new binary release of the component as long
as the new release itself is compatible with clients compiled using the earlier
release. The authors describe their implementation of BCA for Java and
demonstrate its usefulness by showing how it can solve a number of important
integration and evolution problems.  Even though our current implementation was
designed for easy integration with Sun's JDK 1.1 VM rather than for ultimate
speed, the load-time overhead introduced by BCA is small, in the range of one
or two seconds. With its flexibility, relatively simple implementation, and low
overhead, binary component adaptation could significantly improve the
reusability of Java components. 
 ",
 note         = "",
}

------------------------------------------------------------------------------

309

@InProceedings{holzle93a,
 key          = "holzle93a",
 author       = "Urs {H\"olzle}",
 title        = "Integrating Independently--Developed Components in
                 Object-Oriented Languages ",
 booktitle    = "ECOOP'93 - Object-Oriented Programming. 12th European
                 Conference. Proceedings",
 address      = "Kaiserslautern, Germany",
 month        = "26--30 " # jul,
 year         = "1993",
 pages        = "36-56",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract     = "
Object-oriented programming promises to increase programmer productivity
through better reuse of existing code. However, reuse is not yet pervasive in
today's object-oriented programs. Why is this so? We argue that one reason is
that current programming languages and environments assume that components are
perfectly coordinated. Yet in a world where programs are mostly composed out of
reusable components, these components are not likely to be completely
integrated because the sheer number of components would make global
coordination impractical. Given that seemingly minor inconsistencies between
individually designed components would exist, we examine how they can lead to
integration problems with current programming language mechanisms. We discuss
several reuse mechanisms that can adapt a component in place without requiring
access to the component's source code and without needing to re-typecheck it. 
 ",
 note         = "",
}

------------------------------------------------------------------------------

310

@Article{clarke97a,
 key     = "clarke97a",
 author  = "E. Clarke and O. Grumberg and K. Hamaguchi",
 title   = "Another Look at LTL Model Checking",
 journal = "Formal Methods in System Design",
 month   = feb,
 year    = "1997",
 volume  = "10",
 number  = "1",
 pages   = "47-71",
 abstract = "
We show how LTL model checking can be reduced to CTL model checking with
fairness constraints. Using this reduction, we also describe how to construct a
symbolic LTL model checker that appears to be quite efficient in practice. In
particular, we show how the SMV model checking system developed by McMillan
(1993) can be extended to permit LTL specifications. The results that we have
obtained are quite surprising. For the specifications which can be expressed in
both CTL and LTL, the LTL model checker required at most twice as much time and
space as the CTL model checker. We also succeeded in verifying non-trivial LTL
specifications.  The amount of time and space that is required is quite
reasonable. Based on the examples that we considered, it appears that efficient
LTL model checking is possible when the specifications are not excessively
complicated. 
",
 note    = "",
}

------------------------------------------------------------------------------

311

@Article{allen98a,
 key     = "allen98a",
 author  = "Robert J. Allen and David Garlan and James Ivers",
 title   = "Formal Modeling and Analysis of the {HLA} Component Integration
            Standard",
 journal = j-sig,
 month   = nov,
 year    = "1988",
 volume  = "23",
 number  = "6",
 pages   = "70--9",
 abstract = "
An increasingly important trend in the engineering of complex systems is the
design of component integration standards. Such standards define rules of
interaction and shared communication infrastructure that permit composition of
systems out of independently-developed parts. A problem with these standards is
that it is often difficult to understand exactly what they require and provide,
and to analyze them in order to understand their deeper properties. We use our
experience in modeling the High Level Architecture (HLA) for Distributed
Simulation to show how one can capture the structured protocol inherent in an
integration standard as a formal architectural model that can be analyzed to
detect anomalies, race conditions, and deadlocks. 
",
 note    = "",
}

------------------------------------------------------------------------------

312

@Article{hoare83a,
 key     = "hoare83a",
 author  = "C. A. R. Hoare",
 title   = "Communicating Sequential Processes",
 journal = j-cacm,
 month   = jan,
 year    = "1983",
 volume  = "26",
 number  = "1",
 pages   = "100--6",
 abstract= "
This paper suggests that input and output are basic primitives of programming
and that parallel composition of communicating sequential processes is a
fundamental program structuring method. When combined with a development of
Dijkstra's guarded command, these concepts are surprisingly versatile. Their
use is illustrated by sample solutions of a variety of familiar programming
exercises. 
",
 note    = "",
}

------------------------------------------------------------------------------

313

@InProceedings{tarr98a,
 key          = "tarr98a",
 author       = "Peri Tarr and Lori A. Clarke",
 title        = "Consistency Management for Complex Applications",
 booktitle    = "Proceedings of the 1998 International Conference on
                 Software Engineering",
 address      = "Kyoto, Japan",
 month        = "19--25 " # apr,
 year         = "1998",
 pages        = "230--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE Comput. Soc, Los Alamitos, CA",
 abstract = "
Consistency management is important in many complex applications, but current
languages and database systems inadequately support it. To address this
limitation, we defined a consistency management model and incorporated it into
the PLEIADES object management system. This paper illustrates some typical
consistency management requirements and discusses the requirements in terms of
both functionality and cross-cutting concerns that affect how this
functionality is provided. It then describes the model and some design and
implementation issues that arose in instantiating it.  Finally, we discuss user
feedback and future research plans. 
",
 note         = "",
}

------------------------------------------------------------------------------

314

@InProceedings{holder99a,
 key          = "holder99a",
 author       = "Ophir Holder and Israel Ben-Shaul and Hovav Gazit",
 title        = "Dynamic Layout of Distributed Applications in FarGo",
 booktitle    = "Proceedings of the ~21st~ International Conference on
                 Software Engineering",
 address      = "",
 month        = may,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
The design of efficient and reliable distributed applications that operate in
large networks, over links with varying capacities and loads,demands new
programming abstractions and mechanisms. The conventional static design-time
determination of local-remote relationships bewteen components implies that
(dynamic) environmental changes are hard if not impossible to address without
reenginereing. This paper presents a novel programming model that is centered
around the concept of ``dynamic application layout'', which permits the
manipulation of component location at runtime. This leads to a clean separation
between the programming of the application's logic and the programming of the
layout, which can also be performed externally at runtime. The main abstraction
vehicle for layout programming is a reflective inter-component reference, which
embodies co- and re-location semantics. We describe an extensible set of
reference types that drive and constrain the mapping of components to hosts,
and show how this model elevates application's performance and reliability yet
requires minimal changes in programmin ghte applications logic. The model was
realized in the FarGo system, whose design and implementation in Java are
presented, along with an event-based scripting language and corresponding
event-monitoring service for managing the layout of FarGo applications.
",
 note         = "",
}

------------------------------------------------------------------------------

315

@InBook{kiczales93a,
 key          = "kiczales93a",
 author       = "Gregor Kiczales and J. Michael Ashley and  Luis Rodriguez
                 and  Amin Vahdat and Daniel G. Bobrow",
 title        = "Metaobject Protocols: Why We Want Them and What Else They Can
                 Do",
 chapter      = "",
 pages        = "101--118",
 publisher    = "The MIT Press",
 year         = "1993",
 volume       = "",
 series       = "",
 address      = "Cambridge, MA",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 abstract     = "
Originally conceived as a neat idea that could help solve problems in the
design and implementation of CLOS, the metaobject protocol framework now
appears to have applicability to a wide range of problems that come up in
high-level languages. This chapter sketches this wider potential, by drawing an
analogy to ordinary language design, by presenting some early design
principles, and by presenting an overview of three new metaobject protcols we
have designed that, respectively, control the semantics of Scheme, the
compilation of Scheme, and the static parallelization of Scheme programs. 
 ",
 note         = "",
}

------------------------------------------------------------------------------

316

@InProceedings{garlan86a,
 key          = "garlan86a",
 author       = "David Garlan",
 title        = "Views for Tools in Integrated Environments",
 booktitle    = "Advanced Programming Environments. Proceedings of an
                 International Workshop",
 address      = "Trondheim, Norway",
 month        = "16--18 " # jun,
 year         = "1986",
 pages        = "314--43",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
This paper addresses the problem of building tools for integrated programming
environments. Integrated environments have the desirable property that the
tools in it can share a database of common structures. But they have the
undesirable property that these tools are hard to build because typically a
single representation of the database must serve all tools. The solution
proposed in this work allows tools to maintain appropriate representations or
'views' of the objects they manipulate while retaining the benefits of shared
access to common structures. The author illustrates the approach with two
examples of tools for an environment for programming-in-the-large and outlines
current work in progress on efficient implementations of these ideas. 
",
 note         = "",
}

------------------------------------------------------------------------------

317

@InProceedings{deline99a,
 key          = "deline99a",
 author       = "Robert DeLine",
 title        = "Avoiding Packaging Mismatch with {F}lexible {P}ackaging",
 booktitle    = "Proceedings of the ~21st~ International Conference on
                 Software Engineering",
 address      = "Los Angeles, California",
 month        = "16--22 " # may,
 year         = "1999",
 pages        = "97--106",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
To integrate a software component into a system, it must interact properly
with the system's other components. Unfortunately, the decisions about how a
component is to interact with other components are typically committed long
before the moment of integration and are difficult to change. This paper
introduces the Flexible Packaging method, which allows a component developer
to defer some decisions about component interaction until system integration
time.  The method divides the component's source into two pieces: the ware,
which encapsulates the component's functionality; and the packager, which
encapsulates the details of interaction.  Both the ware and the packager are
independently reusable.  A ware, as a reusable part, allows a given piece of
functionality to be employed in systems in different architectural styles.  A
packager, as a reusable part, encapsulates conformance to a component
standard, like an ActiveX control or an ODBC database accessor.  Because the
packager's source code is often formulaic, a tool is provided to generate the
packager's source from a high-level description of the intended interaction, a
description written in the architectural description language UniCon.  The
method and tools are evaluated with two case studies, an image viewer and a
database updater.
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{ousterhout94a,
 key       = "ousterhout94a",
 author    = "J. K. Ousterhout",
 title     = "Tcl and the Tk Toolkit",
 publisher = "Addison Wesley",
 edition   = "4",
 month     = "",
 year      = "1994",
 volume    = "",
 series    = "",
 address   = "Reading Massachusetts",
 note      = "",
}

------------------------------------------------------------------------------

318

@Article{brockschmidt96b,
 key     = "brockschmidt96b",
 author  = "Kraig Brockschmidt",
 title   = "How {COM} Solves the Problems of Component Software Design, Part
            {II}",
 journal = "Microsoft Systems Journal",
 month   = jun,
 year    = "1996",
 volume  = "11",
 number  = "6",
 pages   = "",
 abstract = "
The Component Object Model (COM) and OLE solve many problems in component
software development. Last month, I examined the fundamentals of the COM
architecture, concentrating on versioning. Let's pick up where I left off with
QueryInterface and the IUnknown member function. QueryInterface makes multiple
interfaces possible; once a client obtains the initial interface pointer to
any object, it obtains other interface pointers to the same object through
QueryInterface.
",
 note    = "",
}

------------------------------------------------------------------------------

319

@Article{bates95a,
 key     = "bates95a",
 author  = "Peter C. Bates",
 title   = "Debugging Heterogeneous Distributed Systems Using Event-Based
            Models of Behavior",
 journal = "ACM Transactions on Computer Systems",
 month   = feb,
 year    = "1995",
 volume  = "13",
 number  = "1",
 pages   = "1--31",
 abstract= "
We describe a high-level debugging approach, Event-Based Behavior Abstraction
(EBBA), in which debugging is treated as a process of creating models of
expected program behaviors and comparing these to the actual behaviors
exhibited by the program. The use of EBBA techniques can enhance
debugging-tool transparency, reduce latency and uncertainty for fundamental
debugging activities, and accommodate diverse, heterogeneous architectures.
Using events and behavior models as a basic mechanism provides a uniform view
of heterogeneous systems and enables analysis to be performed in well-defined
ways. Their use also enables EBBA users to extend and reuse knowledge gained
in solving previous problems to new situations. We describe our
behavior-modeling algorithm that matches actual behavior to models and
automates many behavior analysis steps. The algorithm matches behavior in as
many ways as possible and resolves these to return the best match to the user.
It deals readily with partial behavior matches and incomplete information. In
particular, we describe a tool set we have built. The tool set has been used
to investigate the behavior of a wide range of programs. The tools are modular
and can be distributed readily throughout a system. 
",
 note    = "",
}

------------------------------------------------------------------------------

320

@InProceedings{tombros97a,
 key          = "tombros97a",
 author       = "Dimitrios Tombros and Andreas Geppert and Klaus R. Dittrich",
 title        = "Semantics of Reactive Components in Event-Driven Workflow
                 Execution",
 booktitle    = "Proceedings of 9th Conference on Advanced Information Systems
                 Engineering",
 address      = "Barcelona, Spain",
 month        = "16--20 " # jun,
 year         = "1997",
 pages        = "409--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The exact semantics of workflows and the involved processing entities is an
open yet urgent problem. The paper considers the semantics and correctness of
event-driven workflow execution. The basis for the formalization in the
approach is provided by an event history which records all events that have
occurred during the execution of workflows. Workflows are executed by reactive
components which operate on top of that history. Based on the history it is
possible to determine the semantics of these reactive components (and
consequently, the semantics of workflows) as well as to check whether their
observable behavior is correct. 
",
 note         = "",
}

------------------------------------------------------------------------------

321

@Article{luckham95a,
 key     = "luckham95a",
 author  = "David C. Luckham and John J. Kennedy and Larry M. Augustin and
            James Vera and Doug Bryan and Walter Mann",
 title   = "Specification and Analysis of System Architecture Using {Rapide}",
 journal = j-tse,
 month   = apr,
 year    = "1995",
 volume  = "21",
 number  = "4",
 pages   = "336--54",
 abstract = "
Rapide is an event-based, concurrent, object-oriented language specifically
designed for prototyping system architectures. Two principle design goals are:
(1) to provide constructs for defining executable prototypes of architectures
and (2) to adopt an execution model in which the concurrency, synchronization,
dataflow, and timing properties of a prototype are explicitly represented.
This paper describes the partially ordered event set (poset) execution model
and outlines with examples some of the event-based features for defining
communication architectures and relationships between architectures. Various
features of Rapide are illustrated by excerpts from a prototype of the X/Open
distributed transaction processing reference architecture. 
",
 note    = "",
}

------------------------------------------------------------------------------

322

@InProceedings{cook94a,
 key          = "cook94a",
 author       = "Jonathan E. Cook and Alexander L. Wolf",
 title        = "Toward metrics for Process Validation",
 booktitle    = "Proceedings of the Third International Conference on the
                 Software Process",
 address      = "Reston, Virginia",
 month        = "10--11 " # oct,
 year         = "1994",
 pages        = "33--44",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
To a great extent, the usefulness of a formal model of a software process lies
in its ability to accurately predict the behavior of the executing process.
Similarly, the usefulness of an executing process lies largely in its ability
to fulfil the requirements embodied in a formal model of the process. When
process models and process executions diverge, something significant is
happening. We are developing techniques for uncovering discrepancies between
models and executions under the rubric of process validation. Further, we are
developing metrics for process validation that give engineers a feel for the
severity of the discrepancy. We view the metrics presented here as a first
step toward a suite of useful metrics for process validation.  
",
 note         = "",
}

------------------------------------------------------------------------------

323

@Article{luckham95b,
 key     = "luckham95b",
 author  = "David C. Luckham and James Vera",
 title   = "An Event-Based Architecture Definition Language",
 journal = j-tse,
 month   = sep,
 year    = "1995",
 volume  = "21",
 number  = "9",
 pages   = "717--34",
 abstract = "
This paper discusses general requirements for architecture definition
languages, and describes the syntax and semantics of the subset of the Rapide
language that is designed to satisfy these requirements. Rapide is a
concurrent event-based simulation language for defining and simulating the
behavior of system architectures. Rapide is intended for modelling the
architectures of concurrent and distributed systems, both hardware and
software in order to represent the behavior of distributed systems in as much
detail as possible. Rapide is designed to make the greatest possible use of
event-based modelling by producing causal event simulations. When a Rapide
model is executed it produces a simulation that shows not only the events that
make up the model's behavior, and their timestamps, but also which events
caused other events, and which events happened independently.\par

The architecture definition features of Rapide are described: event patterns,
interfaces, architectures and event pattern mappings. The use of these
features to build causal event models of both static and dynamic architectures
is illustrated by a series of simple examples from both software and hardware.
Also we give a detailed example of the use of event pattern mappings to define
the relationship between two architectures at different levels of abstraction.
Finally, we discuss briefly how Rapide is related to other event-based
languages. 
",
 note    = "",
}

------------------------------------------------------------------------------

324

@InProceedings{magee97b,
 key          = "magee97b",
 author       = "Jeff Magee and Jeff Kramer and Dimitra Giannakopoulou",
 title        = "Analysing the Behaviour of Distributed Software Architectures:
                 A Case Study",
 booktitle    = "Proceedings of the Sixth IEEE Computer Society Workshop on
                 Future Trends of Distributed Computing Systems",
 address      = "Tunis, Tunisia",
 month        = "29--31 " # oct,
 year         = "1997",
 pages        = "240--5",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A software architecture is the overall structure of a system in terms of its
constituent components and their interconnections. We describe work to
associate behavioural specifications with the components of a distributed
software architecture and an approach to analysing the behaviour of systems
composed from these components. The approach is based on the use of labelled
transition systems to specify behaviour and compositional reachability
analysis to check composite system models. The architecture description of a
system is used directly to generate the model used for analysis. Analysis
allows a designer to check whether an architecture satisfies the properties
required of it. The approach is illustrated using a case study of an active
badge system. 
",
 note         = "",
}

------------------------------------------------------------------------------

325

@InProceedings{magee97c,
 key          = "magee97c",
 author       = "Jeff Kramer and Jeff Magee",
 title        = "Analysing dynamic change in software architectures: a case
                 study",
 booktitle    = "Proceedings. Fourth International Conference on Configurable
                 Distributed Systems",
 address      = "Annapolis, MA",
 month        = "4--6 " # may,
 year         = "1998",
 pages        = "91--100",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The software architecture of a system is the overall structure of the system
in terms of its constituent components and their interconnections. Dynamic
changes to the instantiated system architecture-to the components and/or
interconnections-may take place while it is running. In order that these
changes do not violate the integrity of the system, we adopt a general model
of dynamic configuration which only permits change to occur when the affected
portions of the system are quiescent. In this paper we investigate the ability
to perform behaviour analysis on systems which conform to the change model.
Our analysis approach associates behavioural specifications with the
components of a software architecture and analyses the behaviour of systems
composed from these components. We use Labelled Transition Systems to specify
behaviour and Compositional Reachability Analysis to check composite system
models. We model the changes that can occur and use analysis to check that the
architecture satisfies the properties required of it: before, during and after
the change. The paper uses an example to illustrate the approach and discusses
some issues arising from the work. 
",
 note         = "",
}

------------------------------------------------------------------------------

326

@InProceedings{magee97d,
 key          = "magee97d",
 author       = "Jeff Magee and Andrew Tseng and Jeff Kramer",
 title        = "Composing distributed objects in {CORBA}",
 booktitle    = "Proceedings of the Third International Symposium on
                 Autonomous Decentralized Systems",
 address      = "Berlin, Germany",
 month        = "9--11 " # apr,
 year         = "1997",
 pages        = "257--63",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The paper addresses the problem of structuring and managing large distributed
systems constructed from many distributed objects. Specifically, the paper
proposes a component model which can be used to compose objects into
manageable entities. Components are specified using Darwin, an architecture
description language developed by the authors. A mapping of distributed
objects into Darwin components is described together with an outline of how
Darwin and its associated tools are implemented in a CORBA compliant
environment. 
",
 note         = "",
}

------------------------------------------------------------------------------

327

@InProceedings{magee95a,
 key          = "magee95a",
 author       = "Jeff Magee and Naranker Dulay and Susan Eisenbach and Jeff
                 Kramer",
 title        = "Specifying Distributed Software Architectures",
 booktitle    = "Proceedings of ESEC `95 - 5th European Software Engineering
                 Conference",
 address      = "Sitges, Spain",
 month        = "25--28 " # sep,
 year         = "1995",
 pages        = "137--53",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
There is a real need for clear and sound design specifications of distributed
systems at the architectural level. This is the level of the design which
deals with the high-level organisation of computational elements and the
interactions between those elements. The paper presents the Darwin notation
for specifying this high-level organisation. Darwin is in essence a
declarative binding language which can be used to define hierarchic
compositions of interconnected components. Distribution is dealt with
orthogonally to system structuring. The language supports the specification of
both static structures and dynamic structures which may evolve during
execution. The central abstractions managed by Darwin are components and
services. Services are the means by which components interact. \par

In addition to its use in specifying the architecture of a distributed system,
Darwin has an operational semantics for the elaboration of specifications such
that they may be used at runtime to direct the construction of the desired
system. The paper describes the operational semantics of Darwin in terms of
the pi -calculus, Milner`s calculus of mobile processes. The correspondence
between the treatment of names in the pi -calculus and the management of
services in Darwin leads to an elegant and concise pi -calculus model of
Darwin`s operational semantics. The model is used to argue the correctness of
the Darwin elaboration process. The overall objective is to provide a soundly
based notation for specifying and constructing distributed software
architectures.
",
 note         = "",
}

------------------------------------------------------------------------------

328

@InProceedings{broy97a,
 key          = "broy97a",
 author       = "Manfred Broy and Christoph Hofmann and Ingolf {Kr\"uger} and
                 Monika Schmidt",
 title        = "Using Extended Event Traces to Describe Communication in
                 Software Architectures",
 booktitle    = "Proceedings of Joint 4th International Computer Science
                 Conference and 4th Asia Pacific Software Engineering
                 Conference",
 address      = "Hong Kong",
 month        = "2--5 " # dec,
 year         = "1997",
 pages        = "203--12",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A crucial aspect of the architecture of a software system is its decomposition
into components and the specification of component interactions. In this
report we use a variant of extended event traces as a graphical technique for
the description of such component interactions. It allows us to define
interaction patterns that occur frequently within an architecture, in the form
of diagrams. The diagrams may be instantiated in various contexts, thus
allowing reuse of interaction patterns. Our notation contains operators
yielding not only exemplary but complete behavior specifications. Extended
event traces have a clear semantics that is based on sets of traces. We
present several application examples that demonstrate the practical use of our
notation.
",
 note         = "",
}

------------------------------------------------------------------------------

329

@Article{barrett96b,
 key     = "barrett96b",
 author  = "Daniel J. Barrett and Lori A. Clarke and Peri L. Tarr and
            Alexander E. Wise",
 title   = "A Framework for Event-Based Software Integration",
 journal = j-tsem,
 month   = oct,
 year    = "1996",
 volume  = "5",
 number  = "4",
 pages   = "378--421",
 abstract = "
Although event-based software integration is one of the most prevalent
approaches to loose integration, no consistent model for describing it exists.
As a result, there is no uniform way to discuss event-based integration,
compare approaches and implementations, specify new event-based approaches, or
match user requirements with the capabilities of event-based integration
systems. The authors attempt to address these shortcomings by specifying a
generic framework for event-based integration, the EBI framework, that
provides a flexible, object-oriented model for discussing and comparing
event-based integration approaches. The EBI framework can model dynamic and
static specification, composition, and decomposition and can be instantiated
to describe the features of most common event-based integration approaches.
They demonstrate how to use the framework as a reference model by comparing
and contrasting three well-known integration systems: FIELD, Polylith, and
CORBA.
",
 note    = "",
}

------------------------------------------------------------------------------

330

@Article{marzullo91a,
 key     = "marzullo91a",
 author  = "Keith Marzullo and Robert Cooper and Mark D. Wood and Kenneth P.
            Birman",
 title   = "Tools for Distributed Application Management",
 journal = j-comp,
 month   = aug,
 year    = "1991",
 volume  = "24",
 number  = "8",
 pages   = "42--51",
 abstract = "
The issues of managing distributed applications are discussed, and a set of
tools, the meta system, that solves some longstanding problems is presented.
The Meta model of a distributed application is described. To make the
discussion concrete, it is shown how NuMon, a seismological analysis system
for monitoring compliance with nuclear test-ban treaties is managed within the
Meta framework. The three steps entailed in using Meta are described. First
the programmer instruments the application and its environment with sensors
and actuators. The programmer then describes the application structure using
the object-oriented data modeling facilities of the authors' high-level
control language, Lomita. Finally, the programmer writes a control program
referencing the data model. Meta's performance and real-time behavior are
examined.
",
 note    = "",
}

------------------------------------------------------------------------------

331

@TechReport{birman85a,
 key         = "birman85a",
 author      = "Kenneth P. Birman",
 title       = "Replication and Fault-Tolerance in the {ISIS} System",
 institution = "Department of Computer Science, Cornell University",
 month       = mar,
 year        = "1995",
 number      = "TR 85-668",
 address     = "Ithaca, New York",
 type        = "",
 abstract    = "
The ISIS system transforms abstract type specifications into fault-tolerant
distributed implementations while insulating users from the mechanisms used to
achieve fault-tolerance. This paper discusses techniques for obtaining a
fault-tolerant implementation from a non-distributed specification and for
achieving improved performance by concurrently updating replicated data. The
system itself is based on a small set of communication primitives, which are
interesting because they achieve high levels of concurrency while respecting
higher level ordering requirements. The performance of distributed
fault-tolerant services running on this initial version of ISIS is found to be
nearly as good as that of non-distributed, fault-intolerant ones. 
",
 note        = "",
}

------------------------------------------------------------------------------

332

@Article{cook98a,
 key     = "cook98a",
 author  = "Jonathan E. Cook and Alexander L. Wolf",
 title   = "Discovering Models of Software Processes From Event-Based Data",
 journal = j-tsem,
 month   = jul,
 year    = "1998",
 volume  = "7",
 number  = "3",
 pages   = "215--49",
 abstract = "
Many software process methods and tools presuppose the existence of a formal
model of a process. Unfortunately, developing a formal model for an on-going,
complex process can be difficult, costly, and error prone. This presents a
practical barrier to the adoption of process technologies, which would be
lowered by automated assistance in creating formal models. To this end, we
have developed a data analysis technique that we term process discovery. Under
this technique, data describing process events are first captured from an
on-going process and then used to generate a formal model of the behavior of
that process. We describe a Markov method that we developed specifically for
process discovery, as well as describe two additional methods that we adopted
from other domains and augmented for our purposes. The three methods range
from the purely algorithmic to the purely statistical. We compare the methods
and discuss their application in an industrial case study.
",
 note    = "",
}

------------------------------------------------------------------------------

333

@Article{compare99a,
 key     = "compare99a",
 author  = "Daniele Compare and Paola Inverardi and Alexander L. Wolf",
 title   = "Uncovering Architectural Mismatch in Component Behavior",
 journal = j-tsem,
 month   = feb,
 year    = "1999",
 volume  = "33",
 number  = "2",
 pages   = "101--31",
 abstract = "
When constructing software systems from existing components, the engineer is
faced with the problem of potential conflicts in the interactions among the
components. Of particular difficulty is guaranteeing compatibility in the
dynamic interaction behavior. Using an architectural description of the system
and its intended components, the engineer can reason about the interactions
early and at a high level of abstraction. In this paper we give a case study
of the compressing proxy system, which was first investigated by Garlan,
Kindred, and Wing. We present architectural specifications and analyses of two
versions of the system. One version is a seemingly obvious melding of the
components. The other is a solution to deadlock problems uncovered by formal
analyses of the first version. We use the Chemical Abstract Machine as an
example of an architectural description formalism that can help uncover
architectural mismatches in the behavior of components.
",
 note    = "",
}

------------------------------------------------------------------------------

334

@InProceedings{allen98b,
 key          = "allen98b",
 author       = "Robert Allen and {R\'emi} Douence and David Garlan",
 title        = "Specifying and Analyzing Dynamic Software Architectures",
 booktitle    = "Fundamental Approaches to Software Engineering",
 address      = "Lisbon, Portugal",
 month        = "28 " # mar # "--" # "4 " # apr,
 year         = "1998",
 pages        = "21--37",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A critical issue for complex component-based systems design is the modeling
and analysis of architecture. One of the complicating factors in developing
architectural models is accounting for systems whose architecture changes
dynamically (during run time). This is because dynamic changes to
architectural structure may interact in subtle ways with on-going computations
of the system. We argue that it is possible and valuable to provide a modeling
approach that accounts for the interactions between architectural
reconfiguration and non-reconfiguration system functionality, while
maintaining a separation of concerns between these two aspects of a system.
The key to the approach is to use a uniform notation and semantic base for
both reconfiguration and steady-state behavior, while at the same time
providing syntactic separation between the two. As we show, this permits us to
view the architecture in terms of a set of possible architectural snapshots,
each with its own steady-state behavior. Transitions between these snapshots
are accounted for by special reconfiguration-triggering events.
",
 note         = "",
}

------------------------------------------------------------------------------

335

@InProceedings{wang97a,
 key          = "wang97a",
 author       = "Yi-Min Wang and Om P. Damani and Woei-Jyh Lee",
 title        = "Reliability and Availability Issues in Distributed Component
                 Object Model ({DCOM})",
 booktitle    = "1997 Fourth International Workshop on Community Networking
                 Processing",
 address      = "Atlanta, GA",
 month        = "11--12 " # sep,
 year         = "1997",
 pages        = "59--63",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Distributed Component Object Model (DCOM) is one of the emerging standards for
distributed objects. Before DCOM can be used to build mission-critical
applications, the reliability and availability issues must be addressed. In
this position paper, we outline the current research directions of the
InterCOM project, which exploits the dynamic behavior, the extensible
architecture, and the component software model of DCOM to provide fault
tolerance capabilities to distributed applications.
",
 note         = "",
}

------------------------------------------------------------------------------

336

@Article{mansouri-samani97a,
 key     = "mansouri-samani97a",
 author  = "Masoud Mansouri-Samani and Morris Sloman",
 title   = "{GEM}: A Generalized Event Monitoring Language for Distributed
            Systems",
 journal = "Distributed Systems Engineering",
 month   = jun,
 year    = "1997",
 volume  = "4",
 number  = "2",
 pages   = "96--108",
 abstract = "
Event-based monitoring is critical for managing and debugging networks and
distributed systems. This paper presents GEM-an interpreted generalized event
monitoring language. It allows high-level, abstract events to be specified in
terms of combinations of lower-level events from different nodes in a loosely
coupled distributed system. Event monitoring components can thus be
distributed within the system to perform filtering, correlation and
notification of events close to where they occur and thus reduce network
traffic. GEM is a declarative rule-based language in which the notion of real
time has been closely integrated and various temporal constraints can be
specified for event composition. The paper discusses the effect of
communication delays on composite event detection and presents a tree-based
solution for dealing with out-of-order event arrivals at event monitors.
",
 note    = "",
}

------------------------------------------------------------------------------

337

@InProceedings{santoro98a,
 key          = "santoro98a",
 author       = "Alexandre Santoro and Walter Mann and Neel Madhav and David
                 Luckham",
 title        = "{eJava}-Extending {Java} With Causality",
 booktitle    = "Proceedings of Tenth International Conference on Software
                 Engineering and Knowledge Engineering",
 address      = "San Francisco, CA",
 month        = "18--20 " # jun,
 year         = "1998",
 pages        = "251--60",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Programming languages like Java provide designers with a variety of classes
that simplify the process of program development. Some of these classes allow
one to easily build multithreaded programs. Though useful, especially in the
creation of reactive systems, multithreaded programs present challenging
problems such as race conditions and synchronization issues. Validating these
programs against a specification is nor trivial since Java does not clearly
indicate thread interaction. These problems can be solved by modifying Java so
that it produces computations, collections of events with both causal and
temporal ordering relations defined for them. Specifically, the causal
ordering is ideal for identifying thread interaction. This paper presents
eJava, an extension to Java that is both event based and causally aware, and
shows how it simplifies the process of understanding and debugging
multithreaded programs.
",
 note         = "",
}

------------------------------------------------------------------------------

338

@InProceedings{chang96a,
 key          = "chang96a",
 author       = "Juei Chang and Debra J. Richardson",
 title        = "Structural Specification-Based Testing with {ADL}",
 booktitle    = "Proceedings of International Symposium on Software Testing
                 and Analysis (ISSTA '96)",
 address      = "San Diego, CA",
 month        = "8--10 " # jan,
 year         = "1996",
 pages        = "62--70",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The paper describes a specification-based black-box technique for testing
program units. The main contribution is the method developed to derive test
conditions, which are descriptions of test cases, from the formal
specification of each program unit. The derived test conditions are used to
guide test selection and to measure comprehensiveness of existing test suites.
Our technique complements traditional code-based techniques such as statement
coverage and branch coverage. It allows the tester to quickly develop a
black-box test suite. In particular, this paper presents techniques for
deriving test conditions from specifications written in the Assertion
Definition Language (ADL), a predicate logic-based language that is used to
describe the relationships between inputs and outputs of a program unit. Our
technique is fully automatable, and we are currently implementing a tool based
on the techniques presented in this paper.
",
 note         = "",
}

------------------------------------------------------------------------------

339

@Article{richardson94a,
 key     = "richardson94a",
 author  = "Debra J. Richardson",
 title   = "{TAOS}: Testing with Analysis and Oracle Support",
 journal = j-sig,
 month   = "",
 year    = "1994",
 volume  = "",
 number  = "",
 pages   = "138--53",
 abstract = "
Few would question that software testing is a necessary activity for assuring
software quality, yet the typical testing process is a human intensive
activity and as such, it is unproductive, error-prone, and often inadequately
done. Moreover, testing is seldom given a prominent place in software
development or maintenance processes, nor is it an integral part of them.
Major productivity and quality enhancements can be achieved by automating the
testing process through tool development and use and effectively incorporating
it with development and maintenance processes. The TAOS toolkit, Testing with
Analysis and Oracle Support, provides support for the testing process. It
includes tools that automate many tasks in the testing process, including
management and persistence of test artifacts and the relationships between
those artifacts, test development, test execution, and test measurement. A
unique aspect of TAOS is its support for test oracles and their use to verify
behavioral correctness of test executions. TAOS also supports
structural/dependence coverage, by measuring the adequacy of test criteria
coverage, and regression testing, by identifying tests associated or dependent
upon modified software artifacts. This is accomplished by integrating the
ProDAG toolset, Program Dependence Analysis Graph, with TAOS, which supports
the use of program dependence analysis in testing, debugging, and maintenance.
The paper describes the TAOS toolkit and its capabilities as well as testing,
debugging and maintenance processes based on program dependence analysis. We
also describe our experience with the toolkit and discuss our future plans.
",
 note    = "",
}

------------------------------------------------------------------------------

340

@Article{biggerstaff98a,
 key     = "biggerstaff98a",
 author  = "Ted J. Biggerstaff",
 title   = "A Perspective of Generative Reuse",
 journal = "Annals of Software Engineering",
 month   = "",
 year    = "1998",
 volume  = "5",
 number  = "",
 pages   = "169--226",
 abstract = "
The paper presents a perspective of generative reuse technologies as they have
evolved over the last 15 years (1983-98) and a discussion of how generative
reuse addresses some key reuse problems. Over that time period, a number of
different reuse strategies have been tried, ranging from pure component reuse
to pure generation. The record of success is mixed and the evidence is
sketchy. Nevertheless, the paper uses some known metric evidence plus
anecdotal evidence, personal experience, and suggestive evidence to define
some of the boundaries of the success envelope. Fundamentally, the paper
argues that the first order term in the success equation of reuse is the
amount of domain-specific content and the second order term is the specific
technology chosen in which to express that content. The overall payoff of any
reuse system correlates well with the amount of content expressed in the
domain specific elements. While not a silver bullet, technology is not without
its contribution and the degree of payoff for any specific technology is
sensitive to many factors. The paper argues that the generative factors
predominate over other technology factors. By looking closely at several
successful generation systems that are exemplars for classes of related
systems, the paper examines how those classes have solved problems associated
with the more convention reuse of concrete components expressed in
conventional programming languages. This analysis distills the key elements of
generative success and provides an opinion of approximately where each class
of generative system fits in the overall picture. The result is a guide to the
generative reuse technologies that appear to work best today. 
",
 note    = "",
}

------------------------------------------------------------------------------

341

@InProceedings{grundy95a,
 key          = "grundy95a",
 author       = "John C. Grundy and John G. Hosking",
 title        = "{ViTABaL}: A Visual Language Supporting Design by Tool
                 Abstraction",
 booktitle    = "Proceedings of Symposium on Visual Languages",
 address      = "Darmstadt, Germany",
 month        = "5--9 " # sep,
 year         = "1995",
 pages        = "53--60",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We describe a visual language and environment for designing and implementing
systems using the tool abstraction paradigm. This paradigm permits systems to
be constructed from toolie and abstract data structure components, using an
event response mechanism to handle inter-component interaction. This approach
leads to systems more easily adapted to functional specification changes than
with conventional design.
",
 note         = "",
}

------------------------------------------------------------------------------

342

@TechReport{garlan88a,
 key         = "garlan88a",
 author      = "David Garlan and Gail Kaiser and David Notkin",
 title       = "On the Criteria To Be Used In Composing Tools Into Systems",
 institution = "University of Washington",
 month       = aug,
 year        = "1988",
 number      = "TR 88-08-09",
 address     = "Seattle, WA",
 type        = "",
 abstract = "
Parnas' well-known notion of data abstraction, which separates the interface
of a module from its implementation, permits the representation and
implementation of data structures to evolve without affecting the module's
clients. However, data abstraction does not directly ease the difficulties and
costs of such enhancements.\par

Tool abstraction complements data abstraction by supporting evolutionary
changes that are made incrementally and independently. This approach
structures a system as a pool of shared, abstract data structures, viewed by a
collection of cooperating tools, each of which provides a piece of the overall
system function. This paper unifies the tool astraction mechanisms on which a
variety of existing systems -- including spreadsheets, production systems, and
integrated programming environments -- are based.
",
 note        = "",
}

------------------------------------------------------------------------------

343

@Article{garlan92b,
 key     = "garlan92b",
 author  = "David Garlan and Gail E. Kaiser and David Notkin",
 title   = "Using Tool Abstraction to Compose Systems",
 journal = j-comp,
 month   = jun,
 year    = "1992",
 volume  = "25",
 number  = "6",
 pages   = "30--8",
 abstract = "
The tool abstraction paradigm, which supports the evolution of large-scale
software systems by easing design changes in the system functions, is
discussed. Systems that support tool abstraction are structured as a pool of
abstract data structures shared by a collection of cooperating 'toolies',
where each toolie provides a piece of the overall system function. When one
toolie updates the shared data, other toolies must be notified: otherwise,
cooperating-but-independent toolies may not execute, and the overall system
function may be compromised. The KWIC (key word in context) index production
system is used to illustrate the idea of tool abstraction. The relationship of
tool abstraction to other concepts is examined.
",
 note    = "",
}

------------------------------------------------------------------------------

344

@InCollection{kaiser89a,
 key       = "kaiser89a",
 author    = "Gail E. Kaiser and David Garlan",
 title     = "Synthesizing Programming Environments from Reusable Features",
 booktitle = "Software Reusability -- Applications and Experience",
 pages     = "35--55",
 publisher = "ACM Press",
 editor    = "Ted J. Biggerstaff and Alan J. Perlis",
 month     = "",
 year      = "1989",
 volume    = "II",
 chapter   = "2",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
",
 note      = "",
}

------------------------------------------------------------------------------

A very good summary of current aspect-oriented technologies

345

@InBook{czarnecki99a,
 key          = "czarnecki99a",
 author       = "Krzysztof Czarnecki and Ulrich Eisenecker",
 title        = "Aspect-Oriented Decomposition and Composition",
 chapter      = "7",
 pages        = "189--250",
 publisher    = "Addison-Wesley",
 year         = "1999",
 volume       = "",
 series       = "",
 address      = "",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 abstract     = "",
 note         = "URL: {\urlBiBTeX{http://nero.prakinf.tu-ilmenau.de/~czarn/aop/}}",
 url          = "http://nero.prakinf.tu-ilmenau.de/~czarn/aop/",
}

------------------------------------------------------------------------------

346

@Misc{czarnecki99b,
 key    = "czarnecki99b",
 author = "Krzysztof Czarnecki and Ulrich Eisenecker and Robert {Gl\"uck} and
           David Vandevoorde and Todd Veldhuizen",
 title  = "Generative Programming and Active Libraries",
 url    = "http://www.extreme.indiana.edu/~tveldhui/papers/dagstuhl/",
 abstract = "
We describe generative programming, an approach to generating customized
programming components, and active libraries which are based on this approach.
Several working examples (Blitz++, GMCL, Xroma) are presented to illustrate
the potential of active libraries. We discuss related implementation
technologies.
",
 note   = "URL: {\urlBiBTeX{http://www.extreme.indiana.edu/~tveldhui/papers/dagstuhl/}}",
}

------------------------------------------------------------------------------

347

@TechReport{luckham96a,
 key         = "luckham96a",
 author      = "David C. Luckham",
 title       = "Rapide: A Language and Toolset for Simulation of Distributed
                Systems by Partial Orderings of Events.",
 institution = "Stanford University, Computer Systems Laboratory",
 month       = sep,
 year        = "1996",
 number      = "CSL-TR-96-705",
 address     = "",
 type        = "",
 abstract = "
This paper describes the Rapide concepts of system architecture, causal event
simulation, and some of the tools for viewing and analysis of causal event
simulations. Illustration of the language and tools isgiven by a detailed
small example.
",
 note        = "",
}

------------------------------------------------------------------------------

348

@TechReport{luckham95c,
 key         = "luckham95c",
 author      = "David C. Luckham and James Vera and Sigurd Meldal",
 title       = "Three Concepts of System Architecture",
 institution = "Stanford University, Computer Systems Laboratory",
 month       = jul,
 year        = "1995",
 number      = "CSL-TR-95-674",
 address     = "",
 type        = "",
 abstract = "
An architecture is a specification of the components of a system and the
communication between them. Systems are constrained to conform to an
architecture. An architecture should guarantee certain behavioral properties
of a conforming system, i.e., one whose components are configured according to
the architecture.  An architecture should also be useful in various ways
during the process of building a system.  This paper presents three
alternative concepts of architecture: object connection architecture,
interface connection architecture, and plug and socket architecture.  We
describe different concepts of interface and connection that are needed for
each of the three kinds of architecture, and different conformance
requirements of each kind.  Simple examples are used to compare the usefulness
of each kind of architecture in guaranteeing properties of conforming systems,
and in correctly modifying a conforming system.  In comparing the three
architecture concepts the principle of communication integrity becomes
central, and two new architecture concepts, duality of sub-interfaces
(services) and connections of dual services (service connection), are
introduced to define plug and socket architecture.  We describe how these
concepts reduce the complexity of architecture definitions, and can in many
cases help guarantee that the components of a conforming system communicate
correctly.  The paper is presented independently of any particular formalism,
since the concepts can be represented in widely differing architecture
definition formalisms, varying from graphical languages to event-based
simulation languages.
",
 note        = "",
}

------------------------------------------------------------------------------

349

@InProceedings{richardson96a,
 key          = "richardson96a",
 author       = "Debra J. Richardson and Alexander L. Wolf",
 title        = "Software Testing at the Architectural Level",
 booktitle    = "Proceedings of the Second International Software Architecture
                 Workshop",
 address      = "San Francisco, California",
 month        = "14--15 " # oct,
 year         = "1996",
 pages        = "68--71",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
This paper argues that with the advent of explicitly specified software
architectures, testing can be done effectively at the architectural level. A
software architecture specification provides a solid foundation for developing
a plan for testing at this level. We propose several architecture-based test
criteria based on the Chemical Abstract Machine model of software
architecture. An architectural (integration) test plan, developed by applying
selected of these criteria, can be used to assess the architecture itself or
to test the implementation's conformance with the architecture. This
facilitates detecting defects earlier in the software lifecycle, enables
leveraging software testing costs across multiple systems developed from the
same architecture, and also leverages the effort put into developing a
software architecture.  ware architectures, architecture-based test criteria
can be defined, which would enable automatically defining data to cover a
system's architectural aspects and also deriving architectural test plans.
Thus, the anticipated, widespread use of formal architecture specification
wilI greatly facilitate testing dynamic component interaction and afford
detecting architectural defects early in the development process.  Perry and
Wolf developed a framework for architectural description [12] in which a
software architecture specification consists of elements (processing, data and
connecting elements) and form (relationships among the elements). Based upon
this framework, Inverardi and Wolf developed a model for operationally
describing software architectures [8] based on viewing a software system as
chemicals whose reactions are governed
",
 note         = "",
}

------------------------------------------------------------------------------

350

@InProceedings{carzaniga98a,
 key          = "carzaniga98a",
 author       = "Antonio Carzaniga and Elisabetta Di Nitto and
                 David S. Rosenblum and Alexander L. Wolf",
 title        = "Issues in Supporting Event-based Architectural Styles",
 booktitle    = "Proceedings Third International Software Architecture
                 Workshop",
 address      = "Orlando, Florida",
 month        = nov,
 year         = "1998",
 pages        = "17--20",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The development of complex software systems is demanding well established
approaches that guarantee robustness of products, economy of the development
process, and rapid time to market. This need is becoming more and more
relevant as the requirements of customers and the potential of computer
telecommunication networks grow. To address this issue, researchers in the
field of software architecture are defining a number of languages and tools
that support the definition and validation of the architecture of systems.
Also, a number of architectural styles are being formalized. A style that is
very prevalent for large-scale distributed applications is the event-based
style. In an event-based style, components communicate by generating and
receiving event notifications. In this paper we identify the event-based style
variations introduced by a number of event-based middleware infrastructures
and point out the advantages and drawbacks of the different approaches as well
as the open issues.
",
 note         = "",
}

------------------------------------------------------------------------------

351

@TechReport{inverardi98a,
 key         = "inverardi98a",
 author      = "Paola Inverardi and Alexander L. Wolf and Daniel Yankelevich",
 title       = "Behavioral Type Checking of Architectural Components Based on
                Assumptions",
 institution = "Department of Computer Science, University of Colorado",
 month       = apr,
 year        = "1998",
 number      = "CU-CS-861-98",
 address     = "Boulder, Colorado",
 type        = "",
 abstract = "
A critical challenge faced by the developer of a software system is to
understand whether the system's components correctly integrate. While type
theory has provided substantial help in detecting and preventing errors in
mismatched static properties, much work remains in the area of dynamics. In
particular, components make assumptions about their behavioral interaction
with other components, but currently we have only limited ways in which to
state those assumptions and to analyze those assumptions for correctness.\par 

We have begun to formulate a method that addresses this problem. The method
operates at the architectural level so that behavioral integration errors,
such as deadlock, can be revealed early in development. For each component, a
specification is given both of its own interaction behavior and of the
assumptions that it makes about the interaction behavior of the external
context in which it expects to operate. We have defined an algorithm that,
given such specifications for a set of components, performs ``adequacy'' checks
between the component context assumptions and the component interaction
behaviors. A configuration of a system is possible if and only if a successful
way of ``matching'' actual behaviors with assumptions can be found. In effect,
we are extending the usual notion of type checking to include the checking of
behavioral compatibility. 
",
 note        = "",
}

------------------------------------------------------------------------------

352

@InProceedings{balzer96a,
 key          = "balzer96a",
 author       = "Robert Balzer",
 title        = "Enforcing Architecture Constraints",
 booktitle    = "Proceedings of the Second International Software Architecture
                 Workshop",
 address      = "San Francisco, California",
 month        = "14--15 " # oct,
 year         = "1996",
 pages        = "80--82",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We have built an infrastructure which reifies the architectural connections
between modules into a data structure that constantly reflects tbe dynamic
structural architecture of a software system. It similarly makes available all
communications passing through those connectors.\par

Since these connections and the communications passing through them constitute
the entire behavior of an architecture, our infrastructure provides tbe basis
for a full range of architecture-level monitoring, testing, debugging, and
enforcement tools.\par

Tbis paper concentrates on the latter by presenting a language for stating
architectural constraints which can be checked against the data structure that
dynamically reflects the software system's structural architecture.
",
 note         = "",
}

------------------------------------------------------------------------------

353

@Misc{balzer99a,
 key    = "balzer99a",
 author = "Robert Balzer",
 title  = "Instrumenting, Monitoring, \& Debugging Software Architectures",
 url    = "http://www.isi.edu/software-sciences/papers/instrumenting-software-architectures.doc",
 abstract = "
Our thesis is very simple: elevating system development from the module to the
architecture level requires a corresponding elevation in our tools for
instrumenting, monitoring, and debugging systems. While we have a long history
and mature technology for the former, we have just begun to recreate these
capabilities at the software architecture level. \par

This paper describes two architecture level tools that utilize architecture
level instrumentation to monitor software architectures through animation and
to create automated drivers for debugging or exercising subsets of those
architectures. The latter has been used to give ``demonstrations'' of
distributed systems in which only the user interface is run live by driving
that user interface from previously recorded system executions. This paper
also describes two implementations of the instrumented connectors these tools
rely on.
",
 note   = "URL: {\urlBiBTeX{http://www.isi.edu/software-sciences/papers/instrumenting-software-architectures.doc}}",
}

------------------------------------------------------------------------------

354

@TechReport{giannakopoulou95a,
 key         = "giannakopoulou95a",
 author      = "Dimitra Giannakopoulou",
 title       = "The {TRACTA} Approach for Behaviour Analysis of Concurrent
                Systems",
 institution = "Department of Computing, Imperial College of Science,
                Technology and Medicine",
 month       = sep,
 year        = "1995",
 number      = "Doc 95/16",
 address     = "Queens's Gate, London",
 type        = "",
 abstract = "
The need for modularity in the behaviour analysis of concurrent systems has
been answered successfully by making reachability analysis compositional.
Compositional reachability analysis (CRA) on the other hand, often exacerbates
the state explosion problem; subsystem analysis leaves out information from
the subsystem environment (context), which could considerably reduce the
number of states allowed into its behaviour state-graph.  To deal with that,
we have chosen to incorporate context constraints in CRA.  In the Tracta
approach developed in our section, context constraints are expressed as
processes in our model (we call them interface processes), that are composed
with the subsystem, without affecting the global system behaviour. Tracta
supports both automatically generated and user-specified interfaces. It also
provides an elegant way of checking violation of safety properties by the
system under analysis. This work, besides introducing the main open problems
in this area of research, is a detailed presentation of Tracta and its
underlying theory, in their current form. 
",
 note        = "",
}

------------------------------------------------------------------------------

355

@Misc{sousa99a,
 key    = "sousa99a",
 author = "Joao Pedro Sousa and David Garlan",
 title  = "Formal Modeling of the {Enterprise JavaBeans} Component Integration
           Framework",
 url    = "http://www.cs.cmu.edu/afs/cs/project/able/www/paper_abstracts/ejb-fm99.html",
 abstract = "
An emerging trend in the engineering of complex systems is the use of
component integration frameworks. Such a framework prescribes an architectural
design that permits flexible composition of third-party components into
applications. A good example is Sun Microsystems Enterprise JavaBeansTM (EJB)
framework, which supports object-oriented, distributed, enterprise-level
applications, such as account management systems. One problem with frameworks
like EJB is that they are documented informally, making it difficult to
understand precisely what is provided by the framework, and what is required
to use it. We believe formal specification can help, and in this paper show
how a formal architectural description language can be used to describe and
provide insight into such frameworks.
",
 note   = "URL: {\urlBiBTeX{http://www.cs.cmu.edu/afs/cs/project/able/www/paper_abstracts/ejb-fm99.html}}",
}

------------------------------------------------------------------------------

356

@TechReport{luckham98a,
 key         = "luckham98a",
 author      = "David C. Luckham and Brian Frasca",
 title       = "Complex Event Processing in Distributed Systems",
 institution = "Stanford University",
 month       = mar,
 year        = "1998",
 number      = "CSL-TR-98-754",
 address     = "",
 type        = "",
 abstract = "
Complex event processing is a new technology for extracting information from
distributed message-based systems. This technology allows users of a system to
specify the information that is of interest to them. It can be low level
network processing data or high level enterprise management intelligence,
depending upon the role and viewpoint of individual users. And it can be
changed from moment to moment while the target system is in operation. This
paper presents an overview of Complex Event Processing applied to a particular
example of a distributed message-based system, a fabrication process
management system. The concepts of causal event histories, event patterns,
event filtering, and event aggregation are introduced and their application to
the process management system is illustrated by simple examples. This paper
gives the reader an overview of Complex Event Processing concepts and
illustrates how they can be applied using the Rapide toolset to one specific
kind of system.
",
 note        = "",
}

------------------------------------------------------------------------------

357

@InProceedings{vera99a,
 key          = "vera99a",
 author       = "James Vera and Louis Perrochon and David C. Luckham",
 title        = "Event-Based Execution Architectures for Dynamic Software
                 Systems",
 booktitle    = "Proceedings of the First Working IFIP Conf. on Software
                 Architecture",
 address      = "San Antonio, Texas",
 month        = "22--24 " # feb,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Distributed systems' runtime behavior can be difficult to understand.
Concurrent, distributed activity make notions of global state difficult to
grasp. We focus on the runtime structure of a system, its execution
architecture, and propose representing its evolution as a partially ordered
set of predefined architectural event types. This representation allows a
system's topology to be visualized, analyzed and con-strained. The use of a
predefined event types allows the execution architectures of different systems
to be readily compared.
",
 note         = "",
}

------------------------------------------------------------------------------

358

@Article{brockschmidt96c,
 key     = "brockschmidt96c",
 author  = "Kraig Brockschmidt",
 title   = "How {COM} Solves the Problems of Component Software Design",
 journal = "Microsoft Systems Journal",
 month   = may,
 year    = "1996",
 volume  = "11",
 number  = "5",
 pages   = "",
 abstract = "
Microsoft continues to make heavy investments in OLE-related technologies. OLE
itself has been in development for over seven years, and essentially every new
technology coming out of Microsoft somehow incorporates elements of OLE. Why
does OLE deserve such an investment, and why is OLE significant for the
independent software vendor (ISV) and the computer industry as a whole? The
answer is that Microsoft created OLE to provide an object-oriented solution
for practical problems encountered in developing operating systems and
applications. OLE provides the necessary specifications and the key services
that make it possible to create component software that ultimately benefits
the entire computing industry.
",
 note    = "",
}

------------------------------------------------------------------------------

359

URL: http://www.cs.wustl.edu/~schmidt/reuse-lessons.html

@Article{schmidt99a,
 key     = "schmidt99a",
 author  = "Douglas C. Schmidt",
 title   = "Why Software Reuse has Failed and How to Make It Work for You",
 journal = "C++ Report",
 month   = jan,
 year    = "1999",
 volume  = "",
 number  = "",
 pages   = "",
 abstract = "
Developing software that achieves these qualities is hard; systematically
developing high quality reusable software components and frameworks is even
harder [GoF:95]. Reusable components and frameworks are inherently abstract,
which makes it hard to engineer their quality and to manage their production.
Moreover, the skills required to develop, deploy, and support reusable
software have traditionally been a ``black art,'' locked in the heads of
expert developers [PLoP:95]. When these technical impediments to reuse are
combined with common non-technical organizational, economical, administrative,
political, and psychological impediments, achieving significant levels of
software reuse throughout an organization becomes decidedly non-trivial. 
",
 note    = "",
}

------------------------------------------------------------------------------

360

@Article{nierstrasz92a,
 key     = "nierstrasz92a",
 author  = "Oscar Nierstrasz and Simon Gibbs and Dennis Tsichritzis",
 title   = "Component-Oriented Software Development",
 journal = j-cacm,
 month   = sep,
 year    = "1992",
 volume  = "35",
 number  = "9",
 pages   = "160--5",
 abstract = "
Object-oriented programming techniques promote a new approach to software
engineering in which reliable, open applications can be largely constructed,
rather than programmed, by reusing ``frameworks'' of plug-compatible software
components.  We outline a series of ongoing research projects at the
University of Geneva that address component-oriented software development at
the levels of languages, tools and frameworks, in particular, (1) the
integration of object-oriented language features that support software
composition with features concerned with other issues, like obc, (2)
application development tools to support composition and reuse, and (3) the
development of reusable application frameworks, specifically in the domain of
multimedia applications.
",
 note    = "",
}

------------------------------------------------------------------------------

361
Not published yet

@Misc{sitaraman98a,
 key    = "sitaraman98a",
 author = "Murali Sitaraman and Steven Atkinson and Gregory Kulczycki and
           Bruce W. Weide and Timothy J. Long and Paolo Bucci and Scott Pike
           and Wayne Heym and Joseph E. Hollingsworth",
 title  = "Reasoning About Software-Component Behavior",
 url    = "http://www.cis.ohio-state.edu/~weide/rsrg/RSRG-instruction.html",
 abstract = "
Well-designed software components can help software engineers understand and
reason soundly about the execution-time behavior of component-based software
systems. A helpful technique for this reasoning process -- one that becomes
technically inevitable in the absence of source code for components -- involves
the use of mathematical modeling to help give clear and unambiguous
specifications of software-component behavior. Model-based specifications
together with a fundamentally sound software design and composition discipline
make it possible to reason about overall system behavior by composing, in a
``modular'' way, predictions about the behavior of individual software
components. Contrary to popular belief, both the modeling and the associated
reasoning process can be made understandable, accessible, and (with tool
support) easy to use for practicing software engineers.
",
 note   = "URL: {\urlBiBTeX{http://www.cis.ohio-state.edu/~weide/rsrg/RSRG-instruction.html}}",
}

------------------------------------------------------------------------------

362

@Article{taylor95a,
 key     = "taylor95a",
 author  = "Richard N. Taylor and Nenad Medvidovic and Kenneth M. Anderson and
            E. James {Whitehead Jr.} and Jason E. Robbins",
 title   = "A Component and Message-Based Architectural Style for {GUI}
            Software",
 journal = j-comp,
 month   = apr,
 year    = "1995",
 volume  = "22",
 number  = "6",
 pages   = "295--304",
 abstract = "
While a large fraction of application system code is devoted to user interface
(UI) functions, support for reuse in this domain has largely been confined to
creation of UI toolkits (``widgets''). We present a novel architectural style
directed at supporting larger grain reuse and flexible system composition.
Moreover, the style supports design of distributed, concurrent, applications.
A key aspect of the style is that components are not built with any
dependencies on what typically would be considered lower-level components,
such as user interface toolkits.  Indeed, all components are oblivious to the
existence of any components to which notification messages are sent.
Asynchronous notification messages and asynchronous request messages are the
sole basis for inter-component communication.  While our focus has been on
applications involving graphical user interfaces, the style has the potential
for broader applicability.  Several trial applications using the style are
described.
",
 note    = "",
}

------------------------------------------------------------------------------

363

@TechReport{jazayeri95a,
 key         = "jazayeri95a",
 author      = "Mehdi Jazayeri",
 title       = "Component Programming --- A Fresh Look at Software Components",
 institution = "Technical University of Vienna",
 month       = feb,
 year        = "1995",
 number      = "TUV-1841-95-01",
 address     = "",
 type        = "",
 abstract = "
All engineering disciplines rely on standard components to design and build
artifacts. The key technical challenge in software engineering is to enable
the adoption of such a model to the development of software. The
transformation from line-by-line development to component-based development
will address many of the industry's productivity and quality problems. Indeed,
component-based software development has been a long- standing dream of the
software industry, prompting a search for both technical and nontechnical
solutions. A successful approach to component- based development requires a
comprehensive solution that draws on advances in programming languages,
programming paradigms, algorithm analysis, and software design. The technical
problem can only be addressed by such an integrated solution. This paper
presents an approach based on the C++ Standard Template Library. More than a
traditional library, STL embodies a concrete approach to software design based
on a well-defined taxonomy and theory of software components. I present the
fundamental contributions of STL to a paradigm of component programming-a
component-based software development paradigm in which there is a clear
separation between component development and application development. I
motivate component programming, give the requirements for components and
catalogs, and give an example of component programming applied to the standard
Keyword in Context (KWIC) problem. I then summarize the implications of
component programming for the software industry and for software engineering
education. 
",
 note        = "",
}

------------------------------------------------------------------------------

364

@InProceedings{franz94a,
 key          = "franz94a",
 author       = "Michael Franz",
 title        = "Technological Steps Toward a Software Component Industry",
 booktitle    = "Programming Languages and System Architectures: Proceedings
                 of the International Conference",
 address      = "Zurich, Switzerland",
 month        = mar,
 year         = "1994",
 pages        = "259--81",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
A machine-independent abstract program representation is presented that is
twice as compact as machine code for a CISC processor. It forms the basis of
an implementation, in which the process of code generation is deferred until
the time of of loading. Separate compilation of program modules with type-safe
interfaces, and dynamic loading (with code generation) on a per-module basis
are both supported.\par

To users of the implemented system, working with modules in the abstract
representation is as convenient as working with native object-files, although
it leads to several new capabilities. The combination of portability with
practicality denotes a step toward a software component industry.
",
 note         = "",
}

------------------------------------------------------------------------------

365
Also at http://www.bell-labs.com/~emerald/dcom-corba/Paper.html

@Misc{chung97a,
 key    = "chung97a",
 author = "Emerald Chung and Yennun Huang and Shalini Yajnik and Deron Liang
           and Joanne C. Shih and Chung-Yih Wang and Yi-Min Wang",
 title  = "{DCOM} and {CORBA} Side by Side, Step by Step and Layer by Layer",
 url    = "http://www.cs.wustl.edu/~schmidt/submit/Paper.html",
 abstract = "
DCOM (Distributed Component Object Model) and CORBA (Common Object Request
Broker Architecture) are two popular distributed object models. In this paper,
we make architectural comparison of DCOM and CORBA at three different layers:
basic programming architecture, remoting architecture, and the wire protocol
architecture. A step-by-step description of remote object activation and
method invocation is provided to demonstrate the similarities and differences
of the two frameworks. A primary goal is for people who are already familiar
with one model to quickly understand the basic architecture of the other. 
",
 note   = "URL: {\urlBiBTeX{http://www.cs.wustl.edu/~schmidt/submit/Paper.html}}",
}

------------------------------------------------------------------------------

366

@InProceedings{holzle93b,
 key          = "holzle93b",
 author       = "Urs {H\"olzle}",
 title        = "Integrating Independently-Developed Components in
                 Object-Oriented Languages",
 booktitle    = "Proceedings of the \mbox{ECOOP}~'93 European Conference on
                 Object-oriented Programming",
 address      = "Kaiserslautern, Germany",
 month        = jul,
 year         = "1993",
 pages        = "36--56",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Object-oriented programming promises to increase programmer productivity
through better reuse of existing code. However, reuse is not yet pervasive in
today's object-oriented programs. Why is this so?  We argue that one reason is
that current programming languages and environments assume that components are
perfectly coordinated.  Yet in a world where programs are mostly composed out
of reusable components, these components are not likely to be completely
integrated because the sheer number of components would make global
coordination impractical.  Given that seemingly minor inconsistencies between
individually designed components would exist, we examine how they can lead to
integration problems with current programming language mechanisms.  We discuss
several reuse mechanisms that can adapt a component in place without requiring
access to the component's source code and without needing to re-typecheck it.
",
 note         = "",
}

------------------------------------------------------------------------------

367

@InProceedings{lumpe97a,
 key          = "lumpe97a",
 author       = "Markus Lumpe and Jean-Guy Schneider and Oscar Nierstrasz and
                 Franz Achermann",
 title        = "Towards a Formal Composition Language",
 booktitle    = "Proceedings of ESEC '97 Workshop on Foundations of
                 Component-Based Systems",
 address      = "Zurich",
 month        = sep,
 year         = "1997",
 pages        = "178--87",
 editor       = "Gary T. Leavens and Murali Sitaraman",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
When do we call a software development environment a composition environment?
A composition environment must be built of three parts: i) a reusable
component library, ii) a component framework determining the software
architecture, and iii) an open and flexible composition language.  Most of the
effort in component technology was spent on the first two parts.  Now it is
crucial to address the last part and find an appropriate model to glue
existing components together.  In this work, we investigate existing component
and glue models, define a set of requirements a composition language must
fulfill, and report our first results using a prototype implementation of a
general-purpose composition language based on the Pi-calculus.
",
 note         = "",
 url          = "ftp://ftp.iam.unibe.ch/pub/scg/Papers/focbs97.ps.gz",
}

------------------------------------------------------------------------------

368

@InCollection{nierstrasz95b,
 key       = "nierstrasz95b",
 author    = "Oscar Nierstrasz and Theo Dirk Meijler",
 title     = "Requirements for a Composition Language",
 booktitle = "Object-Based Models and Languages for Concurrent Systems",
 pages     = "147--61",
 publisher = "Springer-Verlag",
 editor    = "O. Nierstrasz P. Ciancarini and A. Yonezawa",
 month     = sep,
 year      = "1995",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
The key requirement for open systems is that they be flexible, or
recomposable. This suggests that they must first of all be composable.
Object-oriented techniques help by allowing applications to be viewed as
compositions of collaborating objects, but are limited in supporting other
kinds of abstractions that may have finer or coarser granularity than objects.
A composition language supports the technical requirements of a
component-oriented development approach by shifting emphasis from programming
and inheritance of classes to specification and composition of components.
Objects are viewed as processes, and components are abstractions over the
object space.  An application is viewed as an explicit composition of software
components.  By making software architectures explicit and manipulable, we
expect to better support application evolution and flexibility.  In this
position paper we will elaborate our requirements and outline a strategy for
the design and implementation of a composition language for the development of
open systems
",
 note      = "",
 url       = "ftp://ftp.iam.unibe.ch/pub/scg/Papers/reqForACompLang.ps.gz",
}

------------------------------------------------------------------------------

369

@InProceedings{ciancarini97a,
 key          = "ciancarini97a",
 author       = "P. Ciancarini and S. Cimato",
 title        = "Specifying Component-Based Software Architectures",
 booktitle    = "Proceedings of the First Workshop on the Foundations of
                 Component-Based Systems",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "60--70",
 editor       = "Gary T. Leavens and Murali Sitaraman",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Large software systems offer to software designers complex problem to solve
in an efficient and quick way. To manage such complexity several techniques
have been developed to make this task easier and to allow the designer to
reuse prior experience. However such techniques and frameworks often lack
formal notations to support formal reasoning about the resulting products. We
extend Larch family languages, by defining Larch interface language for Java
modules and argue that such notation should help to design and implement
software systems built of Java components.
",
 note         = "",
 url          = "http://www.cs.iastate.edu/~leavens/FoCBS/index.html",
}

------------------------------------------------------------------------------

370

@InProceedings{canal97a,
 key          = "canal97a",
 author       = "Carlos Canal and Ernest Pimentel and {Jos\'e} M. Troya",
 title        = "On the Composition and Extension of Software Components",
 booktitle    = "Proceedings of the First Workshop on the Foundations of
                 Component-Based Systems",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "50--59",
 editor       = "Gary T. Leavens and Murali Sitaraman",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Architectural specifications of software systems show them as a collection of
interrelated components, and constitute what has been called the Software
Architecture level of software design. It is at this level where the
description and verification of structural properties of the system are
naturally addressed. Besides, the use of explicit descriptions of the
architecture of software systems enhances system comprehension and promotes
software reuse. Several notations and languages for architectural
specification have been recently proposed.  How ever some important aspects of
composition, extension and reuse have not been properly addressed, and
deserve further research. These include language aspects, such as derivation
of components and architectures using mechanisms of inheritance, polymorphism
and parameterization, and also verification aspects, like analysis of
compatibility among system components. Our approach tries to address some of
these open problems by combining the use of formal methods, particularly
process algebras, with concepts coming from the object-oriented domain.
",
 note         = "",
 url          = "http://www.cs.iastate.edu/~leavens/FoCBS/index.html",
}

------------------------------------------------------------------------------

371

@InProceedings{bespalko97a,
 key          = "bespalko97a",
 author       = "Stephen J. Bespalko and Alexander Sindt",
 title        = "Context Sensitivity and Ambiguity in Component-based System
                 Design",
 booktitle    = "Proceedings of the First Workshop on the Foundations of
                 Component-Based Systems",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "31--38",
 editor       = "Gary T. Leavens and Murali Sitaraman",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Designers of component-based, real-time systems need to guarantee the
correctness of software and its output. Complexity of a system, and thus the
propensity for error, is best characterized by the number of states a
component can encounter. In many cases, large numbers of states arise where
the processing is highly dependent on context. In these cases, states are
often missed, leading to errors. The following are proposals for compactly
specifying system states which allow the factoring of complex components into
a control module and a semantic processing module. Further, the need for
methods that allow for the explicit representation of ambiguity and
uncertainty in the design of components is discussed. Presented herein are
examples of real-world problems which are highly context-sensitive or are
inherently ambiguous.
",
 note         = "",
 url          = "http://www.cs.iastate.edu/~leavens/FoCBS/index.html",
}

------------------------------------------------------------------------------

372

@InProceedings{weck97a,
 key          = "weck97a",
 author       = "Wolfgang Weck",
 title        = "Inheritance Using Contracts and Object Composition",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "105--12",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
Normal class-based code inheritance across component boundaries creates a
dependency between the involved components. To avoid this, a specification of
the inherited class must be part of the respective component's contract and
the inheriting class must be specified with reference to this specification
only. With this, inheritance can be replaced by object composition without
sacrificing the possibility of static analysis, yet being more flexible.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

373

@InProceedings{hondt97a,
 key          = "hondt97a",
 author       = "Koen De Hondt and Carine Lucas and Patrick Steyaert",
 title        = "Reuse Contracts as Component Interface Descriptions",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "43--50",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
Current interface descriptions are poor in describing components, because they
only provide an external view on a component and they do not lay down how
components interact with each other. Suggestions to improve component
interface descriptions at last year's workshop are reconsidered and reuse
contracts are put forward as a solution that goes one step further.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

374

@InProceedings{baggiolini97a,
 key          = "baggiolini97a",
 author       = "Jurgen Harms Vito Baggiolini",
 title        = "Toward Automatic, Run-time Fault Management for
                 Component-Based Applications",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "5--12",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
While components are well tested and intrinsically more reliable than
custom-made software, applications built of components generally lack global
verifica-tion (especially if they are independently extensible) and are
subject to distributed failure scenarios. We discuss a simple framework for
building fault-resilient applications based on a data flow architecture. We
illustrate the characteristics that make this architecture particularly
suitable for automatic fault management and explain the mechanisms we use for
detecting, diagnosing and correcting faults at run-time.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

375

@InProceedings{bosch97a,
 key          = "bosch97a",
 author       = "Jan Bosch",
 title        = "Adapting Object-Oriented Components",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "13--22",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
Several authors have identified that the only feasible way to increase
productivity in software construction is to reuse existing software. To
achieve this, component-based software development is one of the more
promising approaches.  However, traditional research in component-oriented
programming often assumes that components are reused ``as-is''. Practitioners
have found that ``as-is'' reuse seldomly occurs and that reusable components
generally need to be adapted to match the system requirements. Component
adaptation techniques should be transparent, black-box, composable,
configurable, reusable and efficient to use. Existing component object models,
i.e. white-box techniques, such as copy-paste and inheritance, and black-box
approaches, such as aggregation and wrapping, these requirements.  To address
this, this paper proposes superimposition, a novel black-box adaptation
technique that allows one to impose predefined, but configurable types of
adaptation functionality on a reusable component. In addition, three
categories of typical adaptation types are discussed, related to the component
interface, component composition and monitoring.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

376

@InProceedings{lalanda97a,
 key          = "lalanda97a",
 author       = "Philippe Lalanda",
 title        = "A Control Model for the Dynamic Selection and Configuration
                 of Software Components",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "51-8",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
Component-oriented programming, which amortizes development cost on several
systems, is becoming increasingly important in the software-intensive industry
that is facing today both economical and technical challenges. However, there
is currently little guidance for software developers on how to compose
software components in order to produce running applications. We believe that
the development of domain-specific software architectures (DSSA) provides a
way to integrate properly software components developed by different
organizations. A DSSA takes into account the domain of applications under
consideration and provides the computational framework necessary to solve
typical problems of the domain. The purpose of this paper is to present an
architectural approach that permits the development and exploitation of
DSSAs. This approach builds on a model of dynamic control that permits to
select and configure software components both statically and dynamically.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

377

@InProceedings{murer97a,
 key          = "murer97a",
 author       = "Tobias Murer",
 title        = "The Challenge of the Global Software Process",
 booktitle    = "Proceedings of the Second International Workshop on
                 Component-Oriented Programming (WCOP'97)",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "69-76",
 editor       = "Wolfgang Weck and Jan Bosch and Clemens Szyperski",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Turku Centre for Computer Science",
 abstract = "
The emerging technologies such as software components and the Internet
challenge the way software is produced and marketed. The social, technical and
organizational aspects of the software business change significantly compared
to the traditional understanding. Discussions about software components are
often mainly limited to the technical aspects of interoperability. The purpose
of this position paper is to motivate for a broader interdisciplinary
discussion about components including technical aspects, but also
organizational, social and even marketing aspects.  We investigate these
various aspects to develop the concept of a software engineering environment
capable to face the outlined challenge.
",
 note         = "",
 url          = "http://www.tucs.abo.fi/publications/general/G5.html",
}

------------------------------------------------------------------------------

378

@InProceedings{wileden97a,
 key          = "wileden97a",
 author       = "Jack C. Wileden",
 title        = "Toward a Conceptual Foundation for Interoperation",
 booktitle    = "Proceedings of the First Workshop on the Foundations of
                 Component-Based Systems",
 address      = "",
 month        = sep,
 year         = "1997",
 pages        = "246--55",
 editor       = "Gary T. Leavens and Murali Sitaraman",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
There is steadily growing interest in constructing software applications that
incorporate components from a variety of sources. Individuals and
organizations involved in developing, operating or maintaining such
applications increasingly face interoperability problems - situations in which
components that were implemented using different underlying models or
languages must be combined into a single unified application. To aid in
overcoming such problems, a range of interoperability approaches have begun to
be employed. Unfortunately, existing approaches suffer from various
shortcomings. These shortcomings can often be traced to a lack of suitable
foundations for understanding important aspects of interoperability.\par

This position paper advocates the development of some necessary foundations
for interoperability. Such foundations would contribute to the conceptual base
of several areas of computer science, notably including programmming languages
and software engineering. These foundations would also have potential
practical value, since they could provide a basis for developing improved
tools and techniques applicable to interoperability problems facing practicing
software professionals.\par

Our primary focus, in our own research and in this position paper, is on two
crucial aspects of interoperability: name management and typing. Name
management - how computing systems allow names to be established for entities,
permit entities to be accessed using names, and control the availability and
meaning of names at any point in time - and typing are both important issues
that have been addressed in many computer science domains. From the
perspective of interoperability problems, however, previous work on name
management and typing is not sufficient ly general or robust to contribute to
the desired foundations. We therefore see a need for a conceptual foundation
for interoperation in the form of rigorous, formal models for relevant aspects
of name management, typing and possibly additional aspects of
interoperability.
",
 note         = "",
 url          = "http://www.cs.iastate.edu/~leavens/FoCBS/index.html",
}

------------------------------------------------------------------------------

379

@Article{wileden91a,
 key     = "wileden91a",
 author  = "Jack C. Wileden and Alexander L. Wolf and William R. Rosenblatt
            and Peri L. Tarr",
 title   = "Specification-Level Interoperability",
 journal = j-cacm,
 month   = may,
 year    = "1991",
 volume  = "34",
 number  = "5",
 pages   = "72--87",
 abstract = "
The authors present the specification-level interoperability (SLI) approach,
give a general model of support for SLI and describe the prototype realization
of the SLI approach. They begin by discussing a motivating example, namely
interoperability in software environments. They then describe the
representation-level and specification-level approaches and related work in
this area. Next, they present a general model of support for SLI, a
description of the initial prototype, and a report of some actual experiences
with SLI and the prototype. They conclude with a discussion of future
directions for work on SLI. 
",
 note    = "",
}

------------------------------------------------------------------------------

380

@InProceedings{pfister96a,
 key          = "pfister96a",
 author       = "Cuno Pfister and Clemens Szyperski",
 title        = "Why Objects are Not Enough",
 booktitle    = "Proceedings, International Component Users Conference",
 address      = "Munich, Germany",
 month        = jul,
 year         = "1996",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "SIGS",
 abstract     = "
Object-oriented programming hasn't created a viable software component
industry. From a technical perspective, the reason for this failure lies in an
in sufficient consideration of the unique requirements of component software.
Object-oriented programming too often concentrates on individual objects,
instead of whole collections of objects, i.e., components. Component-oriented
programming requires more stringent information hiding, a more dynamic
approach, and better safety properties than object-oriented programming.
",
 note         = "",
 url          = "http://www.fit.qut.edu.au/~szypersk/pub/",
}

------------------------------------------------------------------------------

381

@InProceedings{vanhilst96a,
 key          = "vanhilst96a",
 author       = "Michael {VanHilst} and David Notkin",
 title        = "Decoupling Change From Design",
 booktitle    = "Proceedings of the Fourth ACM SIGSOFT Symposium on the
                 Foundations of Software Engineering",
 month        = "16--18 " # oct,
 year         = "1996",
 pages        = "58--69",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "USA",
 abstract = "
We present a method of decomposing modules into smaller components for the
purpose of supporting change. Although similar to the approach of modularizing
programs described by Parnas, our approach is specific to decomposing modules.
It is not intended to replace traditional high level modularization but rather
to augment it with a second level of modularization where the standard of
information hiding can be relaxed. The goal of the method is to make modules
easier to change by decomposing them around smaller design decisions-ideally
encoding only one design choice per submodule component. In this paper we show
how submodule components can be used to address the issue of change. We also
demonstrate how the ability to address change with submodule components is, to
a large extent, independent of the design level modularization. Moreover, we
show that, at least in some cases, by using submodule components the choice of
high level modularization can itself be changed without having to rewrite
large amounts of code. A method of implementation is presented using
inheritance, parameterization, and static binding in a way that minimizes
implementation dependencies between components. The method supports fine
grained decomposition with flexible composability and almost no runtime
overhead.
",
 note         = "",
}

------------------------------------------------------------------------------

382

@Article{parnas85a,
 key     = "parnas85a",
 author  = "David Lorge Parnas and Paul C. Clements and David M. Weiss",
 title   = "The Modular Structure of Complex Systems",
 journal = j-tse,
 month   = mar,
 year    = "1985",
 volume  = "SE-11",
 number  = "3",
 pages   = "259--66",
 abstract = "
A discussion is presented of the organization of software that is inherently
complex because of the large number of arbitrary details that must be
precisely right for the software to be correct. The authors show how the
software design technique known as information hiding, or abstraction, can be
supplemented by a hierarchically structured document, which is referred to as
a module guide. The guide is intended to allow both designers and maintainers
to identify easily the parts of the software that they must understand,
without reading irrelevant details about other parts of the software. An
extract from a software module guide to illustrate these proposals is
included.
",
 note    = "",
}

------------------------------------------------------------------------------

383

@Misc{short99a,
 key    = "short99a",
 author = "Keith Short",
 title  = "Component Based Development and Object Modeling",
 url    = "http://www.cool.sterling.com/cbd/whitepaper/coverpg.htm",
 abstract = "
This White Paper ... reveals the
rationale behind the strong emphasis Sterling Software has placed on CBD and
object modeling methods. CBD offers the promise of a vastly better way of
developing applications that is cheaper, faster and which naturally fits with
emerging distributed object technology that is itself rapidly becoming
industrial strength. New ways to understand and model business dynamics and
processes will be required to take advantage of CBD. This is the role for a
new generation of object oriented analysis and design techniques, which must
move out of laboratories and experiments, into the hard world of commercial
application delivery.
",
 note   = "URL: {\urlBiBTeX{http://www.cool.sterling.com/cbd/whitepaper/coverpg.htm}}",
}

------------------------------------------------------------------------------

384

@InProceedings{durnin96a,
 key          = "durnin96a",
 author       = "Mary Anne Durnin and Kevin Terry and Rick Sullins",
 title        = "Establishing a Repository for Enterprise Wide Software Reuse",
 booktitle    = "Proceedings Fifth Annual Workshop on Software Reuse Education
                 and Training",
 month        = "29 " # jul # "-- 1 " # aug,
 year         = "1996",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "USA",
 abstract = "
The Information Systems and Services Group of Texas Instruments is
reenginering software development to enable building applications from
reusable components. A key enabler for this vision involved selecting and
provisioning a repository tool that could define, store, browse, and search
information about software related components or assets, as well as retrieve
the files associated with the asset. The project selected a commercial product
called MOREplus which is based on World Wide Web Architecture. Information
about software related assets, metadata, has been organized and catalogued
into the repository. It was realized that for the repository to be a success
it needed to be able to easily locate the software assets a user desires. Key
factors related to this, are the user interface of the tool, the flexibility
of the tool, and the manner in which the assets in the repository are
catagorized . Several interfaces to software configuration management tools
were defined along with the processes to build and certify reusable software
components. This tool provides a good start in establishing a solid
infrastructure for enterprise wide software reuse.
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

385

@InProceedings{lampson99a,
 key          = "lampson99a",
 author       = "Butler Lampson",
 title        = "How Software Components Grew Up and Conquered the World",
 booktitle    = "Proceedings of the ~21st~ International Conference on
                 Software Engineering",
 address      = "Los Angeles, California",
 month        = "16--22 " # may,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
For many years programmers have dreamed of building systems from a library of
reusable software components together with a little new code. The closest
we've come is Unix commands connected by pipes. I'll discuss the fundamental
reasons why software components of this kind haven't worked in the past and
are unlikely to work in the future. Then I'll explain how the dream has come
true in spite of this failure, and why most people haven't noticed.
",
 note         = "",
 url          = "URL: {\urlBiBTeX{http://www.research.microsoft.com/~lampson/Slides/ReusableComponentsAbstract.htm}}",
}

------------------------------------------------------------------------------

386

@Article{etzkorn97a,
 key     = "etzkorn97a",
 author  = "Letha H. Etzkorn and Carl G. Davis",
 title   = "Automatically Identifying Reusable {OO} Legacy Code",
 journal = j-comp,
 month   = "",
 year    = "1997",
 volume  = "30",
 number  = "10",
 pages   = "66--71",
 abstract = "
Much object oriented code has been written without reuse in mind, making
identification of useful components difficult. The Patricia (Program Analysis
Tool for Reuse) system automatically identifies these components through
understanding comments and identifiers. To understand a program, Patricia uses
a unique heuristic approach, deriving information from the linguistic aspects
of comments and identifiers and from other nonlinguistic aspects of OO code,
such as a class hierarchy. In developing the Patricia system, we had to
overcome the problems of syntactically parsing natural language comments and
syntactically analyzing identifiers-all prior to a semantic understanding of
the comments and identifiers. Another challenge was the semantic understanding
phase, when the organization of the knowledge base and an inferencing scheme
were developed (8 Refs.) 
",
 note    = "",
}

------------------------------------------------------------------------------

(In proceedings)
[cite for how formal spec of DFTs helped make natural language specs better,
which made for easier understanding on the part of the engineer.]

@InProceedings{manian99a,
 key          = "manian99a",
 author       = "Ragavan Manian and David W. Coppit and Kevin J. Sullivan and
                 Joanne Bechta Dugan",
 title        = "Bridging the Gap between Systems and Dynamic Fault Tree
                 Models",
 booktitle    = "Annual Reliability and Maintainability Symposium 1999
                 Proceedings",
 address      = "Washington, DC",
 month        = "18--21 " # jan,
 year         = "1999",
 pages        = "105--11",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Fault tolerant systems are composed of subsystems that interact with each
other, often in complex ways. Analyzing the reliability of these systems calls
for sophisticated modeling techniques. One such technique is dynamic fault
tree analysis. Because the semantics of dynamic fault trees are themselves
complex, there is a question of whether such models are faithful
representations of the modeled systems, and whether the underlying analysis
technique is correct. Previous definitions of the modeling constructs employed
in dynamic fault trees were not precise or consistent enough, leading to
ambiguities in their interpretation. We present our effort at making precise
the dynamic fault tree modeling and evaluation process. Our aim was to improve
our confidence in the validity of dynamic fault tree models of system failure
behavior. By rigorously specifying fault trees and their constituent gates and
basic events, we were able to reason more effectively about the correctness of
fault trees, the underlying analytical Markov models, and the numerical
solution to these analytical models.
",
 note         = "",
}

------------------------------------------------------------------------------

387

@TechReport{coppit98a,
 key         = "coppit98a",
 author      = "David Coppit and Kevin J. Sullivan",
 title       = "Unwrapping",
 institution = "Department of Computer Science, University of Virginia",
 month       = "4 " # may,
 year        = "1998",
 number      = "CS-98-08",
 address     = "",
 type        = "",
 abstract = "
A key driver of software obsolescence is change in hardware and system
software standards. In the area of software tools, there is now great pressure
to host on Intel/Windows platforms tool functions that in the past were
typically found in Unix or mainframe environments.  In such cases, there can
be value in reusing core parts of such legacy systems.  Two standard methods
for doing this are reengineering and wrapping.  Reengineering, being
unrestricted in the changes allowed, permits the removal of obsolete parts of
a system but creates the risk that changes will break the complex and brittle
reused code.  Wrapping involves the reuse of existing code without change, but
at the cost of including obsolete elements into the new environment.  In this
paper we propose unwrapping as a new synthesis of these two approaches.  To
unwrap is to remove unwanted design elements, but with a strong emphasis on
leaving core code unchanged.  We discuss our preliminary use of this approach
to reuse core elements of two Unix-based reliability engineering tools in a
modern tool based on package-oriented programming.
",
 note        = "",
}

------------------------------------------------------------------------------

388

@InProceedings{goldman99a,
 key          = "goldman99a",
 author       = "Neil M. Goldman and Robert M. Balzer",
 title        = "The {ISI} Visual Design Editor Generator",
 booktitle    = "1999 IEEE Symposium on Visual Languages (VL'99)",
 address      = "Tokyo, Japan",
 month        = sep,
 year         = "1999",
 pages        = "20--27",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
The benefits of ``domain specific'' languages and development environments are
widely recognized.  Constructing an environment for a new domain, however,
remains a costly activity, requiring expertise in several areas of software
development as well as in the targeted domain. The ISI design editor generator
and design environment comprises a novel infrastructure that simplifies this
task, producing visual domain-specific design environments. This paper
presents the runtime architecture of these environments, a visual
``specify-by-example'' capability that deals with a major portion of editor
generation, and an implementation that uses a COTS product (Microsoft
PowerPoint) as both graphic middleware and end-user GUI.
 ",
 note         = "",
}

------------------------------------------------------------------------------

@Misc{microsoft99a,
 key    = "microsoft99a",
 author = "Microsoft Corporation",
 title  = "Active Document Containers",
 url    = "http://msdn.microsoft.com/library/devprods/vs6/visualc/vccore/_core_activex_document_containers.htm",
 note   = "URL: {\urlBiBTeX{http://msdn.microsoft.com/library/devprods/vs6/visualc/vccore/_core_activex_document_containers.htm}}",
}

------------------------------------------------------------------------------

@Book{rogerson96a,
 key       = "rogerson96a",
 author    = "Dale Rogerson",
 title     = "Inside {COM}",
 publisher = "Microsoft Press",
 edition   = "",
 month     = "",
 year      = "1996",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

galileo citation

@InProceedings{sullivan99a,
 key          = "sullivan99a",
 author       = "Kevin J. Sullivan and Joanne Bechta Dugan and David Coppit",
 title        = "The {Galileo} Fault Tree Analysis Tool",
 booktitle    = "Proceedings of the 29th Annual International Symposium on
                 Fault-Tolerant Computing",
 address      = "Madison, Wisconsin",
 month        = "15--18 " # jun,
 year         = "1999",
 pages        = "232--5",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
We present Galileo, a dynamic fault tree modeling and analysis tool that
combines the innovative DIF-Tree analysis methodology with a rich user
interface built using package-oriented programming. DIFTree integrates binary
decision diagram and Markov methods under the common notation of dynamic fault
trees, allowing the user to exploit the benefits of both techniques while
avoiding the need to learn additional notations and methodologies.
Package-oriented programming (POP) is a software architectural style in which
large-scale software packages are used as components, exploiting their rich
functionality and familiarity to users. Galileo can be obtained for free under
license for evaluation, and can be downloaded from the World-Wide Web.
 ",
 note         = "",
}

------------------------------------------------------------------------------

@InProceedings{canfora93a,
 key          = "canfora93a",
 author       = "G. Canfora and Aniello Cimitile and M. Munro and C. J. Taylor",
 title        = "Extracting Abstract Data Types from {C} Programs: A Case
                 Study",
 booktitle    = "1993 Conference on Software Maintenance",
 address      = "Quebec, Canada",
 month        = "27--30 " # sep,
 year         = "1993",
 pages        = "200--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
This paper presents the results of a case study in identifying and extracting
reusable abstract data types from C programs. It applies reuse re-engineering
processes already established in the $RE^{2}$ project.  The method for
identifying abstract data types uses an interconnection graph called
variable-reference graph and coincidental and spurious connections within the
graph are resolved using a statistical technique.  A prototype tool is
described which demonstrates the feasibility of the method.  The tool is used
to analyze a C program and a number of abstract data types are identified and
then used in the maintenance of the original program.  The validity of the
method is assessed by a simple manual analysis of the source code.  The
resulting reusable components are specified using the formal notation Z.
",
 note         = "",
}

------------------------------------------------------------------------------

@InProceedings{canfora93b,
 key          = "canfora93b",
 author       = "G. Canfora and Aniello Cimitile and M. Munro",
 title        = "A Reverse Engineering Method for Identifying Reusable
                 Abstract Data Types",
 booktitle    = "Proceedings of Working Conference on Reverse Engineering",
 address      = "Baltimore, MD",
 month        = "21--23 " # may,
 year         = "1993",
 pages        = "73--82",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Results from an experiment in reuse within the RE/sup 2/ project are
presented. It is shown how a particular candidature criterion for identifying
abstract data types in existing software systems can be applied both at the
theoretical and practical level. The RE/sup 2/ project is concerned with the
exploration of reverse engineering and reengineering techniques to facilitate
reuse reengineering by the identification and classification of approximate
candidature criteria (19 Refs.) 
",
 note         = "",
}

------------------------------------------------------------------------------

@Article{canfora94a,
 key     = "canfora94a",
 author  = "G. Canfora and Aniello Cimitile and M. Munro",
 title   = "RE\textsuperscript{2}: Reverse-Engineering and Reuse 
            Re-engineering",
 journal = "Journal of Software Maintenance: Research and Practice",
 month   = mar # "--" # apr,
 year    = "1994",
 volume  = "",
 number  = "",
 pages   = "53--72",
 abstract = "
Initial research in reuse was in the designing and implementation of reusable
software. This research, although fruitful, did not address the area of
extracting reusable components from existing software. In this paper the term
reuse is used to mean the 'reuse of existing source code'. A process called
'reuse re-engineering' is defined and this, together with techniques from
reverse-engineering, form a new method for achieving reuse. A reference
paradigm is established to implement the reuse re-engineering process. This
process is divided into five sequential phases, each characterized by the
objects it produces. These phases are: candidature, election, qualification,
classification and storage, and search and display. This paper concentrates on
the first three phases because they produce reusable modules from existing
systems. In selecting candidates for reuse, different abstractions have to be
applied, namely functional abstraction (algorithms) and data abstractions
(data structures and data types). This paper presents a formalized approach to
each of these abstractions. The approach proposed aims to promote reuse in
industrial software production environments (63 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

@Article{murphy95b,
 key     = "murphy95b",
 author  = "Gail C. Murphy and David Notkin and Kevin Sullivan",
 title   = "Software Reflexion Models: Bridging the Gap Between Source and
            High-Level Models",
 journal = j-sig,
 month   = oct,
 year    = "1995",
 volume  = "20",
 number  = "4",
 pages   = "18--28",
 abstract = "
Software engineers often use high-level models (for instance, box and arrow
sketches) to reason and communicate about an existing software system.  One
problem with high-level models is that they are almost always inaccurate with
respect to the system's source code.  We have developed an approach that helps
an engineer use a high-level model of the structure of an existing software
system as a lens through which to see a model of that system's source code.
In particular, an engineer defines a high-level model and specifies how the
model maps to the source.  A tool then computes a software reflexion model
that shows where the engineer's high-level model agrees with and where it
differs from a model of the source.  The paper provides a formal
characterization of reflexion models, discusses practical aspects of the
approach, and relates experiences of applying the approach and tools to a
number of different systems, The illustrative example used in the paper
describes the application of reflexion models to NetBSD, an implementation of
Unix comprised of 250,000 lines of C code.  In only a few hours, an engineer
computed several reflexion models that provided him with a useful, global
overview of the structure of the NetBSD virtual memory subsystem.  The
approach has also been applied to aid in the understanding and experimental
reengineering of the Microsoft Excel spreadsheet product (17 Refs.)
",
 note    = "",
}

------------------------------------------------------------------------------

@InProceedings{dugan99a,
 key          = "dugan99a",
 author       = "Joanne Bechta Dugan and Kevin J. Sullivan and David Coppit",
 title        = "Developing a High-Quality Software Tool for Fault Tree
                 Analysis",
 booktitle    = "Proceedings of the International Symposium on Software
                 Reliability Engineering",
 address      = "Boca Raton, Florida",
 month        = "1--4 " # nov,
 year         = "1999",
 pages        = "222-31",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
Sophisticated dependability analysis techniques are being developed in
academia and research labs, but few have gained wide acceptance in industry.
To be valuable, such techniques must be supported by usable, dependable
software tools. We present our approach to addressing these issues in
developing a dynamic fault tree analysis tool called Galileo. Galileo is
designed to support efficient system-level analysis by automatically
decomposing fault trees into modules that are solved separately using
appropriate techniques. Usability is addressed by a software architecture
based on a component-based design technique that we call package-oriented
programming. We integrate multiple, volume-priced, mass-market software
packages to provide the bulk of the tool superstructure.  To address tool
dependability, we are developing natural language and partial formal
specifications of fault tree elements, and we exploit the inherent redundancy
associated with multiple analysis techniques as an aid in testing.
 ",
 note         = "",
}

------------------------------------------------------------------------------

389

@Misc{balzer99b,
 key    = "balzer99b",
 author = "Robert M. Balzer and Neil M. Goldman",
 title  = "Dressing {AI} in {COTS} Clothing",
 url    = "http://www.isi.edu/software-sciences/papers/dressing%20ai%20in%20cots%20clothing.pdf",
 abstract = "
Although, the benefits of ``domain specific'' languages and development
environments are widely recognized, constructing a design environment for a
new domain remains a costly activity, requiring expertise in several areas of
AI, software development, and the targeted domain. We've simplified this task
by combining a design editor generator with a COTS product (Microsoft
PowerPoint) that provides both the graphic middleware and end-user GUI for the
generated design editors.
",
 note   = "URL: {\urlBiBTeX{http://www.isi.edu/software-sciences/papers/dressing%20ai%20in%20cots%20clothing.pdf}}",
}

------------------------------------------------------------------------------

390

@InProceedings{bergner98a,
 key          = "bergner98a",
 author       = "Klaus Bergner and Andreas Rausch and Marc Sihling",
 title        = "Componentware -- The Big Picture",
 booktitle    = "Proceedings of the International Workshop on Component-Based
                 Software Engineering",
 address      = "",
 month        = apr,
 year         = "1998",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
The goal of componentware is to carry the old dream of building software
systems by assembling pre-fabricated components to undreamt brilliancy.
Although there is a variety of technical concepts and tools for
component-oriented software engineering, the successful model from the
building industry could not be transferred fully to software development yet.
In our opinion, this is mostly due to the lack of a suitable methodology.
 ",
 note         = "",
}

------------------------------------------------------------------------------

391

@InProceedings{bergner99a,
 key          = "bergner99a",
 author       = "Klaus Bergner and Andreas Rausch and Marc Sihling and
                 Alexander Vilbig",
 title        = "Componentware -- Methodology and Process",
 booktitle    = "CBSE '99 Proceedings of the International Workshop on
                 Component-Based Software Engineering",
 address      = "",
 month        = may,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
Componentware is concerned with the development of software systems by using
components as the essential building blocks. It is not a revolutionary
approach but incorporates successful concepts from established paradigms like
object-orientation while trying to overcome some of their deficiencies. Proper
encapsulation of common functionality, for example, and intuitive graphical
description techniques like class diagrams are keys to the widespread success
of an object-oriented software development process. However, the increasing
size and complexity of modern software systems leads to huge and complicated
conglomerations of classes and objects that are hard to manage and understand.
Those systems obviously require a more advanced means of structuring,
describing and developing them. Componentware is a possible approach to solve
these problems.
 ",
 note         = "",
}

------------------------------------------------------------------------------

392

@InProceedings{bergner98b,
 key          = "bergner98b",
 author       = "Klaus Bergner and Andreas Rausch and Marc Sihling and
                 Alexander Vilbig",
 title        = "An Integrated View On Componentware - Concepts, Description
                 Techniques, and Development Process",
 booktitle    = "IASTED 98, Proceedings of IASTED Conference on Software
                 Engineering",
 address      = "",
 month        = oct,
 year         = "1998",
 pages        = "77--82",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
We outline and clarify the essential concepts of the compo-nentware paradigm.
Based on the proposed definitions, we introduce a number of useful description
techniques, and sketch a flexible process model for component-based
development.  The presented techniques and concepts serve as building blocks
of an overall methodology for componentware which is the focus of our current
work.
 ",
 note         = "",
}

------------------------------------------------------------------------------

393

@Article{sullivan99b,
 key     = "sullivan99b",
 author  = "Kevin J. Sullivan and Mark Marchukov and John Socha",
 title   = "Analysis of a Conflict Between Aggregation and Interface
            Negotiation in {Microsoft's Component Object Model}",
 journal = j-tse,
 month   = sep,
 year    = "1999",
 volume  = "25",
 number  = "5",
 pages   = "",
 abstract = "
Many software project today are based on the integration of independently
designed software components that are acquired on the market, rather than
developed within the projects themselves. A component standard, or integration
architecture, is a set of design rules meant to ensure that such components
can be integrated in defined ways without undue effort. The rules of a
component standard define, among other things, component interoperability and
composition mechanisms. Understanding the properties of such mechanisms and
interactions between them is important for the successful development and
integration of software components, as well as for the evolution of component
standards. This paper presents a rigorous analysis of two such mechanisms:
component aggregation and dynamic interface negotiation, which were first
introduced in Microsoft's Component Object Model (COM). We show that interface
negotiation does not function properly within COM aggregation boundaries. In
particular, interface negotiation generally cannot be used to determine the
identity and set of interfaces of aggregated components. This complicates
integration within aggregates. We provide a mediator-based example, and show
that the problem is in the sharing of interfaces inherent in COM aggregation.
",
 note    = "",
}

------------------------------------------------------------------------------

394

@Misc{williams94a,
 key    = "williams94a",
 author = "Sara Williams and Charlie Kindel",
 title  = "The Component Object Model: A Technical Overview",
 url    = "http://msdn.microsoft.com/library/techart/msdn_comppr.htm",
 abstract = "
The Component Object Model (COM) is a software architecture that allows
applications to be built from binary software components. COM is the
underlying architecture that forms the foundation for higher-level software
services, like those provided by OLE. OLE services span various aspects of
commonly needed system functionality, including compound documents, custom
controls, interapplication scripting, data transfer, and other software
interactions. 
",
 note   = "URL: {\urlBiBTeX{http://msdn.microsoft.com/library/techart/msdn_comppr.htm}}",
}

------------------------------------------------------------------------------

395

@Article{szyperski95a,
 key     = "szyperski95a",
 author  = "Clemens Szyperski",
 title   = "Component-Oriented Programming: A Refined Variation on
            Object-Oriented Programming",
 journal = "The Oberon Tribune",
 month   = dec,
 year    = "1995",
 volume  = "1",
 number  = "2",
 pages   = "",
 abstract = "
There are several motivations to switch over to object-oriented development.
Besides exhibiting advantages in the analysis, design, and implementation
stages of software development, object-oriented technology also enables
entirely new approaches to software engineering in the large, in particular it
provides a (partial) basis for extensibility, an obvious requirement for
pluggable components to work. However, as we shall see, object-oriented
programming is not enough to enable construction of truly extensible systems.
The missing ingredients are safety and modularity. The combination of some
aspects of object-oriented programming, safety, and modularity with
extensibility as a goal is called Component-Oriented Programming.
",
 note    = "",
}

------------------------------------------------------------------------------

396

@Article{szyperski99a,
 key     = "szyperski99a",
 author  = "Clemens Szyperski",
 title   = "Components and Objects Together",
 journal = "Software Development",
 month   = may,
 year    = "1999",
 volume  = "",
 number  = "",
 pages   = "",
 abstract = "
Components are on the upswing; objects have been around for some time. It is
understandable, but not helpful, to see object-oriented programming sold in
new clothes by simply calling objects ``components''. The emerging
component-based approaches and tools combine objects and components in ways
that show they are really separate concepts. In this article, I will examine
some key differences between objects and components to clarify these muddy
waters. In particular, you'll see that approaches based on visual assembly
tools really assemble objects, not components, but they create components when
saving the finished assembly.
",
 note    = "",
}

------------------------------------------------------------------------------

397

@Article{bosch,
 key     = "bosch",
 author  = "Clemens Szyperski",
 title   = "Superimposition: A Component Adaptation Technique",
 journal = "Information and Software Technology",
 month   = "25 " # mar,
 year    = "1999",
 volume  = "41",
 number  = "5",
 pages   = "257--73",
 abstract = "
Several authors have identified that the only feasible way to increase
productivity in software construction is to reuse existing software. To
achieve this, component-based software development is one of the more
promising approaches. However, traditional research in component-oriented
programming often assumes that components are reused ``as-is''. Practitioners
have found that ``as-is'' reuse seldomly occurs and that reusable components
generally need to be adapted to match the system requirements. Existing
component object models provide only limited support for component adaptation,
i.e. white-box techniques such as copy-paste and inheritance and black-box
approaches such as aggregation and wrapping. These techniques suffer from
problems related to reusability, efficiency, implementation overhead or the
self problem. To address these problems, this paper proposes superimposition,
a novel black-box adaptation technique that allows one to impose predefined,
but configurable types of functionality on a reusable component. Three
categories of typical adaptation types are discussed, related to the component
interface, component composition and component monitoring. Superimposition and
the types of com- ponent adaptation are exemplified by several examples.
",
 note    = "",
}

------------------------------------------------------------------------------

398

@InProceedings{johnson94a,
 key       = "johnson94a",
 author    = "Stephen C. Johnson",
 title     = "Objecting to Objects",
 booktitle = "Proceedings of the Winter 1994 USENIX Technical Conference",
 address   = "San Francisco, California",
 month     = jan,
 year      = "1994",
 pages     = "",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "USENIX Association, Berkeley, CA",
 abstract = "
Object-oriented programming (OOP) is an ancient (25-year-old) technology, now
being pushed as the answer to all the world's programming ills. While not
denying that there are advantages to OOP, I argue that it is being oversold.
In particular, OOP gives little support to GUI and network support, some of
the biggest software problems we face today. It is difficult to constrain
relationships between objects (something SmallTalk did better than C++).
Fundamentally, object reuse has much more to do with the underlying models
being supported than with the object-ness of the programming language.
Object-oriented languages tend to burn CPU cycles, both at compile and
execution time, out of proportion to the benefits they provide. In summary,
the goods things about OOP are often the information hiding and consistent
underlying models which derive from clean thoughts, not linguistic cliches.
",
 note      = "",
}

------------------------------------------------------------------------------

399

@Misc{dod96a,
 key    = "dod96a",
 author = "The Department of Defense",
 title  = "Software Reuse Executive Primer",
 url    = "http://dii-sw.ncr.disa.mil/ReuseIC/pol-hist/primer/",
 abstract = "
Reuse is the process of implementing or updating software systems using
existing software assets.  Assets can be specifications, design, code, user
documentation, or anything associated with software. Reuse may occur within a
software system (e.g., common data and operations for all objects  of a
particular subclass); across similar software systems (e.g., report generators
for database management systems); or in widely different software systems
(e.g., WYSIWYG user interfaces for thousands of different application
packages). Reuse is an integral part of every engineering discipline. For
example, mechanical engineers do not design a combustion engine from scratch
for each car rolled off an assembly line; chemical engineers do not develop
the formula anew for each bottle of cleaner that is placed on a hardware
store's shelf; Microsoft software engineers do not recreate the interaction
between an application and its icon in the Windows environment for each new
product; and aerospace engineers do not build solid rocket boosters from
ground zero for each space shuttle. In all of these examples, the architecture
and design of an item is reused to produce and manage a ``product line''.
Software can also be acquired, developed, maintained, and managed via a
``product-line'' approach. 
",
 note   = "URL: {\urlBiBTeX{http://dii-sw.ncr.disa.mil/ReuseIC/pol-hist/primer/}}",
}

------------------------------------------------------------------------------

400

@Misc{cox99a,
 key    = "cox99a",
 author = "Brad Cox",
 title  = "What if there's a Silver Bullet...  And the Competition Gets it
           First?",
 url    = "http://www.virtualschool.edu/cox/CoxByte.html",
 abstract = "
Two centuries after its birth in the industrial revolution, the Age of
Manufacturing has matured and is showing signs of decline. And a new age, the
Age of Information, is emerging, born of the phenomenal achievements that the
Age of Manufacturing brought to transportation, communication, and computing.
\par

However, by eliminating time and space as barriers, the very achievements that
put us within reach of a truly global economy are burying us in irrelevant and
useless data; mountains of low quality ore that must be be laboriously refined
for relevant information; the signal hidden in the noise. The critical
resource for turning this raw data into useful information is computer
software, which is becoming the limiting strategic resource of the Age of
Information much as petroleum is a strategic resource today.
",
 note   = "URL: {\urlBiBTeX{http://www.virtualschool.edu/cox/CoxByte.html}}",
}

------------------------------------------------------------------------------

401

@InProceedings{andersen92a,
 key          = "andersen92a",
 author       = "E. P. Andersen and T. Reenskaug",
 title        = "System Design by Composing Structures of Interacting Objects",
 booktitle    = "ECOOP'92: Proceedings of the European Conference on
                 Object-Oriented Programming",
 address      = "",
 month        = "20--24 " # jul,
 year         = "1992",
 pages        = "133--52",
 editor       = "O. Lehrmann Madsen",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract     = "
Describes the outline of an object-oriented design technique denoted role
modeling, emphasizing the ability to compose parts of a design. The purpose of
role modeling is to achieve separation of concerns, allowing the designer to
consider different aspects, or the same aspect at different levels of detail,
more or less independent of other aspects of the overall design. A role model
represents the concept of a structure of communicating objects; each object
being represented by a role to be 'played' in the context of this role model.
Each role model is considered a design of a separate aspect of some overall
design. Composition of designs is achieved by synthesizing roles in several
role models, constructing more aggregated and specialized roles and role
models.
 ",
 note         = "",
}

------------------------------------------------------------------------------

402

@Article{dingel98a,
 key     = "dingel98a",
 author  = "J. Dingel and D. Garlan and S. Jha and D. Notkin",
 title   = "Reasoning about implicit invocation",
 journal = j-sig,
 month   = nov,
 year    = "1998",
 volume  = "23",
 number  = "6",
 pages   = "209--21",
 abstract = "
Implicit invocation has become an important architectural style for
large-scale system design and evolution. This paper addresses the lack of
specification and verification formalisms for such systems. Based on standard
notions from process algebra and trace semantics, we define a formal
computational model for implicit invocation. A verification methodology is
presented that supports linear time temporal logic and compositional
reasoning. First, the entire system is partitioned into groups of components
(methods) that behave independently. Then, local properties are proved for
each of the groups. A precise description of the cause and the effect of an
event supports this step. Using local correctness, independence of groups, and
properties of the delivery of events, we infer the desired property of the
overall system. Two detailed examples illustrate the use of our framework. 
",
 note    = "",
}

------------------------------------------------------------------------------

403

@InProceedings{deline99b,
 key          = "deline99b",
 author       = "Robert {DeLine}",
 title        = "A Catalog of Techniques for Resolving Packaging Mismatch",
 booktitle    = "Procedings of the Fifth Symposium on Software
                 Reusability ({SSR99})",
 address      = "",
 month        = "21--23 " # may,
 year         = "1999",
 pages        = "44--53",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM Press",
 abstract     = "
A problem that often hampers the smooth integration of reused components into
a new system is the packaging mismatch problem: when one or more of a
component's commitments about interaction with other components are not
supported in the context of integration. Although system integrators have
faced and surmounted this problem for decades, their experience largely exists
as unrecorded folklore and as specific papers in separate research communities
-- a situation which makes it difficult to systematically understand and solve
an instance of this problem. In order to allow system integrators to attack
packaging mismatches systematically, what is known about the problem and its
solutions must assembled and organized. To take a step in this direction, this
paper first discriminates the chief characteristics of component packaging,
which are the sources of mismatch. It provides a catalog of techniques to
resolve packaging mismatch, organized according to the architectural
commitments involved: on-line and off-line bridges, wrappers, intermediate
representations, mediators, unilateral and bilateral negotiation, and
component extension. Finally, it describes the issues involved in resolving
packaging mismatch, aspect by aspect.
 ",
 note         = "",
}

------------------------------------------------------------------------------

404

@Article{zaremski97a,
 key     = "zaremski97a",
 author  = "Amy Moormann Zaremski and Jeannette M. Wing",
 title   = "Specification Matching of Software Components",
 journal = "ACM Transactions on Software Engineering and Methodology",
 month   = oct,
 year    = "1997",
 volume  = "6",
 number  = "4",
 pages   = "333--369",
 abstract = "
Specification matching is a way to compare two software components based on
descriptions of the components' behaviors. In the context of software reuse
and library retrieval, it can help determine whether one component can be
substituted for another or how one can be modified to fit the requirements of
the other. In the context of object-oriented programming, it can help
determine when one type is a behavioral subtype of another.  \par 

We use formal specifications to describe the behavior of software components,
and hence, to determine whether two components match. We give precise
definitions of not just exact match, but more relevantly, various flavors of
relaxed match. These definitions capture the notions of generalization,
specialization, and substitutability of software components. \par

Since our formal specifications are pre- and post-conditions written as
predicates in first-order logic, we rely on theorem proving to determine match
and mismatch. We give examples from our implementation of specification
matching using the Larch Prover.
",
 note    = "",
}

------------------------------------------------------------------------------

405

@InProceedings{sullivan96c,
 key          = "sullivan96c",
 author       = "Kevin J. Sullivan and John C. Knight and Jake Cockrell and
                 Shengtong Zhang",
 title        = "Product Development with Massive Components",
 booktitle    = "Proceedings of the ~21st~ Annual Software Engineering
                 Workshop",
 address      = "Greenbelt, MD",
 month        = "4--5 " # dec,
 year         = "1996",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The problem faced by many of today's software engineers is to build and
maintain broad families of large systems in a cost-effective and timely
manner. Because the market demands rapid creation and modification of systems
in response to evolving requirements, extensive flexibility is needed. This
situation has two implications: first, basic system demands have to be met
quickly; second, responses to requested variations have to be rapid and
effective. One approach to cycle-time improvement that has been studied
extensively is software reuse. Current reuse techniques include system
synthesis using application-generator technologies and component-based
development techniques. On the basis of some experimental systems work, we
suggest that a new approach might merit increased attention from the research
community. The approach is based on the integration of large,
application-scale, binary components, such as shrink-wrapped software
packages. We have demonstrated the aggressive application of this idea in the
development of a fault-tree analysis tool that supports new analysis
techniques developed by Dugan at the University of Virginia. 
",
 note         = "",
}

------------------------------------------------------------------------------

406

@InProceedings{copenhafer99a,
 key          = "copenhafer99a",
 author       = "Michael A. Copenhafer and Kevin J. Sullivan",
 title        = "Exploration Harnesses: Tool-Supported Interactive Discovery
                 of Commercial Component Properties",
 booktitle    = "Proceedings of ASE-97: The 12th IEEE Conference on Automated
                 Software Engineering",
 address      = "Cocoa Beach, Florida",
 month        = "12--15 " # oct,
 year         = "1999",
 pages        = "7--14",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A key problem in component-based software development (CBSD) is that
developers have incomplete knowledge of components. In many cases, the only
available source of such information is experimentation. In this paper we
argue that the provision of tool support for automated and repeatable
experiments can provide significant value to designers. Such tools, which we
call exploration harnesses, promise to help enterprises manage and exploit
component evolution. We evaluated the exploration harness concept by building
a prototype and using it to support the exploration of components in the
design of a fault-tree analysis system called Galileo.  Galileo employs
package-oriented programming, in which shrink-wrapped packages such as
Microsoft Word and Visio Technical are used as large components. Using our
exploration harness helped us to discover a range of undocumented properties
of such components, across several versions, which enabled us to make better
informed design decisions. 
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{musser96a,
 key       = "musser96a",
 author    = "David R. Musser and Atul Saini",
 title     = "{STL} Tutorial and Reference Guide: {C++} Programming with the
              {S}tandard {T}emplate {L}ibrary",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1996",
 volume    = "",
 series    = "",
 address   = "",
 abstract = "
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{szyperski98a,
 key       = "szyperski98a",
 author    = "Clemens Szyperski",
 title     = "Component Software: Beyond Object-Oriented Programming",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1998",
 volume    = "",
 series    = "",
 address   = "",
 abstract = "
",
 note         = "",
}

------------------------------------------------------------------------------

@Book{monsonhaefel99a,
 key       = "monsonhaefel99a",
 author    = "Richard Monson-Haefel",
 title     = "Enterprise JavaBeans",
 publisher = "O'Reilly and Associates",
 edition   = "",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "",
 abstract = "
",
 note         = "",
}

------------------------------------------------------------------------------

@TechReport{sullivan97e,
 key         = "sullivan97e",
 author      = "Kevin J. Sullivan and Mark Marchukov",
 title       = "Interface Negotiation and Efficient Reuse: A Relaxed Theory 
                of the {Component Object Model}",
 institution = "Department of Computer Science, University of Virginia",
 month       = "9 " # may,
 year        = "1997",
 number      = "CS-97-11",
 address     = "",
 type        = "",
 abstract = "
Reconciling requirements for (1) the efficient integration of independently
developed and evolving components and (2) the evolution of systems built from
such components requires novel architectural styles, standards and idioms.
Traditional object-oriented approaches have proven inadequate. Two important
new mechanisms supporting integration and evolution are dynamic interface
negotiation and aggregation, an approach to efficient composition. Both
feature prominently in the Component Object Model (COM), a de facto standard
providing the architectural foundation for many important systems.  Because
these are important mechanisms in general, and because they are central to COM
in particular, it is essential that engineers be able to reason effectively
about them. In earlier work (Sullivan et al. 1997), we showed that reasoning
about them is hard and that formal mathematical theories of such mechanisms
can provide a foundation for effective reasoning. In this paper, we present a
new theory of interface negotiation and aggregation in COM. Our new theory is
based on a relaxed interpretation of the COM specification. Our earlier theory
reflected an interpretation of the specification in which components had to be
designed to follow COM-specified rules for interface negotiation and
aggregation under any possible usage. Our new, strictly weaker theory requires
only that actual system executions not manifest any violations of the rules.
Architectural styles using mediators that we showed to be untenable under the
earlier theory are tenable under this one provided that designers follow
certain rules. We derive these necessary and sufficient conditions for legal
use of interface negotiation in the presence of aggregation.  Our results
provide a basis for documenting what engineers must not do to use aggregation
and interface negotiation properly.
",
 note        = "",
}

------------------------------------------------------------------------------

@Article{dugan2000a,
 key     = "dugan2000a",
 author  = "Joanne Bechta Dugan and Kevin J. Sullivan and David Coppit",
 title   = "Developing a Low-Cost, High-Quality Software Tool for Dynamic
            Fault Tree Analysis",
 journal = "Transactions on Reliability",
 month   = mar,
 year    = "2000",
 volume  = "49",
 number  = "1",
 pages   = "49--59",
 abstract = "
Sophisticated modeling and analysis methods are being developed in academic
and industrial research labs for reliability engineering and other domains.
The evaluation and evolution of such methods based on use in practice is
critical to research progress, but few such methods see significant use. A
critical impediment to disseminating new methods is our inability to produce,
at a reasonable cost, supporting software tools that have the usability and
dependability characteristics that industrial users require and the
evolvability to accommodate software change as the underlying analysis methods
are refined and enhanced. The difficulty of software development thus emerges
as a key impediment to advances in engineering modeling and analysis. \par

Today, producing sophisticated software tools is costly and difficult even for
capable software developers. One problem is that when common design methods,
such as object-oriented programming, are used to build such tools, the results
are often large, complex, thus costly programs. Tools on the order of a
million lines of code are typical, with much of the code devoted to tool
interoperability, the human-computer interface and other issues not directly
related to modeling and analysis.  Making matters worse, domain experts, such
as reliability engineering researchers, often lack skills in modern software
development, while software engineers and researchers lack knowledge of the
application domains. All too often the results of tool development efforts
today are thus costly, hard to use, not dependable and essentially
unmaintainable. \par

In this paper, we present an approach to tool development that attacks these
problems. Progress requires synergistic, interdisciplinary collaborations
between application domain and software engineering researchers. We have
pursued such an approach in developing a fault tree modeling and analysis tool
called Galileo. We describe our innovations in two dimensions.  The first is
Galileo's core reliability modeling and analysis function. The second is our
work on software engineering for high-quality, low-cost modeling and analysis
tools.
",
 note    = "",
}

------------------------------------------------------------------------------

@InProceedings{sullivan97d,
 key          = "sullivan97d",
 author       = "Kevin J. Sullivan and J. C. Socha",
 title        = "Using Formal Methods to Reason about Architectural Standards",
 booktitle    = "Proceedings of the 19th International Conference on Software
                 Engineering",
 address      = "Boston, Massachusetts",
 month        = "17--23 " # may,
 year         = "1997",
 pages        = "503--513",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We present a study in which we used formal methods to reason precisely about
aspects of a widely used software architectural standard, namely Microsoft's
Component Object Model (COM). We developed a formal theory of COM to help us
reason about a proposed compositional architectural style based on COM,
intended for use in a novel commercial multimedia authoring system. The style
combined COM objects, integration mediators, and the COM reuse mechanism of
aggregation. Our use of formal methods averted an architectural disaster by
revealing essential but subtle and counterintuitive properties of COM. We
partially validated our theory by subjecting it to review by the designers of
COM and by testing it against other available data. The theory appears to have
good evidential support. 
",
 note         = "",
}

------------------------------------------------------------------------------

407

@TechReport{marchukov99a,
 key         = "marchukov99a",
 author      = "Mark Marchukov and Kevin J. Sullivan",
 title       = "Reconciling Behavioral Mismatch Through Component Restriction",
 institution = "Department of Computer Science, University of Virginia",
 month       = "30 " # jul,
 year        = "1999",
 number      = "CS-99-22",
 address     = "",
 type        = "",
 abstract = "
In component-based software development there are often mismatches between
system-level requirements and component behaviors. In general, bridging such
mismatches requires mutual adaptation of system requirements and components.
One kind of mismatch occurs when components permit behaviors that are not
permitted by the system- level requirements. We identify restriction, the
disabling of component behaviors, as an important way to bridge such
mismatches. Unlike extension, which is well studied, restriction has received
little attention. We present a model for reasoning about requirements for
restriction, and a corresponding technique for implementing restriction, based
on matching of partial models of component behaviors against
state-machine-based partial system specifications.  Our approach respects
several difficulties in component-based development: (a) behaviorally complex
components, (b) poorly documented component specifications, (c) inability to
change core component implementations, and (d) a general lack of built-in
restriction mechanisms in practice. To address these difficulties we use
lightweight incremental specification of component operations, obtained by
reverse-engineering, and external adaptors that adjust the behaviors of
components by manipulating their input streams. We describe our experience
using this approach to restrict shrink-wrapped package components in the
Galileo fault-tree analysis tool.
",
 note        = "",
}

------------------------------------------------------------------------------

408

@TechReport{mettala92a,
 key         = "mettala92a",
 author      = "Erik Mettala and Marc H. Graham (editors)",
 title       = "The Domain-Specific Software Architecture Program",
 institution = "Software Engineering Institute",
 month       = jun,
 year        = "1992",
 number      = "CMU/SEI-92-SR-9",
 address     = "",
 type        = "",
 abstract    = "
There are six independent projects within the DSSA program. Four of these
projects are working in specific, military-significant domains. Those domains
are Avionics Navigation, Guidance and Flight Director for Helicopters; Command
and Control; Distributed Intelligent Control and Management for Vehicle
Management; Intelligent Guidance, Navigation and Control for Missiles. In
addition, there are two projects working on underlying support technology.
Hybrid (discrete and continuous, non-linear) Control and Prototyping
Technology.  \par

This report contains brief descriptions from each project and an overview. 
",
 note        = "",
}

------------------------------------------------------------------------------

409

@Misc{jackson99a,
 key    = "jackson99a",
 author = "Daniel Jackson",
 title  = "{Alloy}: A Lightweight Object Modelling Notation",
 url    = "http://sdg.lcs.mit.edu/~dnj/publications.html",
 abstract = "
Alloy is a lightweight, precise and tractable notation for object modelling.
It attempts to combine the practicality of UML's static structure notation
with the rigour of Z, and to be expressive enough for most object modelling
problems while remaining amenable to automatic analysis. Alloy is a textual
notation, of which a subset is also expressible graphically. It has a simple
set-based semantics, and a type system that, by treating scalars as singleton
sets, sidesteps the problem of undefined expressions, and allows relations and
functions to be treated uniformly. It also has a novel mutability construct.
This paper explains what object modelling is, and shows how Alloy can be
applied to three problems of rather different flavours. It defines the entire
language, by translation to a small kernel with a simple semantics and type
system. It explains the rationale underlying Alloy's design, and its
divergences from existing languages.
",
 note   = "URL: {\urlBiBTeX{http://sdg.lcs.mit.edu/~dnj/publications.html}}",
}

------------------------------------------------------------------------------

410

@Misc{jackson99b,
 key    = "jackson99b",
 author = "Daniel Jackson",
 title  = "A Comparison of Object Modelling Notations: {Alloy}, {UML} and {Z}",
 url    = "http://sdg.lcs.mit.edu/~dnj/publications.html",
 abstract = "
An example of an object model is given in full in three languages: Alloy, a
new notation; Z, a formal specification language; and UML, a modelling
notation popular in industry. Basic features of Alloy are explained
informally, and briefly justified by comparison of the Alloy version to the
UML and Z versions. 
",
 note   = "URL: {\urlBiBTeX{http://sdg.lcs.mit.edu/~dnj/publications.html}}",
}

------------------------------------------------------------------------------

411

@InProceedings{toyn98a,
 key          = "toyn98a",
 author       = "Ian Toyn",
 title        = "Innovations in the Notation of Standard {Z}",
 booktitle    = "ZUM '98: Z Formal Specification Notation. 11th International
                 Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "24--26 " # sep,
 year         = "1998",
 pages        = "193--213",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
The second Committee Draft of the ISO standard ``Z Notation'' is expected to be
published soon after the ZUM'98 proceedings. The paper provides an overview of
Standard Z from the perspective of the differences between its notation and
that of Spivey's de facto standard ``The Z Notation: A Reference Manual'' (J. M.
Spivey, 1989; 1992). Its aim is to make the differences be more widely known
and hence enable wider exploitation of the improvements, by both specifiers
and tool builders. The differences reported are those for which the author
perceives there to be consensus within the Z panel. As the second Committee
Draft is not yet finished, beware that this consensus could yet change.
",
 note         = "",
}

------------------------------------------------------------------------------

412

@InProceedings{evans97a,
 key          = "evans97a",
 author       = "Andy S. Evans",
 title        = "An improved recipe for specifying reactive systems in {Z}",
 booktitle    = "ZUM '97: Z Formal Specification Notation. 11th International
                 Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "3--4 " # apr,
 year         = "1997",
 pages        = "275--94",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
How can a reactive system be specified in Z without having to use additional
formalisms such as CSP or temporal logic? The conventional wisdom, is that it
cannot. Notations like Z and VDM traditionally describe a system as an
abstract data type. Hence they concentrate on the static system behaviour:
that is why they define operations using state before and state after.
However, it seems clear that in order to specify a reactive system, dynamic
behaviour must be described otherwise concurrent or real-time properties
cannot be specified. It is this aspect which is entirely missing from
conventional Z and VDM specifications. During the late eighties, (Duke et al.,
1988; 1989) provided a partial solution to this problem. They showed how a
conventional Z specification could be augmented with an additional
specification describing its reactive behaviour. The aim of this paper is to
show that this promising approach can be greatly improved and extended upon to
the point where it can provide a practical method for specifying reactive
systems in Z. 
",
 note         = "",
}

------------------------------------------------------------------------------

413

@InProceedings{saaltink97a,
 key          = "saaltink97a",
 author       = "Mark Saaltink",
 title        = "The {Z/EVES} System",
 booktitle    = "ZUM '97: Z Formal Specification Notation. 11th International
                 Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "3--4 " # apr,
 year         = "1997",
 pages        = "72--85",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Describes the Z/EVES system, which allows Z specifications to be analysed in a
number of different ways. Among the significant features of Z/EVES are domain
checking, which ensures that a specification is meaningful, and a theorem
prover that includes a decision procedure for simple arithmetic and a
heuristic rewriting mechanism that recognizes ``obvious'' facts. 
",
 note         = "",
}

------------------------------------------------------------------------------

414

@InProceedings{jacky97a,
 key          = "jacky97a",
 author       = "Jonathan Jacky and Jonathan Unger and Michael Patrick and
                 David Reid and Ruedi Risler",
 title        = "Experience with {Z} developing a control program for a
                 radiation therapy machine",
 booktitle    = "ZUM '97: Z Formal Specification Notation. 11th International
                 Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "3--4 " # apr,
 year         = "1997",
 pages        = "317--28",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
We are developing a control program for a unique radiation therapy machine.
The program is safety-critical, executes several concurrent tasks, and must
meet real-time deadlines. Development employs both formal and traditional
methods: we produce an informal specification in prose (supplemented by
tables, diagrams and a few formulas) and a formal description in Z. The Z
description includes an abstract level that expresses overall safety
requirements and a concrete level that serves as a detailed design, where Z
paragraphs correspond to data structures, functions and procedures in the
code. We validate the Z texts against the prose specification by inspection.
We derive most of the code from the Z texts by intuition and verify it by
inspection but a small amount of code is derived and verified more formally.
We have produced about 250 pages of informal specification and design
description, about 1200 lines of Z and about 6000 lines of code. Experiences
developing a large Z specification and writing the program are reported, and
some errors we discovered and corrected are described.
",
 note         = "",
}

------------------------------------------------------------------------------

415

@InProceedings{heitmeyer97a,
 key          = "heitmeyer97a",
 author       = "Constance Heitmeyer",
 title        = "Formal Methods: A Panacea or Academic Poppycock?",
 booktitle    = "ZUM '97: Z Formal Specification Notation. 11th International
                 Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "3--4 " # apr,
 year         = "1997",
 pages        = "3--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract = "
Much has been written in the past decade about the usefulness of formal
methods for developing computer systems. I describe first what I mean by a
formal method, discuss some systems to which formal methods have been
successfully applied recently, and present several guidelines that I and my
colleagues have found useful in applying formal methods in the development of
practical systems and software. I conclude with a summary of what has been
accomplished to date and why some skepticism about the utility of formal
methods in computer system development remains well-founded.
",
 note         = "",
}

------------------------------------------------------------------------------

416

@TechReport{batory97c,
 key         = "batory97c",
 author      = "Don Batory",
 title       = "Intelligent Components and Software Generators",
 institution = "University of Texas",
 month       = "1 " # apr,
 year        = "1997",
 number      = "CS-TR-97-06",
 address     = "Austin, TX",
 type        = "",
 abstract    = "
The production of well-understood software will eventually be the
responsibility of software generators. Generators will enable
high-perforrnance, customized software systems and subsystems to be assembled
quickly and cheaply from component libraries.  These components will be
intelligent: they will encapsulate domain-specific knowledge (e.g., best
practicers approaches) so that their instances will automatically customize
and optimize themselves to the system in which they are being used.  In this
paper, we explore the topics intelligent components and software generation as
they pertain to the issues of software productivity, perforrnance,
reliability, and quality.
",
 note        = "URL: {\urlBiBTeX{ftp://ftp.cs.utexas.edu/pub/techreports/tr97-06.ps.Z}}",
}

------------------------------------------------------------------------------

417

@InProceedings{zhang97a,
 key          = "zhang97a",
 author       = "Da-Qian Zhang and Kang Zhang",
 title        = "Reserved Graph Grammar: A Specification Tool For 
                 Diagrammatic {VPLs}",
 booktitle    = "Proceedings. 1997 IEEE Symposium on Visual Languages",
 address      = "Isle of Capri, Italy",
 month        = "23--26 " # sep,
 year         = "1997",
 pages        = "284--91",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 journal      = "",
 publisher    = "",
 abstract = "
When implementing textual languages, formal grammars are commonly used to
facilitate understanding languages and creating parsers. In the implementation
of a diagrammatic visual programming language (VPL), this rarely happens,
though graph grammars with their well established theoretical background may
be used as a natural and powerful syntax definition formalism. Yet all graph
grammar parsing algorithms presented up to now are either unable to recognize
interesting visual languages or tend to be hopelessly inefficient (with
exponential time complexity) when applied to graphs with a large number of
nodes and edges. The paper presents a context sensitive graph grammar called
reserved graph grammar which can explicitly, efficiently and completely
describe the syntax of a wide range of diagrams using labeled graphs. Moreover
its parsing algorithm is of polynomial time complexity in most cases.
",
 note         = "",
}

------------------------------------------------------------------------------

418

@InProceedings{kiczales97c,
 key          = "kiczales97c",
 author       = "Gregor Kiczales and John Lamping and Cristina Videira Lopes
                 and Chris Maeda and Anurag Mendhekar and Gail Murphy",
 title        = "Open Implementation Design Guidelines",
 booktitle    = "Proceedings of the 19th International Conference on Software
                 Engineering",
 address      = "Boston, Massachusetts",
 month        = "17--23 " # may,
 year         = "1997",
 pages        = "481--90",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Designing reusable software modules can be extremely difficult. The design
must be balanced between being general enough to address the needs of a wide
range of clients and being focused enough to truly satisfy the requirements of
each specific client. One area where it can be particularly difficult to
strike this balance is in the implementation strategy of the module. The
problem is that general purpose implementation strategies, tuned for a wide
range of clients, aren't necessarily optimal for each specific client-this is
especially an issue for modules that are intended to be reusable and yet
provide high-performance. An examination of existing software systems shows
that an increasingly important technique for handling this problem is to
design the module's interface in such a way that the client can assist or
participate in the selection of the module's implementation strategy. The
authors call this approach open implementation. When designing the interface
to a module that allows its clients some control over its implementation
strategy, it is important to retain, as much as possible, the advantages of
traditional closed implementation modules. The paper explores issues in the
design of interfaces to open implementation modules. They identify key design
choices, and present guidelines for deciding which choices are likely to work
best in particular situations. 
",
 note         = "",
}

------------------------------------------------------------------------------

419

@InProceedings{kiczales97d,
 key          = "kiczales97d",
 author       = "Gregor Kiczales and John Lamping and Anurag Mendhekar and
                 Chris Maeda and Cristina Videira Lopes and Jean-Marc
                 Loingtier and John Irwin",
 title        = "Aspect-oriented programming",
 booktitle    = "ECOOP'97: Proceedings of the European Conference on
                 Object-Oriented Programming",
 address      = "",
 month        = "9--13 " # jun,
 year         = "1997",
 pages        = "220--42",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "Springer-Verlag",
 abstract     = "
The authors have found many programming problems for which neither procedural
nor object-oriented programming techniques are sufficient to clearly capture
some of the important design decisions the program must implement. This forces
the implementation of those design decisions to be scattered throughout the
code, resulting in ``tangled'' code that is excessively difficult to develop and
maintain. They present an analysis of why certain design decisions have been
so difficult to clearly capture in actual code. They call the properties these
decisions address aspects, and show that the reason they have been hard to
capture is that they cross-cut the system's basic functionality. They present
the basis for a new programming technique, called aspect-oriented programming,
that makes it possible to clearly express programs involving such aspects,
including appropriate isolation, composition and reuse of the aspect code. The
discussion is rooted in systems we have built using aspect-oriented
programming. 
 ",
 note         = "",
}

------------------------------------------------------------------------------

420

@Article{brownsword98a,
 key     = "brownsword98a",
 author  = "Lisa Brownsword and David Carney and Tricia Oberndort",
 title   = "The Opportunities and Complexities of Applying 
            Commercial-Off-the-Shelf Components",
 journal = "Crosstalk",
 month   = apr,
 year    = "1998",
 volume  = "11",
 number  = "4",
 pages   = "4--6",
 abstract = "
Government acquisition policies for software-intensive systems emphasize the
use of commercial- off-the-shelf (COTS) products. On the surface, ``the COTS
solution'' appears straightforward.  In actuality, many projects find its use
less than straightforward. This article provides acquisition managers and
policy makers with a basic understanding of how developing systems with COTS
products is different and why and what new COTS capabilities are being
identified.
",
 note    = "",
}

------------------------------------------------------------------------------

421

@Article{fox98a,
 key     = "fox98a",
 author  = "Greg Fox and Steven Marcom",
 title   = "A Software Development Process for {COTS}-Based Information System
            Infrastructure: Part 1",
 journal = "Crosstalk",
 month   = mar,
 year    = "1998",
 volume  = "11",
 number  = "3",
 pages   = "20--25",
 abstract = "
The Infrastructure Incremental Development Process fills two gaps within the
realm of software development processes: It provides a programmatic,
prototype-driven, but carefully controlled approach to
commercial-off-the-shelf selection and integration, and it provides a process
that specifically addresses development not of applications but of the
infrastructure of a large distributed information system.
",
 note    = "",
}

------------------------------------------------------------------------------

422

@Article{fox98b,
 key     = "fox98b",
 author  = "Greg Fox and Steven Marcom and Karen W. Lantner",
 title   = "A Software Development Process for {COTS}-Based Information System
            Infrastructure: Part {II} Lessons Learned",
 journal = "Crosstalk",
 month   = apr,
 year    = "1998",
 volume  = "11",
 number  = "4",
 pages   = "11--13",
 abstract = "
Part I of this article (CROSSTALK, March 1998) described the Infrastructure
Incremental Development Approach process model. Part II describes a particular
application of that model and examines the practical lessons learned and
pitfalls encountered.
",
 note    = "",
}

------------------------------------------------------------------------------

423

@InBook{dsouza98a,
 key          = "dsouza98a",
 author       = "Desmond F. {D'Souza} and Alan C. Wills",
 title        = "Objects, Components and Frameworks with {UML}: The
                 Catalysis Approach",
 chapter      = "6",
 pages        = "816",
 publisher    = "Addison-Wesley",
 year         = "1998",
 volume       = "",
 series       = "Object Technology Series",
 address      = "",
 edition      = "1st",
 month        = "",
 remark       = "",
 type         = "",
 abstract     = "
Objects, Components, and Frameworks with UML: The Catalysis Approach is where
you will learn how to use objects, frameworks, and UML notation to design,
build, and reuse component-based software. Catalysis is a rapidly emerging
UML-based method for object and component-based development. It provides you
with a clear meaning of and systematic uses for the UML notation. \par

``The Catalysis Approach'' explains how patterns can be characterized as model
frameworks. Through the application of frameworks in requirements,
specifications, architectures, and designs, you will find that all models
contain recurring patterns of structure, behavior, and refinement. This opens
the way to building models and designs rapidly by adapting and composing both
generic and domain-specific modeling frameworks.
 ",
 note         = "",
}

------------------------------------------------------------------------------

424

@Article{waldo99a,
 key     = "waldo99a",
 author  = "Jim Waldo",
 title   = "The {Jini} Architecture for Network-centric Computing",
 journal = j-cacm,
 month   = jul,
 year    = "1999",
 volume  = "42",
 number  = "7",
 pages   = "76--82",
 abstract = "
A federation of spontaneously networked electronic components of all types can
communicate, interact, and share their services and functions, as explained by
Jini's lead architect.
",
 note    = "",
}

------------------------------------------------------------------------------

@Book{hammond98,
 key       = "hammond98",
 author    = "John S. Hammond and Ralph L. Keeney and Howard Raiffa",
 title     = "Smart Choices : A Practical Guide to Making Better Decisions",
 publisher = "Harvard Business School Press",
 edition   = "",
 month     = sep,
 year      = "1998",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

425

@InProceedings{consel98a,
 key          = "consel98a",
 author       = "Charles Consel and Renaud Marlet",
 title        = "Architecturing Software Using A Methodology for Language
                 Development",
 booktitle    = "Principles of Declarative Programming. 10th International
                 Symposium, PLIP'98.",
 address      = "Pisa, Italy",
 month        = "16--18 " # sep,
 year         = "1998",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Domain-specific languages (DSLs) can be viewed from both a programming
language and a software architecture perspective. The goal of the paper is to
relate the two viewpoints. In particular, we demonstrate that DSLs can be
constructed using an existing formal methodology for developing general
purpose languages (GPLs) while expressing software architecture concerns.
Software architectures express how systems should be built from various
components and how those components should interact. From a software
architecture perspective, a DSL can be seen as a parameterization mechanism as
well as an interface model. (40 References).
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

426

@InProceedings{cimitile98a,
 key          = "cimitile98a",
 author       = "Aniello Cimitile and Ugo De Carlini and Andrea De Lucia",
 title        = "Incremental Migration Strategies: Data Flow Analysis for
                 Wrapping",
 booktitle    = "Proceedings Fifth Working Conference on Reverse Engineering",
 address      = "Honolulu, HI",
 month        = "12--14 " # oct,
 year         = "1998",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Incremental migration strategies entail the decomposition of large legacy
systems in components that can be independently and selectively replaced; this
reduces the costs and risks of a migration program. The legacy components are
encapsulated into object wrappers and used (through the wrapper interface) in
their original form until new components take up their functions with an
acceptable level of reliability. The decomposition of legacy programs in
components to be encapsulated in different wrappers involves reengineering
activities for creating a new program for each component. Data flow analysis
methods are needed for identifying the formal parameters in the interfaces of
such programs. We present the approach defined within the project ERCOLE, a
research project aiming at migrating legacy systems towards object-oriented
platforms. (23 References). 
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

427

@InProceedings{sneed97b,
 key          = "sneed97b",
 author       = "Harry M. Sneed",
 title        = "Program Interface Reengineering for Wrapping",
 booktitle    = "Proceedings of the Fourth Working Conference on Reverse
                 Engineering",
 address      = "Amsterdam, Netherlands",
 month        = "6--8 " # oct,
 year         = "1997",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The paper describes a technique for reengineering program interfaces as a
prerequisite to wrapping them. Program wrapping is used to encapsulate old
software components in a new object-oriented architecture. For this the
programs must have well defined interfaces which contain all of their global
data. Program interface reengineering as described in the paper is a means to
achieving that end. It has been used by the author to prepare an ATM booking
system for wrapping. The result was not only reusable objects but also more
maintainable programs. (19 References).
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

428

@Article{bowen95a,
 key     = "bowen95a",
 author  = "Jonathan P. Bown and Michael G. Hinchey",
 title   = "Seven More Myths of Formal Methods",
 journal = j-soft,
 month   = jul,
 year    = "1995",
 volume  = "12",
 number  = "4",
 pages   = "34-41",
 abstract= "
New myths about formal methods are gaining tacit acceptance both outside and
inside the system-development community. The authors address and dispel these
myths based on their observations of industrial projects. The myths include:
formal methods delay the development process; they lack tools; they replace
traditional engineering design methods; they only apply to software; are
unnecessary; not supported; and formal methods people always use formal
methods. (0 References).
",
 note    = "",
}

------------------------------------------------------------------------------

429

@Article{kitchenham95a,
 key     = "kitchenham95a",
 author  = "Barbara Kitchenham and Lesley Pickard and Shari Lawrence Pfleeger",
 title   = "Case Studies for Method and Tool Evaluation",
 journal = j-soft,
 month   = jul,
 year    = "1995",
 volume  = "12",
 number  = "4",
 pages   = "52-62",
 abstract= "
Case studies help industry evaluate the benefits of methods and tools and
provide a cost-effective way to ensure that process changes provide the
desired results. However, unlike formal experiments and surveys, case studies
do not have a well-understood theoretical basis. This article provides
guidelines for organizing and analyzing case studies so that they produce
meaningful results. (10 References).
",
 note    = "",
}

------------------------------------------------------------------------------

430

@Article{bisbal99a,
 key     = "bisbal99a",
 author  = "{Jes\'us} Bisbal and Deirdre Lawless and Bing Wu and Jan Grimson",
 title   = "Legacy Information Systems: Issues and Directions",
 journal = j-soft,
 month   = sep # "--" # oct,
 year    = "1999",
 volume  = "16",
 number  = "5",
 pages   = "103-11",
 abstract= "
A legacy information system represents a massive, long-term business
investment. Unfortunately, such systems are often brittle, slow and
non-extensible. Capturing legacy system data in a way that can support
organizations into the future is an important but relatively new research
area. The authors offer an overview of existing research and present two
promising methodologies for legacy information system migration. (25
References).
",
 note    = "",
}

------------------------------------------------------------------------------

431

@Misc{tilley95a,
 key    = "tilley95a",
 author = "Scott R. Tilley and Dennis B. Smith",
 title  = "Perspectives on Legacy System Reengineering",
 url    = "http://www.sei.cmu.edu/reengineering/lsysree.html",
 abstract = "
One of the goals of modern software engineering is to move towards a paradigm
of evolutionary systems. In this model, the articial distinction between
development and maintenance is replaced by the notion of continous evolution
of the subject system. While this notion holds much promise for future
systems, there is one major stumbling block on the road to its wide-spread
acceptance and use: the existing base of legacy systems. \par

Reengineering offers an approach to migrating a legacy system towards an
evolvable system in a disciplined manner. The process of reengineering may be
viewed as applying engineering principles to an existing system in order for
it to meet new requirements. However, in order to be successful, reengineering
requires insights from a number of different perspectives. \par

This document outlines current issues and trends in reengineering from several
perspectives.  These include an engineering perspective, a system perspective,
a software perspective, a managerial perspective, an evolutionary perspective,
and a maintenance perspective. While not exhaustive, these viewpoints serve as
a framework for placing reengineering in the context of evolutionary systems.
",
 note   = "URL: {\urlBiBTeX{http://www.sei.cmu.edu/reengineering/lsysree.html}}",
}

------------------------------------------------------------------------------

@Book{hayes93a,
 key       = "hayes93a",
 author    = "I. Hayes",
 title     = "Specification Case Studies",
 publisher = "Prentice Hall International Series in Computer Science",
 edition   = "2nd",
 month     = "",
 year      = "1993",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{woodcock96a,
 key       = "woodcock96a",
 author    = "J. C. P. Woodcock and J. Davies",
 title     = "Using {Z}: Specification, Proof and Refinement",
 publisher = "Prentice Hall International Series in Computer Science",
 edition   = "",
 month     = "",
 year      = "1996",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{wordsworth92a,
 key       = "wordsworth92a",
 author    = "J. B. Wordsworth",
 title     = "Software Development with {Z}",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1992",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@InProceedings{coppit2000b,
 key          = "coppit2000b",
 author       = "David Coppit and Kevin J. Sullivan and Joanne Bechta Dugan",
 title        = "Formal Semantics of Models for Computational Engineering:
                 A Case Study on Dynamic Fault Trees",
 booktitle    = "Proceedings of the International Symposium on Software
                 Reliability Engineering",
 address      = "San Jose, California",
 month        = "8--11 " # oct,
 year         = "2000",
 pages        = "270--282",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Computational modeling tools are critical to engineering. In the
absence of a sufficiently complete, mathematically precise, abstract
specification of the semantics of the modeling framework supported by
such a tool, rigorous validation of the framework and of models built
using it is impossible; there is no sound basis for program
implementation, verification or documentation; the scientific
foundation of the framework remains weak; and significant conceptual
errors in framework definition and implementation are likely.  Yet
such specifications are rarely defined.  We present an approach based
on the use of formal specification and denotational semantics
techniques from software engineering and programming language design.
To illustrate the approach, we present elements of a formal semantics
for a dynamic fault tree framework that promises to aid reliability
analysis. No such specification of the meaning of dynamic fault trees
has been defined previously.  The approach revealed important
shortcomings in the previous, informal definitions of the framework,
and thus led to significant improvements, suggesting that formally
specifying framework semantics is critical to effective framework
design.
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

In proceedings

@InProceedings{coppit2000c,
 key          = "coppit2000c",
 author       = "David Coppit and Kevin J. Sullivan",
 title        = "Multiple Mass-Market Applications as Components",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11 " # jun,
 year         = "2000",
 pages        = "273--82",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Truly successful models for component-based software development continue to
prove elusive. One of the few is the use of operating system, database and
similar programs in many systems. We address three related problems in this
paper. First, we lack needed models. Second, we do not know the conditions
under which such models can succeed. In particular, it is unclear whether the
notable success with operating systems can be replicated. Third, we do not
know whether certain specific models can succeed. We are addressing these
problems by evaluating a particular model that shares important
characteristics with the successful operating system example: using compatible
PC packages as components. Our approach to evaluating such a model is to
engage in a case study that aims to build an industrially successful system
representative of an important class of systems. We report on our use of the
model to develop a computational tool for reliability engineering. We draw two
conclusions. First, this kind of model has the potential to succeed. Second,
even today, the model can produce significant returns, but it clearly carries
considerable risks.
",
 note         = "",
}

------------------------------------------------------------------------------

In proceedings

@InProceedings{coppit2000d,
 key          = "coppit2000d",
 author       = "David Coppit and Kevin J. Sullivan",
 title        = "{Galileo}: A Tool Built From Mass-Market Applications",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11 " # jun,
 year         = "2000",
 pages        = "750--3",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We present Galileo, an innovative engineering modeling and analysis tool built
using an approach we call package-oriented programming (POP). Galileo
represents an ongoing evaluation of the POP approach, where multiple large,
architecturally coherent components are tightly integrated in an overall
software system. Galileo utilizes Microsoft Word, Internet Explorer, and Visio
to provide a low cost, richly functional fault tree modeling superstructure.
Based on the success of previous prototypes of the tool, we are now building a
version for industrial use under an agreement with NASA Langley Research
Center.
",
 note         = "",
}

------------------------------------------------------------------------------

@Article{feder71a,
 key     = "feder71a",
 author  = "Jerome Feder",
 title   = "Plex Languages",
 journal = "Information Sciences",
 month   = jul,
 year    = "1971",
 volume  = "3",
 number  = "3",
 pages   = "225--241",
 abstract= "
The phrase-structure grammar scheme used for specifying string
languages is extended to structures called plexes composed of symbols
with an arbitrary number of ``attaching points''. Classes of plex
languages that parallel existing classes of string languages are
defined, and these classes are shown to be distinct. Context-free
grammars for languages of chemical structures, logic diagrams,
electrical circuits, and flowcharts are given. Plex languages are used
to specify the interconnection of encoded geometric curves to form
mesh-like line patterns, and methods are given by which languages of
such line patterns can be classified. (10 References).
",
 note    = "",
}

------------------------------------------------------------------------------

432

@Article{butler93a,
 key     = "butler93a",
 author  = "Ricky W. Butler and George B. Finelli",
 title   = "The Infeasibility of Quantifying the Reliability of Life-Critical
            Real-Time Software",
 journal = j-tse,
 month   = jan,
 year    = "1993",
 volume  = "19",
 number  = "1",
 pages   = "3--12",
 abstract = "
This work affirms that the quantification of life-critical software
reliability is infeasible using statistical methods, whether these methods are
applied to standard software or fault-tolerant software. The classical methods
of estimating reliability are shown to lead to exorbitant amounts of testing
when applied to life-critical software. Reliability growth models are examined
and also shown to be incapable of overcoming the need for excessive amounts of
testing. The key assumption of software fault tolerance-separately programmed
versions fail independently-is shown to be problematic. This assumption cannot
be justified by experimentation in the ultrareliability region, and subjective
arguments in its favor are not sufficiently strong to justify it as an axiom.
Also, the implications of the recent multiversion software experiments support
this affirmation. (16 References).
",
 note    = "",
}

------------------------------------------------------------------------------

433

@InProceedings{boyle99a,
 key          = "boyle99a",
 author       = "James M. Boyle and R. Daniel Resler and Victor L. Winter",
 title        = "Do You Trust Your Compiler? applying Formal Methods To
                 Constructing High-Assurance Compilers.",
 booktitle    = "Proceedings. 1997 High-Assurance Engineering",
 address      = "Los Alamitos, CA",
 month        = "11--12 " # aug,
 year         = "1997",
 pages        = "14--24",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Describes how automatic transformation technology can be used to construct a
verified compiler for an imperative language. Our approach is to
``transformationally'' pass a source program through a series of canonical
forms, each of which correspond to some goal or objective in the compilation
process (e.g. introduction of registers, simplification of expressions, etc.).
We describe a denotational semantics-based framework in which it is possible
to verify the correctness of transformations; the correctness of the compiler
follows from the correctness of the transformations. (17 References).
",
 note         = "",
}

------------------------------------------------------------------------------

434

@InProceedings{costagliola94a,
 key          = "costagliola94a",
 author       = "Gennaro Costagliola and Andrea De Lucia and Sergio Orefice",
 title        = "Towards Efficient Parsing of Diagrammatic Languages",
 booktitle    = "Proceedings of the Workshop on Advanced Visual Interfaces
                 AVI '94",
 address      = "Bari, Italy",
 month        = "1--4 " # jun,
 year         = "1994",
 pages        = "162--71",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Many models have been presented to specify visual languages and big efforts
are being made to characterize a class of visual languages which is expressive
enough and, at the same time, efficient to parse. Along this direction, the
positional grammar model has been defined to extend the LR parsing techniques
and parse efficiently not only the string languages but also iconic languages.
We present an extension of the positional grammar model in order to describe a
wide variety of diagrammatic languages. We show that this new formalism allows
the construction of an efficient LR-like parser for visual languages of very
practical interest. (21 References).
",
 note         = "",
}

------------------------------------------------------------------------------

435

@Article{demillo79a,
 key     = "demillo79a",
 author  = "Richard A. De Millo and Richard J. Lipton and Alan J. Perlis",
 title   = "Social Processes and Proofs of Theorems and Programs",
 journal = j-cacm,
 month   = may,
 year    = "1979",
 volume  = "22",
 number  = "5",
 pages   = "271--80",
 abstract = "
It is argued that formal verifications of programs, no matter how obtained,
will not play the same key role in the development of computer science and
software engineering as proofs do in mathematics. Furthermore the absence of
continuity, the inevitability of change, and the complexity of specification
of significantly many real programs make the formal verification process
difficult to justify and manage. It is felt that ease of formal verification
should not dominate program language design. (20 References).
",
 note    = "",
}

------------------------------------------------------------------------------

436

@InProceedings{rhodes99a,
 key          = "rhodes99a",
 author       = "Bradley J. Rhodes and Nelson Minar and Josh Weaver",
 title        = "Wearable Computing Meets Ubiquitous Computing",
 booktitle    = "Digest of Papers. Third International Symposium on Wearable
                 Computers",
 address      = "San Francisco, CA",
 month        = "18--19 " # oct,
 year         = "1999",
 pages        = "141--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
This paper describes what we see as fundamental difficulties in both the pure
ubiquitous computing and pure wearable computing paradigms when applied to
context-aware applications. In particular, ubiquitous computing and smart room
systems tend to have difficulties with privacy and personalization, while
wearable systems have trouble with localized information, localized resource
control, and resource management between multiple people. These difficulties
are discussed and a peer-to-peer network of wearable and ubiquitous computing
components is proposed as a solution. This solution is demonstrated through
several implemented applications. (31 References).
",
 note         = "",
}

------------------------------------------------------------------------------

437

@InProceedings{necula2000a,
 key          = "necula2000a",
 author       = "George C. Necula",
 title        = "Translation Validation for an Optimizing Compiler",
 booktitle    = "ACM SIGPLAN '00 Conference on Programming Language Design and
                 Implementation (PDLI)",
 address      = "Vancouver, BC, Canada",
 month        = "18--21 " # jun,
 year         = "2000",
 pages        = "83--94",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM SIGPLAN",
 abstract = "
We describe a translation validation infrastructure for the GNU C compiler.
During the compilation the infrastructure compares the intermediate form of
the program before and after each compiler pass and verifies the preservation
of semantics. We discuss a general framework that the optimizer can use to
communicate to the validator what transformations were performed. Our
implementation however does not rely on help from the optimizer and it is
quite successful by using instead a few heuristics to detect the
transformations that take place. The main message of the paper is that a
practical translation validation infrastructure, able to check the correctness
of many of the transformations performed by a realistic compiler, can be
implemented with about the effort typically required to implement one compiler
pass. We demonstrate this in the context of the GNU C compiler for a number of
its optimizations while compiling realistic programs such as the compiler
itself or the Linux kernel. We believe that the price of such an
infrastructure is small, considering the qualitative increase in the ability
to isolate compilation errors during compiler testing and maintenance. (25
References).
",
 note         = "",
}

------------------------------------------------------------------------------

438

@Article{wasserman97a,
 key     = "wasserman97a",
 author  = "Hal Wasserman and Manuel Blum",
 title   = "Software Reliability via Run-Time Result-Checking",
 journal = "Journal of the ACM",
 month   = nov,
 year    = "1997",
 volume  = "44",
 number  = "6",
 pages   = "826--49",
 abstract = "
We review the field of result-checking, discussing simple checkers and
self-correctors. We argue that such checkers could profitably be incorporated
in software as an aid to efficient debugging and enhanced reliability. We
consider how to modify traditional checking methodologies to make them more
appropriate for use in real-time, real-number computer systems. In particular,
we suggest that checkers should be allowed to use stored randomness: that is,
that they should be allowed to generate, preprocess, and store random bits
prior to run-time, and then to use this information repeatedly in a series of
run-time checks. In a case study of checking a general real-number linear
transformation (e.g., a Fourier transform), we present a simple checker which
uses stored randomness, and a self-corrector which is particularly efficient
if stored randomness is employed. (41 References).
",
 note    = "",
}

------------------------------------------------------------------------------

439

@Article{blum95a,
 key     = "blum95a",
 author  = "Manuel Blum and Sampath Kannan",
 title   = "Designing Programs That Check Their Work",
 journal = "Journal of the ACM",
 month   = jan,
 year    = "1995",
 volume  = "42",
 number  = "1",
 pages   = "269--91",
 abstract = "
A program correctness checker is an algorithm for checking the output of a
computation. That is, given a program and an instance on which the program is
run, the checker certifies whether the output of the program on that instance
is correct. This paper defines the concept of a program checker. It designs
program checkers for a few specific and carefully chosen problems in the class
FP of functions computable in polynomial time. Problems in FP for which
checkers are presented in this paper include Sorting, Matrix Rank and GCD. It
also applies methods of modern cryptography, especially the idea of a
probabilistic interactive proof, to the design of program checkers for group
theoretic computations. Two structural theorems are proven here. One is a
characterization of problems that can be checked. The other theorem
establishes equivalence classes of problems such that whenever one problem in
a class is checkable, all problems in the class are checkable. (42
References).
",
 note    = "",
}

------------------------------------------------------------------------------

440

@Article{hunt99a,
 key     = "hunt99a",
 author  = "Galen C. Hunt and Michael L. Scott",
 title   = "The {Coign} Automatic Distributed Partitioning System",
 journal = "ACM. Operating Systems Review",
 month   = "Winter",
 year    = "1998",
 volume  = "",
 number  = "",
 pages   = "187--200",
 abstract = "
Although successive generations of middleware (such as RPC, CORBA, and DCOM)
have made it easier to connect distributed programs, the process of
distributed application decomposition has changed little: programmers manually
divide applications into subprograms and manually assign those sub-programs to
machines. Often the techniques used to choose a distribution are ad hoc and
create one-time solutions biased to a specific combination of users, machines,
and networks. We assert that system software, not the programmer, should
manage the task of distributed decomposition. To validate our assertion we
present Coign, an automatic distributed partitioning system that significantly
eases the development of distributed applications. Given an application (in
binary form) built from distributable COM components, Coign constructs a graph
model of the application's inter-component communication through
scenario-based profiling. Later, Coign applies a graph-cutting algorithm to
partition the application across a network and minimize execution delay due to
network communication. Using Coign, even an end user (without access to source
code) can transform a non-distributed application into an optimized,
distributed application. Coign has automatically distributed binaries from
over 2 million lines of application code, including Microsoft's PhotoDraw 2000
image processor. To our knowledge, Coign is the first system to automatically
partition and distribute binary applications. (42 References).
",
 note    = "",
}

------------------------------------------------------------------------------

441

@Article{siegel98a,
 key     = "siegel98a",
 author  = "Jon Siegel",
 title   = "{OMG} Overview: {CORBA} and the {OMA} in Enterprise Computing",
 journal = j-cacm,
 month   = oct,
 year    = "1998",
 pages   = "37--43",
 abstract = "
With an architecture based on the interface definition language OMG/ISO IDL,
the object request broker and the Internet Inter-ORB Protocol (IIOP), CORBA
components interoperate regardless of location, platform, programming
language, system vendor or network. Desirable features such as support for
asynchronous invocation, real-time and embedded systems and fault tolerance
are being added, as is a component model. Building on this architecture, the
Object Management Architecture (OMA) adds the CORBA services, a set of
standardized definitions of components providing services such as object
naming, life cycle, security, transactions, and more. Domain-provided
additions specify frameworks for specialized but industry-standard components.
At the beginning of the software process, the Unified Modeling Language (UML)
and Meta-Object Facility support modeling and design. This environment has
tremendous appeal for virtually any enterprise, but especially for those for
which heterogeneity is an issue. With coordinated support for every phase of
computing from design through implementation to run-time, CORBA integrates
legacy functionality with today's sophisticated hardware and software,
allowing businesses to shop for the best products and integrate them into a
coherent, maintainable architecture. The OMG Web site lists a large number of
success stories of large mission-critical CORBA applications in use at various
enterprises today. The broadly based, growing support for CORBA attests that
the number of success stories will continue to expand rapidly. (6 References).
",
 note    = "",
}

------------------------------------------------------------------------------

442

@Article{ward93a,
 key     = "ward93a",
 author  = "Martin Ward",
 title   = "Abstracting a Specification From Code",
 journal = "Journal of Software Maintenance",
 month   = jun,
 year    = "1993",
 pages   = "101--22",
 abstract = "
Much of the work on developing program transformation systems has concentrated
on systems to assist in program development. However, the four separate
surveys carried out between 1977 and 1990 (Foster and Kiekuth, 1990; Lientz
and Swanson, 1980; Morton, 1988; Nosek and Palvia, 1990), summarized in Foster
(1991), show that between 40% and 60% of all commercial software effort is
devoted to software maintenance rather than the development of new systems.
This paper describes a joint project between the University of Durham and CSM
Ltd to develop a method and tool for reverse-engineering and software
maintenance based on program transformation theory. Presents an example which
illustrates how such a tool can extract a high-level abstract specification
from the low-level source code of a program by a process of formal program
transformation based on a theory of program equivalence (Ward, 1989). All the
code-level reverse-engineering of the example program was carried out on the
prototype tool with the resulting code pasted directly into the paper. (37
References).
",
 note    = "",
}

------------------------------------------------------------------------------

443

@InProceedings{ward94a,
 key          = "ward94a",
 author       = "Martin Ward",
 title        = "Specifications from Source Code---Alchemists' Dream or
                Practical Reality?",
 booktitle    = "IEE Colloquium on ``Reverse Engineering for Software Based
                Systems''",
 address      = "London, UK",
 month        = "10 " # nov,
 year         = "1994",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM SIGPLAN",
 abstract = "
The author describes the application of formal program transformations to
uncover specifications from source code. He takes as an example a small report
writing program. Due to errors in the original design, this program had
several bugs which were gradually uncovered and fixed in the usual way. The
resulting program appears to work, but has a complex and messy structure which
makes it extremely difficult to maintain. The aim of the case study is to
restructure the program and extract its specification (a concise, high-level
description of what the program does which ignores the low-level details of
how this result is achieved). It should be noted that his aim is emphatically
not that of design recovery in the sense of recovering the original design.
Instead he aims to transform the base metal of unstructured code into the gold
of a high-level specification. The approach is based on a Wide Spectrum
Language (called WSL) which includes both high level abstract specifications
and low-level programming constructs within the same language. (13
References).
",
 note         = "",
}

------------------------------------------------------------------------------

445

@Article{weiser93a,
 key     = "weiser93a",
 author  = "Mark Weiser",
 title   = "Some Computer Science Issues in Ubiquitous Computing",
 journal = j-cacm,
 month   = jul,
 year    = "1993",
 pages   = "74--84",
 abstract = "
Ubiquitous computing enhances computer use by making many computers available
throughout the physical environment, while making them effectively invisible
to the user. This article explains what is new and different about the
computer science involved in ubiquitous computing. First, it provides a brief
overview of ubiquitous computing, then elaborates through a series of examples
drawn from various subdisciplines of computer science: hardware components
(e.g. chips), network protocols, interaction substrates (e.g. software for
screens and pens), applications, privacy, and computational methods.
Ubiquitous computing offers a framework for new and exciting research across
the spectrum of computer science. (28 References).
",
 note    = "",
}

------------------------------------------------------------------------------

446

@Article{vanommering2000a,
 key     = "vanommering2000a",
 author  = "Rob van Ommering and Frank van der Linden and Jeff Kramer and Jeff
            Magee",
 title   = "Koala Component Model for Consumer Electronics Software",
 journal = j-comp,
 month   = mar,
 year    = "2000",
 volume  = "33",
 number  = "3",
 pages   = "78--85",
 abstract = "
Most consumer electronics today contain embedded software. In the early days,
developing CE software presented relatively minor challenges, but in the past
several years three significant problems have arisen: size and complexity of
the software in individual products; the increasing diversity of products and
their software; and the need for decreased development time. The question of
handling diversity and complexity in embedded software at an increasing
production speed becomes an urgent one. The authors present their belief that
the answer lies not in hiring more software engineers. They are not readily
available, and even if they were, experience shows that larger projects induce
larger lead times and often result in greater complexity. Instead, they
believe that the answer lies in the use and reuse of software components that
work within an explicit software architecture. The Koala model, a
component-oriented approach detailed in this article, is their way of handling
the diversity of software in consumer electronics. Used for embedded software
in TV sets, it allows late binding of reusable components with no additional
overhead. (7 References).
",
 note    = "",
}

------------------------------------------------------------------------------

447

@Article{ernst2001,
 key     = "ernst2001",
 author  = "Michael D. Ernst and Jake Cockrell and William G. Griswold and
           David Notkin",
 title   = "Dynamically Discovering Likely Program Invariants to Support
           Program Evolution",
 journal = j-tse,
 month   = feb,
 year    = "2001",
 volume  = "27",
 number  = "2",
 pages   = "1--25",
 abstract = "
Explicitly stated program invariants can help programmers by identifying
program properties that must be preserved when modifying code. In practice,
however, these invariants are usually implicit. An alternative to expecting
programmers to fully annotate code with invariants is to automatically infer
likely invariants from the program itself. This research focuses on dynamic
techniques for discovering invariants from execution traces. \par
This article reports three results. First, it describes techniques for
dynamically discovering invariants, along with an implementation, named
Daikon, that embodies these techniques. Second, it reports on the application
of Daikon to two sets of target programs. In programs from Gries's work on
program derivation, the system rediscovered predefined invariants. In a C
program lacking explicit invariants, the system discovered invariants that
assisted a software evolution task. These experiments demonstrate that, at
least for small programs, invariant inference is both accurate and useful.
Third, it analyzes scalability issues such as invariant detection runtime and
accuracy as functions of test suites and program points instrumented.
",
 note    = "",
}

------------------------------------------------------------------------------

448

@InProceedings{arthan98a,
 key          = "arthan98a",
 author       = "R. D. Arthan",
 title        = "Recursive definitions in {Z}",
 booktitle    = "ZUM '98: The Z Formal Specification Notation. 11th
                 International Conference of Z Users. Proceedings",
 address      = "Berlin, Germany",
 month        = "24--26 " # sep,
 year         = "1998",
 pages        = "154--71",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
The paper considers some issues in the theory and practice of defining
functions over recursive data types in Z. Principles justifying such
definitions are formulated. Z free types (R. D. Arthan, 1992; A. Smith, 1991;
J. M. Spivey, 1996) are contrasted with the free algebras of universal algebra:
the notions turn out to be related but not isomorphic. (13 References).
",
 note         = "",
}

------------------------------------------------------------------------------

449

@InProceedings{engler2000a,
 key          = "engler2000a",
 author       = "Dawson Engler and Benjamin Chelf and Andy Chou and
                 Seth Hallem",
 title        = "Checking System Rules Using System-Specific,
                 Programmer-Written Compiler Extensions",
 booktitle    = "Symposium on Operating Systems Design and Implementation
                 (OSDI 2000)",
 address      = "San Diego, CA",
 month        = "23--25 " # oct,
 year         = "2000",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Systems software such as OS kernels, embedded systems, and libraries must obey
many rules for both correctness and performance. Common examples include
``accesses to variable A must be guarded by lock B,'' ``system calls must
check user pointers for validity before using them,'' and ``message handlers
should free their buffers as quickly as possible to allow greater
parallelism.'' Unfortunately, adherence to these rules is largely unchecked.
\par
This paper attacks this problem by showing how system implementors can use
meta-level compilation (MC) to write simple, system-specific compiler
extensions that automatically check their code for rule violations. By melding
domain-specific knowledge with the automatic machinery of compilers, MC brings
the benefits of language-level checking and optimizing to the higher, ``meta''
level of the systems implemented in these languages. This paper demonstrates
the effectiveness of the MC approach by applying it to four complex, real
systems: Linux, OpenBSD, the Xok exokernel, and the FLASH machine's embedded
software. MC extensions found roughly 500 errors in these systems and led to
numerous kernel patches. Most extensions were less than a hundred lines of
code and written by implementors who had a limited understanding of the
systems checked. 
",
 note         = "",
}

------------------------------------------------------------------------------

450

@InProceedings{evans99a,
 key          = "evans99a",
 author       = "Andy Evans and Robert France and Kevin Lano and Bernhard
                 Rumpe",
 title        = "The {UML} as a Formal Modeling Notation",
 booktitle    = "The Unified Modeling Language---Workshop UML'98: Beyond the
                 Notation",
 address      = "Mulhouse, France",
 month        = "3--4 " # jun,
 year         = "1998",
 pages        = "336--48",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
The Unified Modeling Language (UML) is rapidly emerging as a de-facto
standard for modelling OO systems. Given this role, it is imperative
that the UML needs a well-defined, fully explored semantics. Such
semantics is required in order to ensure that UML concepts are
precisely stated and defined. In this paper we motivate an approach to
formalizing UML in which formal specification techniques are used to
gain insight into the semantics of UML notations and diagrams and
describe a roadmap for this approach.  The authors initiated the
Precise UML (PUML) group in order to develop a precise semantic model
for UML diagrams.  The semantic model is to be used as the basis for a
set of diagrammatical transformation rules, which enable formal
deductions to be made about UML diagrams. A small example shows how
these rules can be used to verify whether one class diagram is a valid
deduction of another. Because these rules are presented at the
diagrammatical level, it will be argued that UML can be successfully
used as a formal modelling tool without the notational complexities
that are commonly found in textual specification techniques.
",
 note         = "URL: {\urlBiBTeX{http://www4.informatik.tu-muenchen.de/papers/EFLR98d.html}}",
}

------------------------------------------------------------------------------

@Misc{puml2000a,
 key    = "puml2000a",
 author = "The Precise UML Group",
 title  = "The Precise {UML} Group Home Page",
 url    = "http://www.cs.york.ac.uk/puml/",
 abstract = "
The precise UML (pUML) group aims to bring together international
researchers and practitioners who share the aim of developing the
Unified Modelling Language (UML) as a precise (i.e. well defined)
modelling language. It is particularly concerned with the development
of new theories and practices required to: (1) clarify and make
precise the semantics of UML, (2) reason with properties of UML
models, (3) verify the correctness of UML designs, and (4) construct
tools to support the rigorous application of UML.
",
 note   = "URL: {\urlBiBTeX{http://www.cs.york.ac.uk/puml/}}",
}

------------------------------------------------------------------------------

@Book{harbison94a,
 key       = "harbison94a",
 author    = "Samuel P. Harbison and Guy L. Steele",
 title     = "{C}, A Reference Manual",
 publisher = "Prentice Hall",
 edition   = "4th",
 month     = oct,
 year      = "1994",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@InProceedings{sullivan96d,
 key          = "sullivan96d",
 author       = "K. J. Sullivan and J. C. Knight",
 title        = "Building Programs from Massive Components",
 booktitle    = "Proceedings of the 21st Annual Software Engineering
                 Workshop",
 address      = "Greenbelt, MD",
 month        = "4-5 " # dec,
 year         = "1996",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
The problem faced by many of today's software engineers is to build
and maintain broad families of large systems in a cost-effective and
timely manner. Because the market demands rapid creation and
modification of systems in response to a spectrum of evolving
requirements, extensive flexibility in systems is required. This
situation has two implications: first, basic system demands have to be
met quickly; and, second, responses to requested variations have to be
rapid and effective. System development and modification cycle time
must be shortened significantly. 
\par
On the basis of some experimental systems work, we suggest that a
relatively new approach might merit increased attention from the
research community. The approach is based on the integration of large,
application-scale, binary components. To date the approach has been
employed industrially using shrink-wrapped packages, such as Microsoft
Office and Visio Corporation's Visio technical drawing tool, mostly
for business and office automation tasks.  We have shown that this
approach can be applied more aggressively, using today's technology
combined with advanced integration strategies such as mediators, to
develop systems in at least one domain far removed from business data
processing, quickly and at low cost. Our demonstration application is
a fault-tree analysis tool embodying new analysis techniques developed
by Joanne Bechta Dugan at the University of Virginia. 
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

@Misc{microsoft2001a,
 key    = "microsoft2001a",
 author = "Microsoft",
 title  = "Active Document Containers",
 url    = "http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vccore98/HTML/_core_activex_document_containers.asp",
 abstract = "
An active document container allows you to work with several documents
of different application types within a single frame (instead of
forcing you to create and use multiple application frames for each
document type).
",
 note   = "URL: {\urlBiBTeX{http://msdn.microsoft.com/library/default.asp?url=/library/en-us/vccore98/HTML/_core_activex_document_containers.asp}}",
}

------------------------------------------------------------------------------

451

@Article{holzmann2001a,
 key     = "holzmann2001a",
 author  = "Gerard J. Holzmann",
 title   = "The {Spin} Model Checker",
 journal = j-tse,
 month   = may,
 year    = "1997",
 volume  = "23",
 number  = "5",
 pages   = "279--95",
 abstract = "
SPIN is an efficient verification system for models of distributed
software systems. It has been used to detect design errors in
applications ranging from high-level descriptions of distributed
algorithms to detailed code for controlling telephone exchanges. This
paper gives an overview of the design and structure of the verifier,
reviews its theoretical foundation, and gives an overview of
significant practical applications.
",
 note    = "",
}

------------------------------------------------------------------------------

@Misc{graphviz2001,
 key    = "graphviz2001",
 author = "John Ellson and Emden Gansner and Eleftherios Koutsofios and
           Stephen North",
 title  = "GraphViz",
 url    = "http://www.research.att.com/sw/tools/graphviz",
 abstract = "
graphviz is a set of graph drawing tools for Unix or MS-Windows
(win32), including a web service interface (webdot). Source code and
binary executables for common platforms are available. Graph drawing
addresses the problem of visualizing structural information by
constructing geometric representations of abstract graphs and
networks. Automatic generation of graph drawings has important
applications in key technologies such as database design, software
engineering, VLSI and network design and visual interfaces in other
domains. 
",
 note   = "URL: {\urlBiBTeX{http://www.research.att.com/sw/tools/graphviz}}",
}

------------------------------------------------------------------------------

@Book{prosise1999a,
 key       = "prosise1999a",
 author    = "Jeff Prosise",
 title     = "Programming Windows With {MFC}",
 publisher = "Microsoft Press",
 edition   = "",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

in proceedings

@InProceedings{lamsweerde2000a,
 key          = "lamsweerde2000a",
 author       = "Axel {van Lamsweerde}",
 title        = "Formal Specification: a Roadmap",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering---The Future of Software Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11 " # jun,
 year         = "2000",
 pages        = "149--59",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We present Galileo, an innovative engineering modeling and analysis tool built
using an approach we call package-oriented programming (POP). Galileo
represents an ongoing evaluation of the POP approach, where multiple large,
architecturally coherent components are tightly integrated in an overall
software system. Galileo utilizes Microsoft Word, Internet Explorer, and Visio
to provide a low cost, richly functional fault tree modeling superstructure.
Based on the success of previous prototypes of the tool, we are now building a
version for industrial use under an agreement with NASA Langley Research
Center.
",
 note         = "",
}

------------------------------------------------------------------------------

@Misc{spivey2000a,
 key    = "spivey2000a",
 author = "Mike Spivey",
 title  = "The {fuzz} Manual",
 url    = "http://spivey.oriel.ox.ac.uk/~mike/fuzz/",
 abstract = "
fuzz is a collection of tools that help you to format and print Z
specications and check them for compliance with the Z scope and type
rules.  One part of the package is a style option for the LaTEX
type-setting system that defines extra LaTEX commands for laying out Z
specifications, and a font that contains Z's special symbols. The other
part is a program for analysing and checking specifications that are
written using these commands.
",
 note   = "URL: {\urlBiBTeX{http://spivey.oriel.ox.ac.uk/~mike/fuzz/}}",
}

------------------------------------------------------------------------------

@Misc{jia98a,
 key    = "jia98a",
 author = "Xiaoping Jia",
 title  = "{ZTC}: A Type Checker for {Z}. Notation User's Guide",
 url    = "http://se.cs.depaul.edu/fm/ztc.html",
 abstract = "
Z is a non-executable but strongly-typed specification language. ZTC
is a type-checker for Z, which determines if there are syntactical and
typing errors in Z specifications. There is no compiler for Z.
However, there are tools to animate, or execute, subsets of Z. 
",
 note   = "URL: {\urlBiBTeX{http://se.cs.depaul.edu/fm/ztc.html}}",
}

------------------------------------------------------------------------------

00902447.pdf

@InProceedings{zhu2001a,
 key          = "zhu2001a",
 author       = "Hiahong Zhu and Shixao Zhou and Joanne Bechta Dugan
                 and Kevin J. Sullivan",
 title        = "A Benchmark for Quantitative Fault Tree Reliability
                 Analysis",
 booktitle    = "Annual Reliability and Maintainability Symposium 2001
                 Proceedings",
 address      = "Philadelphia, PA",
 month        = "22--25 " # jan,
 year         = "2001",
 pages        = "86--93",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
Reliability analysis of critical computer-based systems is often
performed using software tools, typically using fault tree analysis.
Many software tools are available, either commercially or from
university research groups, and each tool uses different solution
techniques, ranging from fast approximations to more complex analysis
such as Markov analysis or binary decision diagrams. Analysts are thus
faced with the difficult task of validating a tool which is being
considered for use in a real application. One approach to verification
might be to select a set of representative examples, and compare the
tool results with some known set of ``correct'' solutions. However, the
development of a reliable benchmark can be time consuming, and the
determination of the correct result may be very difficult. It would
therefore be very useful if there was a published benchmark, with the
correct solutions, from which the analyst could select test cases.
Towards this goal the authors present a set of example systems and
their analysis. They use these examples to help validate theirr
dynamic fault tree analysis tool, Galileo. Their goal is to gather a
set of representative examples which are reasonably challenging and
which are derived from real systems. 
",
 note         = "",
}

------------------------------------------------------------------------------

00641975.pdf

@Article{clark98a,
 key     = "clark98a",
 author  = "David D. Clark and Edward A. Feigenbaum and Juris
            Hartmanis and Robert W. Lucky and Robert M. Metcalfe and
            Raj Reddy and Mary Shaw",
 title   = "Innovation and Obstacles: The Future of Computing",
 journal = j-comp,
 month   = jan,
 year    = "1998",
 volume  = "31",
 number  = "1",
 pages   = "29--38",
 abstract = "
In this multidisciplinary glimpse forward, some of this decade's key
players offer opinions on a range of topics.from what has driven
progress, to where innovation will come from, and to obstacles we have
yet to overcome.
",
 note    = "",
}

------------------------------------------------------------------------------

@Book{dillman1999a,
 key       = "dillman1999a",
 author    = "Don A. Dillman",
 title     = "Mail and Internet Surveys: The Tailored Design Method",
 publisher = "John Wiley \& Sons",
 edition   = "2nd",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

lfm.97.pdf

@InProceedings{knight97b,
 key          = "knight97b",
 author       = "John C. Knight and Colleen L. DeJong and Matthew S. Gibble
 and {Lu\'is} Nakano",
 title        = "Why Are Formal Methods Not Used More Widely?",
 booktitle    = "Fourth NASA Formal Methods Workshop",
 month        = "" # sep,
 year         = "1997",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Hampton, Virginia",
 abstract = "
Despite extensive development over many years and significant demonstrated
benefits, formal methods remain poorly accepted by industrial practitioners.
Many reasons have been suggested for this situation such as a claim that they
extent the development cycle, that they require difficult mathematics, that
inadequate tools exist, and that they are incompatible with other software
packages. There is little empirical evidence that any of these reasons is
valid. The research presented here addresses the question of why formal
methods are not used more widely. The approach used was to develop a formal
specification for a safety-critical application using several specification
notations and assess the results in a comprehensive evaluation framework.  The
results of the experiment suggests that there remain many impediments to the
routine use of formal methods.
",
 note         = "",
}

------------------------------------------------------------------------------

@InBook{grundy2001a,
 key          = "grundy2001a",
 author       = "John Grundy and John Hosking",
 title        = "Wiley Encyclopaedia of Software Engineering",
 chapter      = "Software Engineering",
 pages        = "",
 publisher    = "Wiley InterScience",
 year         = "2001",
 volume       = "",
 series       = "",
 address      = "",
 edition      = "2nd",
 month        = "",
 remark       = "",
 type         = "",
 abstract = "
Software is growing ever-more complex and new software processes, methods and
products put greater demands on software engineers than ever before. The
support of appropriate software tools is essential for developers to maximise
their ability to effectively and efficiently deliver quality software
products. This article surveys current practice in the software tools area,
along with recent and expected near-future trends in software tools
development. We provide a summary of tool applications during the software
lifecycle, but focus on particular aspects of software tools that have changed
in recent years and are likely to change in the near future as tools continue
to evolve. These include the internal structure of tools, provision of
multiple view interfaces, tool integration techniques, collaborative work
support and the increasing use of automated assistance within tools.  We hope
this article will both inform software engineering practitioners of current
research trends, and tool researchers of the relevant state-of-the-art in
commercial tools and various likely future research trends in tools
development.
",
 note         = "",
}

------------------------------------------------------------------------------

@Article{grundy2000a,
 key     = "grundy2000a",
 author  = "John Grundy and Warwick Mugridge and John Hosking",
 title   = "Constructing Component-based Software Engineering
            Environments: Issues and Experiences",
 journal = "Information and Software Technology",
 month   = "",
 year    = "2000",
 volume  = "42",
 number  = "2",
 pages   = "",
 abstract = "
Developing software engineering tools is a difficult task, and the
environments in which these tools are deployed continually evolve as software
developers. processes, tools and tool sets evolve. To more effectively develop
such evolvable environments, we have been using component-based approaches to
build and integrate a range of software development tools, including CASE and
workflow tools, file servers and versioning systems, and a variety of reusable
software agents. We describe the rationale for a component-based approach to
developing such tools, the architecture and support tools we have used, some
resultant tools and tool facilities we have developed, and summarise possible
future research directions in this area.
",
 note    = "",
}

------------------------------------------------------------------------------

@InProceedings{knight2002a,
 key          = "knight2002a",
 author       = "J. C. Knight",
 title        = "Safety Critical Systems: Challenges and Directions",
 booktitle    = "Proceedings of the 24th International Conference on Software
                 Engineering",
 address      = "Orlando, Florida",
 month        = "19--25  " # may,
 year         = "2002",
 pages        = "547--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Safety-critical systems are those systems whose failure could result in loss
of life, significant property damage, or damage to the environment. There are
many well known examples in application areas such as medical devices,
aircraft flight control, weapons, and nuclear systems. Many modern information
systems are becoming safety-critical in a general sense because financial loss
and even loss of life can result from their failure. Future safety-critical
systems will be more common and more powerful. From a software perspective,
developing safety critical systems in the numbers required and with adequate
dependability is going to require significant advances in areas such as
specification, architecture, verification, and process. The very visible
problems that have arisen in the area of information-system security suggests
that security is a major challenge also. 
",
 note         = "",
}

------------------------------------------------------------------------------

@Misc{metacase2002a,
 key    = "metacase2002a",
 author = "{MetaCase} Consulting",
 title  = "Domain-Specific Modeling: 10 Times Faster Than {UML}",
 url    = "http://www.metacase.com/papers/index.html",
 abstract = "
Current development methods are based on the code world, leading to a
poor mapping to organisations' own domains and duplication of effort in
problem-solving, design and coding. Developing an in-house, domain-specific
method allows faster development, based on models of the product
rather than on models of the code. Industrial applications of this approach
show remarkable improvements in productivity and training time: up to ten
times faster in both cases. In this paper we describe why domain-specific
modelling is faster than current development methods and how to build a
domain-specific modelling language.
",
 note   = "URL: {\urlBiBTeX{http://www.metacase.com/papers/index.html}}",
}

------------------------------------------------------------------------------

@Misc{honeywell2002a,
 key    = "honeywell2002a",
 author = "Honeywell",
 title  = "{DOME} Users' Guide",
 url    = "http://www.htc.honeywell.com/dome/support.htm",
 abstract = "
DOME is an extensible system for graphically developing, analyzing and
transforming models of systems and software.  DOME comes with a pre-built set
of notations which can be used ``out of the box'' including UML, Coad-Yourdon
OOA, Colbert OOSD, IDEF0, and Petri-Nets. The core of DOME, however, is its
capability to develop new notations.
",
 note   = "URL: {\urlBiBTeX{http://www.htc.honeywell.com/dome/support.htm}}",
}

------------------------------------------------------------------------------

451, r2057.pdf

@Article{parnas96a,
 key     = "parnas96a",
 author  = "David Lorge Parnas",
 title   = "Why Software Jewels Are Rare",
 journal = j-comp,
 month   = feb,
 year    = "1996",
 volume  = "29",
 number  = "2",
 pages   = "57--60",
 abstract = "
Can ``lean'' software compete in the marketplace? Can useful software be
elegant? The author laments that a real jewel of a program--written in a
consistent style and free of kludges, with simple, organized components--is
hard to find. He also wonders why, after more than 30 years of software
construction, the industry doesn't produce more jewels. Creators of elegant
systems sometimes write articles to tell how they wrote the software and to
suggest how others should use their methods. The author points to the T.H.E.
system (named for the Technicsche Hogeschool Eindhoven, where the system was
built), which has served him as a source of new ideas and insight for 25
years. He also refers to Niklaus Wirth's recent publications (see Computer,
February 1995) as musts for every software designer. However, if published
papers do indeed contain the secret of success, where are the jewels? Admired
systems are often produced under conditions that are rare in industry. In
particular, their designers are free from the constraints limiting those who
must sell their products. The author discusses why the recipes of the masters
haven't led to more elegant commercial software and closes with some advice
for those who would like to produce better software. 
",
 note    = "",
}

------------------------------------------------------------------------------

452, rx065.pdf

@Article{leveson94a,
 key     = "leveson94a",
 author  = "Nancy G. Leveson",
 title   = "High-Pressure Steam Engines And Computer Software",
 journal = j-comp,
 month   = oct,
 year    = "1994",
 volume  = "27",
 number  = "10",
 pages   = "65--73",
 abstract = "
The introduction of computers into the control of potentially dangerous
devices has led to a growing awareness of the possible contribution of
software to serious accidents. The number of computer-related accidents so far
has been small due to the restraint shown in introducing computers into
safety-critical control loops. However, as the economic and technological
benefits of using computers become more widely accepted, their use is
increasing dramatically. We need to ensure that computers are introduced into
safety-critical systems in the most responsible way possible and at a speed
that does not expose people to undue risk. Risk induced by technological
innovation existed long before computers; this is not the first time that
humans have come up with an extremely useful new technology that is
potentially dangerous. Studying parallels in the early development of
high-pressure steam engines and of software engineering can help.
",
 note    = "",
}

------------------------------------------------------------------------------

453

@Article{hazelrigg97a,
 key     = "hazelrigg97a",
 author  = "G. A. Hazelrigg",
 title   = "On Irrationality In Engineering Design",
 journal = "Journal of Mechanical Design",
 month   = jun,
 year    = "1997",
 volume  = "119",
 number  = "",
 pages   = "194--6",
 abstract = "
It is well recognized that virtually all engineering design is suboptimal. The
consequence of suboptimal design is that the design can be improved against
the desired objective measure, with the result that it fails to provide full
potential benefit and it opens the door to competition. In this paper, we
define the concept of irrationality in design. Irrationality is a source of
suboptimal design that can be addressed through proper project management. It
occurs when design engineers make design decisions against different objective
measures, with the result that the end esign can be suboptimal with repsect to
all conceivable objective measures of a specified class.
",
 note    = "",
}

------------------------------------------------------------------------------

454

@Article{hazelrigg96a,
 key     = "hazelrigg96a",
 author  = "G. A. Hazelrigg",
 title   = "The Impliciations of Arrow's Impossibility Theorem on Approaches
            to Optimal Engineering Design",
 journal = "Journal of Mechanical Design",
 month   = jun,
 year    = "1996",
 volume  = "118",
 number  = "",
 pages   = "161--4",
 abstract = "
Many modern approaches to engineering design seek to optimize design in order
to maximize the value of the system to its customers. These approaches rely on
the formulation of a system utility function as a measure of system worth. It
is shown here that, under certain curcumstances, however, such a measure
cannot exist. It is then indicated that these circumstances comprise the rule
rather than the exception. Finally it is shown that pursuing the objective of
design optimization as defined by the customers via contemporary approaches
can lead the designer to highly inappropriate and undesirable designs. As a
conseqeuence of this, it becomes apparent that the methods of Total Quality
Management (TQM) and Quality Function Deployment (QFD) can lead to highly
erroneous results.
",
 note    = "",
}

------------------------------------------------------------------------------

455

@Article{hazelrigg98a,
 key     = "hazelrigg98a",
 author  = "G. A. Hazelrigg",
 title   = "A Framework for Decision-Based Engineering Design",
 journal = "Journal of Mechanical Design",
 month   = jun,
 year    = "1998",
 volume  = "120",
 number  = "",
 pages   = "653--8",
 abstract = "
Engineering design is increasingly recognized as a decision-making process.
This recognition brings with it the richness of many well-developed theories
and methods from economics, operations research, decision sciences, and other
disciplines. Done correctly, it forces the process of engineering design into
a total systems context, and demands that design decisions account for a
product's total life cycle. It also provides a theory of design that is based
on a rigorous set of axioms that underlie value theory. But the rigor of
decision-based design also places stringent conditions on the process of
engineering design that eliminate popular approaches such as Quality Function
Deployment. This paper presents the underlying notations of decision-based
design, points to some of the axioms that underlie the theory of
decision-based design, and discusses the consequences of the theory on
engineering education.
",
 note    = "",
}

------------------------------------------------------------------------------

456, siv.pdf

@InProceedings{sivzattian2001a,
 key          = "sivzattian2001a",
 author       = "Siv Sivzattian and Bashare Nuseibeh",
 title        = "Calibrating Value Estimates of Requirements",
 booktitle    = "Third International Workshop on Economics-Driven Software
                 Engineering Research ",
 month        = "14--15 " # may,
 year         = "2001",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Toronto, Canada",
 abstract = "
Selecting requirements for implementation of software applications depends on
the subjective judgements of the stakeholders who participate in the task. We
claim that portfolio analysis provides a market driven, systematic, and more
objective approach to supplement the selection of requirements, and also
accounts for uncertainty and incomplete knowledge in the real world. We
illustrate through two examples, how portfolio-based reasoning facilitates
calibrating . that is, aligning . our value estimates of requirements with
capital market valuations.
",
 note         = "",
}

------------------------------------------------------------------------------

457, paulson.pdf

@InProceedings{succi2001a,
 key          = "succi2001a",
 author       = "Giancarlo Succi and James Paulson and Armin Eberlein",
 title        = "Preliminary Results from an Empirical Study on the Growth of
                 Open Source and Commercial Software Products",
 booktitle    = "Third International Workshop on Economics-Driven Software
                 Engineering Research ",
 month        = "14--15 " # may,
 year         = "2001",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Toronto, Canada",
 abstract = "
This article describes an empirical study of the growth and evolution of three
open source and three commercial software projects. This study identifies
commonality and differences in the growth of these projects, and discusses how
these results may be useful in planning software development strategies.
",
 note         = "",
}

------------------------------------------------------------------------------

458

@Article{parnas85b,
 key     = "parnas85b",
 author  = "David Lorge Parnas",
 title   = "Software Aspects of Strategic Defense Systems",
 journal = j-cacm,
 month   = dec,
 year    = "1985",
 volume  = "28",
 number  = "12",
 pages   = "1326--35",
 abstract = "
The author, a former member of the SDIO Panel on Computing in Support of
Battle Management, explains why he believes the 'star wars' effort will not
achieve its stated goals. This report comprises eight short papers that were
completed while he was a member of the Panel on Computing in Support of Battle
Management, convened by the Strategic Defense Initiative Organization (SDIO).
The panel was asked to identify the computer science problems that would have
to be solved before an effective antiballistic missile (ABM) system could be
deployed. It is clear to everyone that computers must play a critical role in
the systems that SDIO is considering. The essays that constitute this report
were written to organize thoughts on these topics and were submitted to SDIO
with his resignation from the panel.
",
 note    = "",
}

------------------------------------------------------------------------------

459

@InProceedings{parnas78a,
 key          = "parnas78a",
 author       = "David Parnas",
 title        = "On a ``Buzzword'': Hierarchical Structure",
 booktitle    = "Proceedings of the IFIP Congress 1974",
 month        = "",
 year         = "1978",
 pages        = "335--42",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "North Holland Publishing Co.",
 address      = "",
 abstract = "
This paper discusses the use of the term ``hierarchically structured'' to
describe the design of operating systems. Although the various uses of this
term are often considered to be closely related, close examination of the use
of the term shows that it has a number of quite different meanings. For
example, one can find two different senses of ``hierarchy'' in a single
operating system. An understanding of the different meanings of the term is
essential, if a designer wishes to apply recent work in software engineering
and design methodology. This paper attempts to provide such an understanding. 
",
 note         = "",
}

------------------------------------------------------------------------------

460

@Article{dijkstra83a,
 key     = "dijkstra83a",
 author  = "Edsger W. Dijkstra",
 title   = "The Structure of ``{THE}''-Multiprogramming System",
 journal = j-cacm,
 month   = may,
 year    = "1983",
 volume  = "11",
 number  = "5",
 pages   = "341--346",
 abstract = "
A multiprogramming system is described in which all activities are divided
over a number of sequential processes. These sequential processes are placed
at various hierarchical levels, in each of which one or more independent
abstractions have been implemented. The hierarchical structure proved to be
vital for the verification of the logical soundness of the design and
correctneess of its implementation.
",
 note    = "",
}

------------------------------------------------------------------------------

461,acm00.ps

@InProceedings{lutz2000a,
 key          = "lutz2000a",
 author       = "Robyn R. Lutz",
 title        = "Software Engineering for Safety: A Roadmap",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering---The Future of Software Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11 " # jun,
 year         = "2000",
 pages        = "213--26",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Many safety-critical systems rely on software to achieve their purposes. The
number of such systems increases as additional capabilities are realized in
software.  Miniaturization and processing improvements have enabled the spread
of safety-critical systems from nuclear and defense applications to domains as
diverse as implantable medical devices, traffic control, smart vehicles, and
interactive virtual environments. Future technological advances and consumer
markets can be expected to produce more safety-critical applications. To meet
this demand is a challenge. One of the major findings in a recent report by
the President's Information Technology Advisory Committee [1999] was, ``The
Nation depends on fragile software.'' Safety is a system problem [Leveson
1995; McDermid 1996]. Software can contribute to a system's safety or can
compromise it by putting the system into a dangerous state. Software
engineering of a safety-critical system thus requires a clear understanding of
the software's role in, and interactions with, the system. 
",
 note         = "",
}

------------------------------------------------------------------------------

462,08810128.pdf

@InProceedings{gannod2000a,
 key          = "gannod2000a",
 author       = "Gerald C. Gannod and Sudhakiran V. Mudiam and Timothy E.
                 Lindquist",
 title        = "An Architecture-Based Approach for Synthesizing and
                 Integrating Adapters for Legacy Software",
 booktitle    = "Proceedings of the Seventh Working Conference on Reverse
                 Engineering",
 address      = "Brisbane, Qld., Australia",
 month        = "23--25 " # nov,
 year         = "2000",
 pages        = "128--37",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
In software organizations there is a very real possibility that a commitment
to existing assets will require migration of legacy software towards new
environments that use modern technology. One technique that has been suggested
for facilitating the migration of existing legacy assets to new platforms is
via the use of the adapter design pattern, also known as component wrapping.
We describe an approach for facilitating the integration of legacy software
into new applications using component wrapping. That is, we demonstrate the
use of a software architecture description language as a means for specifying
various properties that can be used to assist in the construction of wrappers.
In addition, we show how these wrapped components can be used within a
distributed object infrastructure as services that are dynamically integrated
at runtime.
",
 note         = "",
}

------------------------------------------------------------------------------

463

@Article{luckham85a,
 key     = "luckham85a",
 author  = "David C. Luckham and Friedrich W. {von Henke}",
 title   = "Overview of {Anna}, a Specification Language for {Ada}",
 journal = j-soft,
 month   = mar,
 year    = "1985",
 volume  = "2",
 number  = "2",
 pages   = "9-224",
 abstract= "
The authors discuss some of the features of Anna and the ideas behind their
design. The Anna approach to specification language design strengthens the
existing explanatory constructs in Ada and adds new ones where there is an
obvious need. Annotations are examined in detail, and the semantics and
implementation of Anna are discussed.
",
 note    = "",
}

------------------------------------------------------------------------------

464

@Article{klepper95a,
 key     = "klepper95a",
 author  = "Robert Klepper and Douglas Bock",
 title   = "Third and Fourth Generation Language Productivity Differences",
 journal = j-cacm,
 month   = sep,
 year    = "1995",
 volume  = "38",
 number  = "9",
 pages   = "69--79",
 abstract = "
This article reports the results of an empirical study of the impact of fourth
generation languages (4GLs) on system development productivity relative to
third generation languages (3GLs). Improving software development productivity
has long been a major objective of the information systems community, and the
large number of technological and managerial productivity improvement
strategies reflect the importance of this objective.
",
 note    = "",
}

------------------------------------------------------------------------------

465

@Article{liu95a,
 key     = "liu95a",
 author  = "Shaoying Liu and Victoria Stavridou and Bruno Dutertre",
 title   = "The Practice of Formal Methods in Safety Critical Systems",
 journal = j-jss,
 month   = jan,
 year    = "1995",
 volume  = "28",
 number  = "1",
 pages   = "77--87",
 abstract = "
By describing several industrial-scale applications of formal methods, we
demonstrate that formal methods for software development and safety analysis
are being increasingly adopted in the safety-critical systems sector. The
benefits and limitations of formal methods are described, and the problems in
developing software for safety-critical systems are analyzed.
",
 note    = "",
}

------------------------------------------------------------------------------

466

@Article{helmbold85a,
 key     = "helmbold85a",
 author  = "David Helmbold and David Luckham",
 title   = "Debuggin {Ada} Tasking Programs",
 journal = j-soft,
 month   = mar,
 year    = "1985",
 volume  = "2",
 number  = "2",
 pages   = "47--59",
 abstract = "
The authors describe the debugging facilities for dealing with deadness errors
in an experimental monitor. Similar facilities could be implemented, using
these techniques, in task debuggers of future Ada programming support
environments. They then give an example of the monitor's use on a tasking
program, a simplified model of activities in a gas station. This example
illustrates both the strong and weak points in the current monitor. Finally
they describe several possible enhancements to the monitor's debugging
facilities. These enhancements not only improve the monitor's reporting of
dead states, but also allow a wide class of task sequencing errors to be
detected and diagnosed.
",
 note    = "",
}

------------------------------------------------------------------------------

467

@Article{hansen98a,
 key     = "hansen98a",
 author  = "Kirsten M. Hansen and Anders P. Ravn and Victoria Stavridou",
 title   = "From Safety Analysis to Software Requirements",
 journal = j-tse,
 month   = jul,
 year    = "1998",
 volume  = "24",
 number  = "7",
 pages   = "573--84",
 abstract= "
Software for safety critical systems must deal with the hazards identified by
safety analysis. This paper investigates, how the results of one safety
analysis technique, fault trees, are interpreted as software safety
requirements to be used in the program design process. We propose that fault
tree analysis and program development use the same system model. This model is
formalized in a real-time, interval logic, based on a conventional dynamic
systems model with state evolving over time. Fault trees are interpreted as
temporal formulas, and it is shown how such formulas can be used for deriving
safety requirements for software components. 
",
 note    = "",
}

------------------------------------------------------------------------------

468,CIA9.ps

@InProceedings{butler93b,
 key          = "butler93b",
 author       = "Ricky W. Butler",
 title        = "Formal Methods for Life-Critical Software",
 booktitle    = "AIAA Computing in Aerospace 9 Conference",
 address      = "San Diego, CA",
 month        = "19--21 " # oct,
 year         = "1993",
 pages        = "319--29",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
The use of computer software in life-critical applications, such as for civil
air transports, demands the use of rigorous formal mathematical verification
procedures. This paper demonstrates how to apply formal methods to the
development and verification of software by leading the reader step-by-step
through requirements analysis, design, implementation, and verification of an
electronic phone book application. The current maturity and limitations of
formal methods tools and techniques are then discussed, and a number of
examples of the successful use of formal methods by industry are cited.
",
 note         = "",
}

------------------------------------------------------------------------------

469

@Article{sullivan2001a,
 key     = "sullivan2001a",
 author  = "Kevin Sullivan and Yuanfang Cai and Ben Hallen and William G.
            Griswold",
 title   = "The Structure and Value of Modularity in Software Design",
 journal = j-sig,
 month   = sep,
 year    = "2001",
 volume  = "26",
 number  = "5",
 pages   = "99--108",
 abstract = "
The concept of information hiding modularity is a cornerstone of modern
software design thought, but its formulation remains casual and its emphasis
on changeability is imperfectly related to the goal of creating added value in
a given context. We need better explanatory and prescriptive models of the
nature and value of information hiding. We evaluate the potential of a new
theory-developed to account for the influence of modularity on the evolution
of the computer industry-to inform software design. The theory uses design
structure matrices to model designs and real options techniques to value them.
To test the potential utility of the theory for software we apply it to
Parnas's (1972) KWIC designs. We contribute an extension to design structure
matrices, and we show that the options results are consistent with Parnas's
conclusions. Our results suggest that such a theory does have potential to
help inform software design.
",
 note    = "",
}

------------------------------------------------------------------------------

470

@Article{golzmann97a,
 key     = "golzmann97a",
 author  = "Gerard J. Holzmann",
 title   = "The Model Checker Spin",
 journal = j-tse,
 month   = may,
 year    = "1997",
 volume  = "23",
 number  = "5",
 pages   = "279--95",
 abstract = "
SPIN is an efficient verification system for models of distributed software
systems. It has been used to detect design errors in applications ranging from
high-level descriptions of distributed algorithms to detailed code for
controlling telephone exchanges. The paper gives an overview of the design and
structure of the verifier, reviews its theoretical foundation, and gives an
overview of significant practical applications.
",
 note    = "",
}

------------------------------------------------------------------------------

471,Ledeczi_A_5_17_2001_The_Generi.pdf

@InProceedings{ledeczi2001b,
 key          = "ledeczi2001b",
 author       = "Akos Ledeczi and Miklos Maroti and Arpad Bakay and Gabor
                 Karsai and Jason Garrett and Charles Thomason and Greg
                 Nordstrom and Jonathan Sprinkle and Peter Volgyesi",
 title        = "The Generic Modeling Environment",
 booktitle    = "Workshop on Intelligent Signal Processing",
 month        = "17 " # may,
 year         = "2001",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Budapest, Hungary",
 abstract = "
The Generic Modeling Environment (GME) is a configurable toolset that supports
the easy creation of domain-specific modeling and program synthesis
environments.  The primarily graphical, domain-specific models can represent
the application and its environment including hardware resources, and their
relationship. The models are then used to automatically synthesize the
application and/or generate inputs to COTS analysis tools. In addition to
traditional signal processing problems, we have applied this approach to tool
integration and structurally adaptive systems among other domains. This paper
describes the GME toolset and compares it to other similar approaches. A case
study is also presented that illustrates the core concepts through an example.
",
 note         = "",
}

------------------------------------------------------------------------------

472,00963443.pdf

@Article{ledeczi2001a,
 key     = "ledeczi2001a",
 author  = "{\'Akos} {L\'edeczi} and {\'Arp\'ad} Bakay and {Mikl\'os}
            {Mar\'oti} and {P\'eter} {V\"olgyesi} and Greg Nordstsrom
            and Jonathan Sprinkle and {G\'abor} Karsai",
 title   = "Composing Domain-Specific Design Environments",
 journal = j-comp,
 month   = nov,
 year    = "2001",
 volume  = "34",
 number  = "11",
 pages   = "44--51",
 abstract = "
Model-integrated computing can help compose domain-specific design
environments rapidly and cost-effectively. The authors discuss the
toolset that implements MIC and present a practical application of the
technology---a tool environment for the process industry.
",
 note    = "",
}

------------------------------------------------------------------------------

473,00918216.pdf

@Article{succi2001b,
 key     = "succi2001b",
 author  = "Giancarlo Succi and Witold Pedrcyz and Eric Liu and
           Jason Yip",
 title   = "Package-oriented software engineering: a generic architecture",
 journal = "{IT} Professional",
 month   = mar # "--" # apr,
 year    = "2001",
 volume  = "3",
 number  = "2",
 pages   = "29--36",
 abstract = "
New methodologies and better techniques are the rule in software
engineering, and users of large and complex methodologies benefit
greatly from specialized software support tools. However, developing
such tools is both difficult and expensive, because developers must
implement a lot of functionality in a short time. A promising solution
is component-based software development, in particular
package-oriented programming (POP). POP fails, however, to satisfy all
the requirements of large, complex software engineering tasks. A more
generic POP architecture would better serve the development of
software engineering environments for large and complex methodologies.
Such an architecture emerged from our development experiences with two
software engineering research tools: Holmes, a domain analysis support
tool; and Egidio, a unified-modeling-language-based business modeling
tool. We found this particular architecture simple to understand, easy
to implement, and a natural candidate for a generic POP architecture.
Our generic architecture satisfies the additional requirements we deem
important for larger, more complex software engineering activities.
Our experiences show that the strength of this architecture lies in
its simplicity and ability to work with multiple users and quickly
integrate a wide variety of applications. It is not perfect, but we
present it as a first step toward a more general package-oriented
architecture to encourage further research in this area. 
",
 note    = "",
}

------------------------------------------------------------------------------

474

@Article{morgenstern95a,
 key     = "morgenstern95a",
 author  = "Joe Morgenstern",
 title   = "The Fity-Nine-Story Crisis",
 journal = "The New Yorker",
 month   = "29" # may,
 year    = "1995",
 volume  = "71",
 number  = "14",
 pages   = "45--53",
 abstract= "
On a warm June day in 1978 William J. LeMessurier, one o the nation's leading
structural engineers, received a phone call at his headquarters, in Cambridge,
Massachusetts, from an engineering student in New Jersey. The young man, whose
name has been lost in the swirl of subsequent events, said that his professor
had assigned him to write a paper on the Citicorp tower, the slash-topped
silver skyscraper that had become, on its completion in Manhattan the year
before, the se4venth-tallest building in the world.
",
 note    = "",
}

------------------------------------------------------------------------------

475,kamin.pdf

@InProceedings{kamin97a,
 key          = "kamin97a",
 author       = "Samual N. Kamin and David Hyatt",
 title        = "A Special-Purpose Languae for Picture-Drawing",
 booktitle    = "Proceedings of the Conference on Domain-Specific
 Languages",
 month        = "15--17 " # oct,
 year         = "1997",
 pages        = "297--310",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "USENIX Assoc. Berkeley, CA",
 address      = "Santa Barbara, CA",
 abstract = "
Special purpose languages are typically characterized by a type of
primitive data and domain specific operations on this data. One
approach to special purpose language design is to embed the data and
operations of the language within an existing functional language. The
data can be defined using the type constructions provided by the
functional language, and the special purpose language then inherits
all of the features of the more general language. We outline a domain
specific language, FPIC, for the representation of two dimensional
pictures. The primitive data and operations are defined in ML. We
outline the operations provided by the language, illustrate the power
of the language with examples, and discuss the design process.
",
 note         = "",
}

------------------------------------------------------------------------------

476

@InProceedings{widmaier2000a,
 key          = "widmaier2000a",
 author       = "James C. Widmaier and Carol Smidts and Xin Huang",
 title        = "Producing More Reliable Software: Mature Software Engineering
                 Process vs. State-of-the-Art Technology?",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11 " # jun,
 year         = "2000",
 pages        = "88--93",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A customer of high assurance software recently sponsored a software
engineering experiment in which a real-time software system was developed
concurrently by two popular software development methodologies. One company
specialized in the state-of-the-practice waterfall method rated at a
Capability Maturity Model Level 4. A second developer employed his
mathematically based formal method with automatic code generation. As
specified in separate contracts, C++ code plus development documentation and
process and product metrics (errors) were to be delivered. Both companies were
given identical functional specs and agreed to a generous and equal cost,
schedule, and explicit functional reliability objectives. At conclusion of the
experiment an independent third party determined through extensive statistical
testing that neither methodology was able to meet the user's reliability
objectives within cost and schedule constraints. The metrics collected
revealed the strengths and weaknesses of each methodology and why they were
not able to reach customer reliability objectives. This paper explores the
specification for the system under development, the two competing development
processes, the products and metrics captured during development, the analysis
tools and testing techniques by the third party, and the results of a
reliability and process analysis.
",
 note         = "",
}

------------------------------------------------------------------------------

477

@InProceedings{potts97a,
 key          = "potts97a",
 author       = "Colin Potts and Wendy C. Newstetter",
 title        = "Naturalistic Inquiry and Requirements Engineering:
                 Reconciling Their Theoretical Foundations",
 booktitle    = "Proceedings of the Third IEEE International Symposium on
                 Requirements Engineering",
 address      = "Annapolis, MD",
 month        = "6--10 " # jun,
 year         = "1997",
 pages        = "118--27",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A growing awareness of the need to take into account social and contextual
factors requirements engineering (RE) has led to expanded use of naturalistic
inquiry (NI) methods, such as ethnography, for capturing relevant data. There
is little debate about the potential value of NI to the development of
systems; however, most previous discussions have emphasized practical
techniques and benefits. Less attention has been given to the ontological and
epistemological commitments that a naturalistic research paradigm assume and
the extent to which these assumptions conflict with those that pervade RE
practice. In this paper we present the axioms that NI. In each case we address
both the points of agreement and tension that arise when these axioms are
compared with the implicit assumptions upon which RE practice and research
methods are based. We illustrate the discussion with specific examples from
published sources and our experience.
",
 note         = "",
}

------------------------------------------------------------------------------

478

@Article{bush2000a,
 key     = "bush2000a",
 author  = "William R. Bush and Jonathan D. Pincus and David J. Sielaff",
 title   = "A Static Analyzer for Finding Dynamic Programming Errors",
 journal = j-spe,
 month   = jun,
 year    = "2000",
 volume  = "30",
 number  = "7",
 pages   = "775--802",
 abstract = "
There are important classes of programming errors that are hard to diagnose,
both manually and automatically, because they involve a program's dynamic
behavior. This article describes a compile-time analyzer that detects these
dynamic errors in large, real-world programs. The analyzer traces execution
paths through the source code, modeling memory and reporting inconsistencies.
In addition to avoiding false paths through the program, this approach
provides valuable contextual information to the programmer who needs to
understand and repair the defects. Automatically-created models, abstracting
the behavior of individual functions, allow inter-procedural defects to be
detected efficiently. A product built on these techniques has been used
effectively on several large commercial programs. 
",
 note    = "",
}

------------------------------------------------------------------------------

479,annals97.ps

@Article{lutz97a,
 key     = "lutz97a",
 author  = "Robyn R. Lutz and Robert M. Woodhouse",
 title   = "Requirements Analysis Using Forward and Backward Search",
 journal = "Annals of Software Engineering",
 month   = "",
 year    = "1997",
 volume  = "3",
 number  = "",
 pages   = "",
 abstract = "
The requirements analysis of critical software components often involves a
search for hazardous states and failure modes. This paper describes the
integration of a forward search for consequences of reaching these forbidden
modes with a backward search for contributing causes. Results are reported
from two projects in which the integrated search method was used to analyze
the requirements of critical spacecraft software. The search process was found
to be successful in identifying some ambiguous, inconsistent, and missing
requirements. More importantly, it identified four significant, unresolved
requirements issues involving complex system interfaces and unanticipated
dependencies. The results suggest that recent efforts by researchers to
integrate forward and backward search have merit. 
",
 note    = "",
}

------------------------------------------------------------------------------

480

@Article{dijkstra68a,
 key     = "dijkstra68a",
 author  = "Edsger W. Dijkstra",
 title   = "Go To Statement Considered Harmful",
 journal = j-cacm,
 month   = mar,
 year    = "1968",
 volume  = "11",
 number  = "3",
 pages   = "147--148",
 abstract= "
",
 note    = "",
}

------------------------------------------------------------------------------

481

@Article{biddle86a,
 key     = "biddle86a",
 author  = "Wayne Biddle",
 title   = "How Much Bang for the Buck?",
 journal = "Discover",
 month   = sep,
 year    = "1986",
 volume  = "",
 number  = "",
 pages   = "50--63",
 abstract = "
Our weapons tests now use so much computer modeling and simulation that no one
knows whether some new arms really work.
",
 note    = "",
}

------------------------------------------------------------------------------

@TechReport{coppit2002a,
 key         = "coppit2002a",
 author      = "David Coppit and Kevin J. Sullivan and Joanne Bechta Dugan",
 title       = "A Formal Semantics For Dynamic Fault Trees",
 institution = "Department of Computer Science, University of Virginia",
 month       = "9 " # sep,
 year        = "2002",
 number      = "CS-2002-##",
 address     = "",
 type        = "",
 abstract = "
",
 note        = "",
}

------------------------------------------------------------------------------

@Book{chappell96a,
 key       = "chappell96a",
 author    = "David Chappell",
 title     = "Understanding ActiveX and OLE",
 publisher = "Microsoft Press",
 edition   = "",
 month     = "",
 year      = "1996",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{monsonhaefel2001a,
 key       = "monsonhaefel2001a",
 author    = "Richard Monson-Haefel",
 title     = "Enterprise JavaBeans",
 publisher = "O'Reilly \& Associates",
 edition   = "3rd",
 month     = "",
 year      = "2001",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

482

@InProceedings{meudec1999a,
 key          = "meudec1999a",
 author       = "Christophe Meudec",
 title        = "Tests Derivation from Model Based Formal Specifications",
 booktitle    = "Proceedings of Third Irish Workshop in Formal Methods",
 month        = jul,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "USA",
 abstract = "
Software testing consumes a large percentage of total software development
costs. Yet, it is still usually performed
manually in a non rigorous fashion. In this work we suggest how state of the
art practices in the area of testing can
be applied to the systematic generation of tests from model-based formal
specifications.
",
 note         = "",
}

------------------------------------------------------------------------------

483

@Article{graves2001a,
 key     = "graves2001a",
 author  = "Todd L. Graves and Mary Jean Harrold and Jung-Min Kim and Adam
            Porter and Gregg Rothermel",
 title   = "An Empirical Study of Regression Test Selection Techniques",
 journal = j-tsem,
 month   = apr,
 year    = "2001",
 volume  = "10",
 number  = "2",
 pages   = "184--208",
 abstract = "
Regression testing is the process of validating modified software to detect
whether new errors have been introduced into previously tested code and to
provide confidence that modifications are correct. Since regression testing is
an expensive process, researchers have proposed regression test selection
techniques as a way to reduce some of this expense. These techniques attempt
to reduce costs by selecting and running only a subset of the test cases in a
program's existing test suite. Although there have been some analytical and
empirical evaluations of individual techniques, to our knowledge only one
comparative study, focusing on one aspect of two of these techniques, has been
reported in the literature. We conducted an experiment to examine the relative
costs and benefits of several regression test selection techniques. The
experiment examined five techniques for reusing test cases, focusing on their
relative abilities to reduce regression testing effort and uncover faults in
modified programs. Our results highlight several differences between the
techniques, and expose essential trade-offs that should be considered when
choosing a technique for practical application.
",
 note    = "",
}

------------------------------------------------------------------------------

484

@InProceedings{grieskamp2002a,
 key          = "grieskamp2002a",
 author       = "Wolfgang Grieskamp and Yuri Gurevich and Wolfram Schulte and
                 Margus Veanes",
 title        = "Generating Finite State Machines from Abstract State Machines",
 booktitle    = "Proceedings of International Symposium on Software Testing
                 and Analysis (ISSTA 2002)",
 address      = "Rome, Italy",
 month        = "22--24 " # jul,
 year         = "2002",
 pages        = "112--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
We give an algorithm that derives a finite state machine (FSM) from a given
abstract state machine (ASM) specification. This allows us to integrate ASM
specs with the existing tools for test case generation from FSMs. ASM specs
are executable but have typically too many, often infinitely many states. We
group ASM states into finitely many hyperstates which are the nodes of the
FSM. The links of the FSM are induced by the ASM state transitions. 
",
 note         = "",
}

------------------------------------------------------------------------------

485

@InProceedings{tahat2001a,
 key          = "tahat2001a",
 author       = "Luay H. Tahat and Boris Vaysburg and Bogdan Korel and Atef J.
                 Bader",
 title        = "Requirement-Based Automated Black-Box Test Generation",
 booktitle    = "Proceedings of the 25th Annual International Computer
                 Software and Applications Conference. COMPSAC 2001. ",
 address      = "Chicago, IL",
 month        = "8--12 " # oct,
 year         = "2001",
 pages        = "489--95",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Testing large software systems is very laborious and expensive. Model-based
test generation techniques are used to automatically generate tests for large
software systems. However, these techniques require manually created system
models that are used for test generation. In addition, generated test cases
are not associated with individual requirements. In this paper, we present a
novel approach of requirement-based test generation. The approach accepts a
software specification as a set of individual requirements expressed in
textual and SDL formats (a common practice in the industry). From these
requirements, system model is automatically created with requirement
information mapped to the model. The system model is used to automatically
generate test cases related to individual requirements. Several test
generation strategies are presented. The approach is extended to
requirement-based regression test generation related to changes on the
requirement level. Our initial experience shows that this approach may provide
significant benefits in terms of reduction in number of test cases and
increase in quality of a test suite.
",
 note         = "",
}

------------------------------------------------------------------------------

486

@Article{li1999a,
 key     = "li1999a",
 author  = "Yuejian Li and Nancy J. Wahl",
 title   = "An Overview of Regression Testing",
 journal = j-sig,
 month   = jan,
 year    = "1999",
 volume  = "24",
 number  = "1",
 pages   = "69--73",
 abstract = "
Regression testing is an important part of the software development life
cycle. Many articles have been published detailing the different approaches.
This article is an overview of regression testing in the following areas:
types of regression testing; unit, integration and system level testing;
regression testing of global variables; regression testing of object-oriented
software; comparisons of selective regression techniques; and cost comparisons
of the types of regression testing. 
",
 note    = "",
}

------------------------------------------------------------------------------

487

@Article{williams2000a,
 key     = "williams2000a",
 author  = "Laurie  Williams and Robert R. Kessler and Ward Cunningham and
            Ron Jeffries",
 title   = "Strengthening the Case for Pair Programming",
 journal = j-soft,
 month   = jul # "--" # aug,
 year    = "2000",
 volume  = "17",
 number  = "4",
 pages   = "19--25",
 abstract = "
The software industry has practiced pair programming--two programmers working
side by side at one computer on the same problem--for years. But people who
haven't tried it often reject the idea as a waste of resources. The authors
demonstrate that pair programming yields better software products in less
time--and happier, more confident programmers. Their supportive evidence comes
from professional programmers and an experiment with university students. 
",
 note    = "",
}

------------------------------------------------------------------------------

488

@InProceedings{rayadurgam2001a,
 key          = "rayadurgam2001a",
 author       = "Sanjai Rayadurgam and Mats P. E. Heimdahl",
 title        = "Coverage Based Test-Case Generation Using Model Checkers ",
 booktitle    = "Proceedings of the Eighth Annual IEEE International
                 Conference and Workshop on the Engineering of Computer
                 Based Systems (ECBS '01) ",
 address      = "Washington DC",
 month        = "17--20 " # apr,
 year         = "2001",
 pages        = "83--93",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
This paper presents a method for automatically generating test cases to
structural coverage criteria. We show how a model checker can be used to
automatically generate complete test sequences that will provide a predefined
coverage of any software development artifact that can be represented as a
finite state model. Our goal is to help reduce the high cost of developing
test cases for safety-critical software applications that require a certain
level of coverage for certification, for example, safety-critical avionics
systems that need to demonstrate MC/DC (modified condition and decision)
coverage of the code. We define a formal framework suitable for modeling
software artifacts, like, requirements models, software specifications, or
implementations. We then show how various structural coverage criteria can be
formalized and used to make a model checker provide test sequences to achieve
this coverage. To illustrate our approach, we demonstrate, for the first time,
how a model checker can be used to generate test sequences for MC/DC coverage
of a small case example. 
",
 note         = "",
}

------------------------------------------------------------------------------

489

@InProceedings{grimson2000a,
 key          = "grimson2000a",
 author       = "Jane B. Grimson and Hans-{J\"urgen} Kugler",
 title        = "Software Needs Engineering---A Position Paper",
 booktitle    = "Proceedings of the 22nd International Conference on Software
                 Engineering",
 address      = "Limerick, Ireland",
 month        = "4--11  " # jun,
 year         = "2000",
 pages        = "541--4",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
When the general press refers to `software' in its headlines, then this is
often not to relate a success story, but to expand on yet another
`software-risk-turned-problem-story'. For many people, the term `software'
evokes the image of an application package running either on a PC or some
similar stand-alone usage. Over 70% of all software, however, are not
developed in the traditional software houses as part of the creation of such
packages. Much of this software comes in the form of products and services
that end-users would not readily associate with software. These can be complex
systems with crucial connections made through software, such as
telecommunication or banking systems, or the logistics systems of airports; or
they can be end-user products with software embedded, ranging from battery
management systems in electric shavers, to mobile phones, to engine management
and safety systems in cars. E-commerce systems fall into this category too.
Yes, there is software that works reliably and as expected, and there are
professional approaches to create such products - one can engineer software,
in the right environment, with the right people.
",
 note         = "",
}

------------------------------------------------------------------------------

490

@InBook{goerigk1999a,
 key          = "goerigk1999a",
 author       = "Wolfgang Goerigk and Thilo Gaul and Wolf Zimmermann",
 title        = "Tool Support for System Specification and Verification",
 chapter      = "Programs without Proof? On Checker Based Program Verification",
 pages        = "108--23",
 publisher    = "Springer Verlag",
 year         = "1999",
 volume       = "",
 series       = "Springer Series Advances in Computing Science",
 address      = "London, UK",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}

------------------------------------------------------------------------------

491

@InProceedings{boyapati2002a,
 key          = "boyapati2002a",
 author       = "Chandrasekhar Boyapati and Sarfraz Khurshid and Darko Marinov",
 title        = "{Korat}: Automated Testing Based on {Java} Predicates",
 booktitle    = "Proceedings of the International Symposium on Software Testing
                 and Analysis (ISSTA 2002)",
 address      = "Rome, Italy",
 month        = "22--24 " # jul,
 year         = "2002",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
This paper presents Korat, a novel framework for automated testing
of Java programs. Given a formal specification for a method, Korat
uses the method precondition to automatically generate all nonisomorphic
test cases bounded by a given size. Korat then executes
the method on each of these test cases, and uses the method postcondition
as a test oracle to check the correctness of each output.\par
To generate test cases for a method, Korat constructs a Java predicate
(i.e., a method that returns a boolean) from the method.s precondition.
The heart of Korat is a technique for automatic test case
generation: given a predicate and a bound on the size of its inputs,
Korat generates all nonisomorphic inputs for which the predicate
returns true. Korat exhaustively explores the input space of the
predicate but does so efficiently by monitoring the predicate.s executions
and pruning large portions of the search space.\par
This paper illustrates the use of Korat for testing several data structures,
including some from the Java Collections Framework. The
experimental results show that it is feasible to generate test cases
from Java predicates, even when the search space for inputs is very
large. This paper also compares Korat with a testing framework
based on declarative specifications. Contrary to our initial expectation,
the experiments show that Korat generates test cases much
faster than the declarative framework.
",
 note         = "",
}

------------------------------------------------------------------------------

492

@InProceedings{marinov2001a,
 key          = "marinov2001a",
 author       = "Darko Marinov and Sarfraz Khurshid",
 title        = "{TestEra}: A Novel Framework for Automated Testing of {Java}
                 Programs",
 booktitle    = "Proceedings of the 16th IEEE Conference on Automated
                 Software Engineering (ASE 2001)",
 address      = "San Diego, CA",
 month        = "26--29 " # nov,
 year         = "2001",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
TestEra is a novel framework for automated testing of Java programs. TestEra
automatically generates all non-isomorphic test cases, within a given input
size, and evaluates correctness criteria. As an enabling technology, TestEra
uses Alloy, a first-order relational language, and the Alloy Analyzer.
Checking a program with TestEra involves modeling the correctness criteria for
the program in Alloy and specifying abstraction and concretization
translations between instances of Alloy models and Java data structures.
TestEra produces concrete Java inputs as counterexamples to violated
correctness criteria. We have used TestEra's analysis in several case studies:
methods that manipulate singly linked lists and red-black trees, a naming
architecture, and a part of the Alloy Analyzer.
",
 note         = "",
}

------------------------------------------------------------------------------

493

@InProceedings{necula1997a,
 key          = "necula1997a",
 author       = "George C. Necula",
 title        = "Proof-Carrying Code",
 booktitle    = "Conference Record of POPL '97: The 24th ACM SIGPLAN-SIGACT
               Symposium on Principles of Programming Languages",
 address      = "Paris, France",
 month        = "15--17 " # jan,
 year         = "1997",
 pages        = "106--19",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
This paper describes proof-carrying code (PCC), a mechanism by which a host
system can determine with certainty that it is safe to execute a program
supplied (possibly in binary form) by an untrusted source. For this to be
possible, the untrusted code producer must supply with the code a safety proof
that attests to the code's adherence to a previously defined safety policy.
The host can then easily and quickly validate the proof without using
cryptography and without consulting any external agents. In order to gain
preliminary experience with PCC, we have performed several case studies. We
show in this paper how proof-carrying code might be used to develop safe
assembly-language extensions of ML programs. In the context of this case
study, we present and prove the adequacy of concrete representations for the
safety policy, the safety proofs, and the proof validation. Finally, we
briefly discuss how we use proof-carrying code to develop network packet
filters that are faster than similar filters developed using other techniques
and are formally guaranteed to be safe with respect to a given operating
system safety policy. 
",
 note         = "",
}

------------------------------------------------------------------------------

494

@InProceedings{chang1999a,
 key          = "chang1999a",
 author       = "Juei Chang and Debra J. Richardson",
 title        = "Structural Specification-Based Testing: Automated Support and
                 Experimental Evaluation",
 booktitle    = "ESEC/ FSE-99: Proceedings of the 7th European Software
                 Engineering Conference / 7th ADM SIGSoft Symposium on
                 the Foundations of Software Engineering",
 month        = "6--10 " # sep,
 year         = "1999",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "USA",
 abstract = "
In this paper, we describe a testing technique, called structural
specification-based testing (SST), which utilizes the formal specification of
a program unit an the basis for test selection and test coverage measurement.
We also describe an automated testing tool, called ADLscope, which supports
SST for program units specified in Sun Microsystems' Assertion Definition
Language (ADL). ADLscope automatically generates coverage conditions from a
program's ADL specification. While the program is tested, ADLscope determines
which of these conditions are covered by the tests. An uncovered condition
exhibits aspects of the specification inadequately exercised during testing.
The tester uses this information to develop new test data to exercise the
uncovered conditions.\par

We provide an overview of SST's specification-based test
criteria and describe the design and implementation of ADLscope.
Specification-based testing is guided by a specification, whereby the testing
activity is directly related to what a component under test is supposed to do,
rather than what it actually does. Specification-based testing is a
significant advance in testing, because it is often more straightforward to
accomplish and it can reveal failures that are often missed by traditional
code-based testing techniques. As an initial evaluation of the capabilities of
specification-based testing, we conducted an experiment to measure defect
detection capabilities, code coverage and usability of SST/ADLscope; we report
here on the results. 
",
 note         = "",
}

------------------------------------------------------------------------------

@PhdThesis{coppit2003a,
 key          = "coppit2003a",
 author       = "David Coppit",
 title        = "Engineering Modeling and Analysis:
                 Sound Methods and Effective Tools",
 address      = "Charlottesville, Virginia",
 school       = "The University of Virginia",
 month        = jan,
 year         = "2003",
 abstract     = "
Developing high quality software tools for specialized domains is difficult.
One problem is the cost of developing feature-rich and usable tool interfaces.
Another problem is the task of providing a sound basis for trustworthiness of
the tool and the overall method which it supports. In this dissertation we
present and evaluate an approach which addresses these key difficulties.  The
approach is based on two concepts: using specialized and tightly integrated
mass-market applications to provide the bulk of the tool's functionality, and
the use of formal methods for the precise specification of the tool's
domain-dependent modeling language. We have evaluated our component-based work
in part by developing a tool using the technique, deploying it to NASA, and
having engineers from across the organization use and evaluate it. In the area
of formal methods, we have developed and validated, both informally and
formally, a mathematically precise specification of the language employed by
an innovative modeling and analysis method for the reliability of fault
tolerant systems.  We have also developed a prototype tool that shows in
concrete terms that our combined approach can work. The chief contribution of
this work is a new approach to developing software tools having formal
foundations for trustworthiness and sophisticated user interfaces.
Constituent contributions include a qualified positive evaluation of the
component-based approach, a proof of feasibility of using formal methods for
domain-specific modeling languages, and the precise definition of an important
modeling language, namely one for dynamic fault tree analysis.
",
 note         = "URL: {\urlBiBTeX{http://www.cs.wm.edu/~coppit/papers/dissertation.pdf}}",
}

------------------------------------------------------------------------------

@InProceedings{frailey79a,
 key          = "frailey79a",
 author       = "Dennis J. Frailey",
 title        = "An Intermediate Language for Source and Target Independent
                 Code Optimization",
 booktitle    = "Proceedings of the {SIGPLAN} '79 Symposium on Compiler
                 Construction",
 month        = "6--10 " # aug,
 year         = "1979",
 pages        = "188--200",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "Denver, Colorado",
 abstract = "
",
 note         = "",
}

------------------------------------------------------------------------------

@Misc{fsf2003a,
 key    = "fsf2003a",
 author = "{The Free Software Foundation}",
 title  = "The {GCC} Home Page",
 url    = "http://gcc.gnu.org/",
 abstract = "
GCC is the GNU Compiler Collection, which currently contains front ends for C,
C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these
languages (libstdc++, libgcj,...).
",
 note   = "URL: {\urlBiBTeX{http://gcc.gnu.org/}}",
}

------------------------------------------------------------------------------

@InProceedings{coppit2003b,
 key          = "coppit2003b",
 author       = "David Coppit and Kevin J. Sullivan",
 title        = "Sound Methods and Effective Tools for Engineering Modeling
                 and Analysis",
 booktitle    = "Proceedings of the 25th International Conference on Software
                 Engineering",
 address      = "Portland, Oregon",
 month        = "3--10 " # may,
 year         = "2003",
 pages        = "198--207",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Modeling and analysis is indispensable in engineering.  To be safe and
effective, a modeling method requires a language with a validated semantics;
feature-rich, easy-to-use, dependable tools; and low engineering costs. Today
we lack adequate means to develop such methods. We present a partial solution
combining two techniques: formal methods for language design, and
package-oriented programming for function and usability at low cost. We have
evaluated the approach in an end-to-end experiment. We deployed an existing
reliability method to NASA in a package-oriented tool and surveyed engineers
to assess its usability. We formally specified, improved, and validated the
language. To assess cost, we built a package-based tool for the new language.
Our data show that the approach can enable cost-effective deployment of sound
methods by effective tools.
",
 note         = "",
}

------------------------------------------------------------------------------

@InProceedings{coppit2003c,
 key          = "coppit2003c",
 author       = "David Coppit and Robert R. Painter and Kevin J. Sullivan",
 title        = "Toward a Unified Semantics and Implementation for
                 Computational Engineering",
 booktitle    = "Proceedings of the International Symposium on Software
                 Reliability Engineering",
 address      = "Denver, Colorado",
 month        = "17--20 " # nov,
 year         = "2003",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
Modeling languages and the software tools which support them are essential to
engineering. However, as modeling languages become increasingly sophisticated,
it becomes difficult to ensure both their semantic validity and the
trustworthiness of their software implementations. In this paper we present an
approach in which we identify the common underlying semantics of a class of
languages, and use this semantics to develop an intermediate language which
can serve as a unifying theoretical foundation and the basis for a standard
reusable software implementation.  To test this approach, we applied it to the
domain of reliability modeling and analysis. We developed an intermediate
modeling language called the {\em failure automaton} (FA), and used it to
formally define the semantics of {\em reliability block diagrams} (RBDs) and
{\em dynamic fault trees} (DFTs). In addition, we developed a formally-based
software implementation for FAs, and used it for the implementation of both
RBDs and DFTs.  Our experiences suggest that our approach can greatly ease the
difficulty of defining the semantics of sophisticated modeling languages,
lower the effort required to implement them in software, and improving the
trustworthiness of the resulting implementation.
 ",
 note         = "To appear.",
}

------------------------------------------------------------------------------

@Book{morgan94a,
 key       = "morgan94a",
 author    = "Carroll Morgan",
 title     = "Programming from Specifications",
 publisher = "Prentice Hall",
 edition   = "2nd",
 month     = "",
 year      = "1994",
 volume    = "",
 series    = "",
 address   = "Hempstead, UK",
 note      = "",
}

------------------------------------------------------------------------------

@Book{dijkstra76a,
 key       = "dijkstra76a",
 author    = "Edsger W. Dijkstra",
 title     = "A Discipline of Programming",
 publisher = "Prentice Hall",
 edition   = "",
 month     = "",
 year      = "1976",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{linger79a,
 key       = "linger79a",
 author    = "R. C. Linger and H. D. Mills and B. I. Witt",
 title     = "Structured Programming: Theory and Practice",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1979",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Inbook{emerson90a,
 key          = "emerson90a",
 author       = "E. A. Emerson",
 title        = "Temporal and Modal Logics",
 chapter      = "16",
 pages        = "",
 publisher    = "Elsevier",
 year         = "1990",
 volume       = "",
 series       = "",
 address      = "",
 edition      = "",
 month        = "",
 remark       = "",
 type         = "",
 note         = "",
}

------------------------------------------------------------------------------

@Book{raymond99a,
 key       = "raymond99a",
 author    = "Eric S. Raymond",
 title     = "The Cathedral \& the Bazaar: Musings on {Linux} and
              Open Source by an Accidental Revolutionary",
 publisher = "O'Reilly \& {Associates, Inc.}",
 edition   = "",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{booch99a,
 key       = "booch99a",
 author    = "Grady Booch and James Rumbaugh and Ivar Jacobson",
 title     = "The Unified Modeling Language User Guide",
 publisher = "Addison-Wesley",
 edition   = "",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "Reading, Massachusettes",
 note      = "",
}

------------------------------------------------------------------------------

@InProceedings{floyd67a,
 key          = "floyd67a",
 author       = "Robert W. Floyd",
 title        = "Assigning Meanings to Programs",
 booktitle    = "Mathematical Aspects of Computer Science",
 address      = "New York, New York",
 month        = "5--7" # apr,
 year         = "1967",
 pages        = "19--32",
 editor       = "J. T. Schwartz",
 organization = "",
 volume       = "19",
 series       = "Proceedings of Symposia in Applied Mathematics",
 publisher    = "American Mathematical Society",
 abstract     = "
 ",
 note         = "",
}

------------------------------------------------------------------------------

497

@InProceedings{bartetzko2001a,
 key          = "bartetzko2001a",
 author       = "Detlef Bartetzko and Clemens Fischer and Michael {M\"oller}
                 and Heike Wehrheim",
 title        = "{Jass} -- {Java} with Assertions",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Design by Contract, proposed by Meyer for the programming language Eiffel, is
a technique that allows run-time checks of specification violation and their
treatment during program execution. Jass, Java with assertions, is a Design by
Contract extension for Java allowing to annotate Java programs with
specifications in the form of assertions. The Jass tool is a pre-compiler that
translates annotated into pure Java programs in which compliance with the
specification is dynamically tested.  Besides the standard Design by Contract
features known from classical program verification (e.g. pre- and
postconditions, invariants), Jass additionally supports refinement, i.e.
subtyping, checks and the novel concept of trace assertions.  Trace assertions
are used to monitor the dynamic behaviour of objects in time.
 ",
 note    = "",
}

------------------------------------------------------------------------------

513

@InProceedings{ducasse2001a,
 key          = "ducasse2001a",
 author       = "Mireille {Ducass\'e} and Erwan Jahier",
 title        = "Efficient Automated Trace Analysis: Examples with {Morphine}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Opium, Morphine and Coca are three automated trace analyzers based on the same
principles for three different programming languages. An automated trace
analyzer is connected to an event-oriented tracer. The traced program is run
in coprocessing with the trace analysis session in which the user enters
high-level queries about the traced execution. The trace is then automatically
processed according to the query. The key of efficiency is that most of the
work is done (1) on the fly and (2) in the traced process. In this article, we
first present these mechanisms and then illustrate them through a debugging
session and monitoring examples with Morphine.
 ",
 note    = "",
}

------------------------------------------------------------------------------

517

@InProceedings{dudani2001a,
 key          = "dudani2001a",
 author       = "Surrendra Dudani and Joao Geada and Grzegorz Jakacki and
                 Daniel Vainer",
 title        = "Dynamic Assertions Using {TXP}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
In this paper, we present a new temporal property specification language TXP.
The language is designed to support dynamic monitoring of temporal properties
at simulation runtime, as well as to provide the input specification for
formal property checking. For design verification of hardware systems,
hardware description languages (HDL) provide modeling capabilities, but they
are inadequate for concise specification of complex assertions where logic
relationships involve multi cycle behavior. TXP is a declarative language that
provides a rich set of operators based on regular expressions over sequences
of values and events. Its key features are to allow multi-cycle behavior, time
shift operations in the past or future, conditional matching, repetition of
sequences, and restrictions over sequences. The sequences can be constructed
with logical connectives such as ``and'' and ``or'' to compose more complex
assertions. A TXP engine has been developed to monitor the properties at
runtime.
 ",
 note    = "",
}

------------------------------------------------------------------------------

518

@InProceedings{finkbeiner2001a,
 key          = "finkbeiner2001a",
 author       = "Bernd Finkbeiner and Henny Sipma",
 title        = "Checking Finite Traces using Alternating Automata",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
We present three algorithms to check at runtime whether a reactive program
satisfies a temporal specification, expressed by a future linear-time temporal
logic formula. The three methods are all based on alternating automata, but
traverse the automaton in different ways: depth-first, breadth-first, and
backwards, respectively. All three methods have been implemented and
experimental results are presented. We outline an extension to these
algorithms that is applicable to LTL formulas containing both past and future
operators.
 ",
 note    = "",
}

------------------------------------------------------------------------------

495

@InProceedings{gates2001a,
 key          = "gates2001a",
 author       = "A. Q. Gates and S. Roach and O. Mondragon and N. Delgado",
 title        = "{DynaMICs}: Comprehensive Support for Run-Time Monitoring",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Design by Contract, proposed by Meyer for the programming language Eiffel, is
a technique that allows run-time checks of specification violation and their
treatment during program execution. Jass, Java with assertions, is a Design by
Contract extension for Java allowing to annotate Java programs with
specifications in the form of assertions. The Jass tool is a pre-compiler that
translates annotated into pure Java programs in which compliance with the
specification is dynamically tested.  Besides the standard Design by Contract
features known from classical program verification (e.g. pre- and
postconditions, invariants), Jass additionally supports refinement, i.e.
subtyping, checks and the novel concept of trace assertions.  Trace assertions
are used to monitor the dynamic behaviour of objects in time.
 ",
 note    = "",
}

------------------------------------------------------------------------------

502

@InProceedings{geilen2001a,
 key          = "geilen2001a",
 author       = "Marc Geilen",
 title        = "On the Construction of Monitors for Temporal Logic Properties",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Temporal logic is a valuable tool for specifying correctness properties of
reactive programs. With the advent of temporal logic model checkers, it has
become an important aid for the verification of concurrent and reactive
systems. In model checking the temporal logic properties are verified against
models expressed in the tool's modelling language. In addition, model-checking
techniques are useful to test actual implementations or to verify models of
the system that are too detailed to be analysed by a model checker, by means
of, for instance, simulation. \par

A tableau construction is an algorithm that translates a temporal logic
formula into a finite-state automaton that accepts precisely all the models of
the formula. It is a key ingredient to checking satisfiability of a formula as
well as to the automata-theoretic approach to model checking. An improvement
to the efficiency of tableau constructions has been the development of
on-the-fly versions. \par

In this paper, we present a particular tableau construction for the
incremental analysis of execution traces during test, simulation or
model-checking. The automaton forms the basis of a monitor that detects both
good and bad prefix of a particular kind, namely those that are informative
for the property under investigation. We elaborate on the construction of the
monitor and demonstrate its correctness. 
 ",
 note    = "",
}

------------------------------------------------------------------------------

501

@InProceedings{havelund2001a,
 key          = "havelund2001a",
 author       = "Klaus Havelund and Grigore Rosu",
 title        = "Monitoring {Java} Programs with {Java PathExplorer}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
We present recent work on the development of Java PathExplorer (JPaX), a tool
for monitoring the execution of Java programs. JPaX can be used during program
testing to gain increased information about program executions, and can
potentially furthermore be applied during operation to survey safety critical
systems. The tool facilitates automated instrumentation of a program's byte
code, which will then emit events to an observer during its execution. The
observer checks the events against user provided high level requirement
specifications, for example temporal logic formulae, and against lower level
error detection procedures, usually concurrency related such as deadlock and
data race algorithms. High level requirement specifications together with
their underlying logics are defined in rewriting logic using Maude, and then
can either be directly checked using Maude rewriting engine, or be first
translated to efficient data structures and then checked in Java.
 ",
 note    = "",
}

------------------------------------------------------------------------------

499

@InProceedings{kim2001a,
 key          = "kim2001a",
 author       = "Moonjoo Kim and Sampath Kannan and Insup Lee and Oleg
                 Sokolsky and Mahesh Viswanathan",
 title        = "{Java-MaC}: a Run-time Assurance Tool for {Java} Programs",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
We describe Java-MaC, a prototype implementation of the Monitoring and
Checking (MaC) architecture for Java programs. The MaC architecture provides
assurance about the correct execution of target programs at run-time.
Monitoring and checking is performed based on a formal specification of system
requirements. MaC bridges the gap between formal verification, which ensures
the correctness of a design rather than an implementation, and testing, which
only partially validates an implementation. Java-MaC provides a lightweight
formal method solution as a viable complement to the current heavyweight
formal methods. An important aspect of the architecture is the clear
separation between monitoring implementation-dependent low-level behaviors and
checking high-level behaviors against a formal requirements specification.
Another salient feature is automatic instrumentation of executable codes. The
paper presents an overview of the MaC architecture and a prototype
implementation Java-MaC.
 ",
 note    = "",
}

------------------------------------------------------------------------------

512

@InProceedings{kortenkamp2001a,
 key          = "kortenkamp2001a",
 author       = "David Kortenkamp and Tod Milam and Reid Simmons and 
                 Joaquin Lopez Fernandez",
 title        = "Collecting and Analyzing Data from Distributed Control
                 Programs",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
This paper describes a set of tools that allows a developer to instrument a
C/C++ program to log data at run-time and then analyze that data to verify
correct behavior. The logging tools provide the developer with a means to log
a variety of different data to a variety of different outputs. They also allow
for synchronized logging of data from distributed programs. One logging output
option is an SQL database. We have developed a set of analysis tools that
retrieve data from the database to answer common developer questions. The
analysis tools use an interval temporal logic to frame database queries. The
data logging tools are fully implemented and performance results are given in
this paper. The data analysis tools are currently being tested on data from
real NASA applications.
 ",
 note    = "",
}

------------------------------------------------------------------------------

515

@InProceedings{nimmer2001a,
 key          = "nimmer2001a",
 author       = "Jeremy W. Nimmer and Michael D. Ernst",
 title        = "Static verification of dynamically detected program
                 invariants: Integrating {Daikon} and {ESC/Java}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
This paper shows how to integrate two complementary techniques for
manipulating program invariants: dynamic detection and static verification.
Dynamic detection proposes likely invariants based on program executions, but
the resulting properties are not guaranteed to be true over all possible
executions. Static verification checks that properties are always true, but it
can be difficult and tedious to select a goal and to annotate programs for
input to a static checker. Combining these techniques overcomes the weaknesses
of each: dynamically detected invariants can annotate a program or provide
goals for static verification, and static verification can confirm properties
proposed by a dynamic tool.  \par

We have integrated a tool for dynamically detecting likely program invariants,
Daikon, with a tool for statically verifying program properties, ESC/Java.
Daikon examines run-time values of program variables; it looks for patterns
and relationships in those values, and it reports properties that are never
falsified during test runs and that satisfy certain other conditions, such as
being statistically justified. ESC/Java takes as input a Java program
annotated with preconditions, postconditions, and other assertions, and it
reports which annotations cannot be statically verified and also warns of
potential runtime errors, such as null dereferences and out-of-bounds array
indices.  \par

Our prototype system runs Daikon, inserts its output into code as ESC/Java
annotations, and then runs ESC/Java, which reports unverifiable annotations.
The entire process is completely automatic, though users may provide guidance
in order to improve results if desired. In preliminary experiments, ESC/Java
verified all or most of the invariants proposed by Daikon. 
 ",
 note    = "",
}

------------------------------------------------------------------------------

@InProceedings{tripakis2001a,
 key          = "tripakis2001a",
 author       = "Stavros Tripakis and Sergio Yovine",
 title        = "Timing Analysis and Code Generation of Vehicle Control
                 Software using {Taxys}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "23" # jul,
 year         = "2001",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "55",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
PATH's automated vehicle control application software is responsible for the
longitudinal and lateral control of each vehicle in a platoon. The software
consists of a set of processes running concurrently on a PC, reading data from
various sensors (e.g., radar, speedometer, accelerometer, magnetometer),
writing to actuators (throttle, brake and steering), and using radio to
communicate data to other vehicles. The processes exchange data with each
other using a publish/subscribe scheme. In this paper, we describe the current
software, and propose a model written in the synchronous language . We use
Taxys, a tool for timing analysis of Esterel based on the Kronos model-checker
kronos, and the Esterel compiler Saxo-RT, to verify that the application meets
its deadlines. Timing analysis is done on-the-fly during the execution of the
appropriately instrumented C code generated by the compiler. Instrumentation
allows the verifier to observe the execution time of the application code. The
C code generated by Saxo-RT, appropriately linked to the publish/subscribe
library, can be run on the vehicles.
 ",
 note    = "",
}

------------------------------------------------------------------------------

500

@InProceedings{brorkens2002a,
 key          = "brorkens2002a",
 author       = "Mark {Br\"orkens} and Michael {M\"oller}",
 title        = "Dynamic Event Generation for Runtime Checking using the
                 {JDI}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Approaches to runtime checking have to track the execution of a software
system and therefore have to deal with generating and processing execution
events. Often these techniques are applied at the code level -- either by
inserting new source code prior to the compilation or by modifying the target
code, e.g. Java byte code, before running the program. \par

The jassda framework and tool enable runtime checking of Java programs against
a CSP-like specification. For generating events it uses the Java Debug
Interface (JDI) and thus no modifications to the code are necessary. Another
advantage is that events are generated on demand, i.e. dynamically at runtime
it is determined which events to generate for the current debug run without
modifying the program itself. This paper shows how this event generation is
done by the jassda framework.
 ",
 note    = "",
}

------------------------------------------------------------------------------

514

@InProceedings{finkbeiner2002a,
 key          = "finkbeiner2002a",
 author       = "Bernd Finkbeiner and Sriram Sankaranarayanan and Henny
                 Sipma",
 title        = "Collecting Statistics over Runtime Executions",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
By collecting statistics over runtime executions of a program we can answer
complex queries, such as ``what is the average number of packet
retransmissions'' in a communication protocol, or ``how often does process P1
enter the critical section while process P2 waits'' in a mutual exclusion
algorithm. We present an extension to linear-time temporal logic that combines
the temporal specification with the collection of statistical data. By
translating formulas of this language to alternating automata we obtain a
simple and efficient query evaluation algorithm. We illustrate our approach
with examples and experimental results.
 ",
 note    = "",
}

------------------------------------------------------------------------------

498

@InProceedings{karaorman2002a,
 key          = "karaorman2002a",
 author       = "Murat Karaorman and Parker Abercrombie",
 title        = "{jContractor}: Introducing Design-by-Contract to {Java} Using
                 Reflective Bytecode Instrumentation",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "56--80",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Design by Contract is a software engineering practice that allows semantic
information to be added to a class or interface to precisely specify the
conditions that are required for its correct operation. The basic constructs
of Design by Contract are method preconditions and postconditions, and class
invariants. \par

This paper presents a detailed design and implementation overview of
jContractor, a freely available tool that allows programmers to write
``contracts'' as standard Java methods following an intuitive naming
convention. Preconditions, postconditions, and invariants can be associated
with, or inherited by, any class or interface.  jContractor performs
on-the-fly bytecode instrumentation to detect violation of the contract
specification during a program's execution. jContractor's bytecode engineering
technique allows it to specify and check contracts even when source code is
not available. jContractor is a pure Java library providing a rich set of
syntactic constructs for expressing contracts without extending the Java
language or runtime environment. These constructs include support for
predicate logic expressions, and referencing entry values of attributes and
return values of methods. Fine grain control over the level of monitoring is
possible at runtime. Since contract methods are allowed to use unconstrained
Java expressions, in addition to runtime verification they can perform
additional runtime monitoring, logging, and testing.
 ",
 note    = "",
}

------------------------------------------------------------------------------

516

@InProceedings{kim2002a,
 key          = "kim2002a",
 author       = "Moonjoo Kim and Sampath Kannan and Insup Lee and
                 Oleg Sokolsky and Mahesh Viswanathan",
 title        = "Computational Analysis of Run-time Monitoring --
                 Fundamentals of {Java-MaC}",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
A run-time monitor shares computational resources, such as memory and CPU
time, with the target program. Furthermore, heavy computation performed by a
monitor for checking target program's execution with respect to requirement
properties can be a bottleneck to the target program's execution. Therefore,
computational characteristics of run-time monitoring cause a significant
impact on the target program's execution. \par

We investigate computational issues on run-time monitoring. The first issue is
the power of run-time monitoring. In other words, we study the class of
properties run-time monitoring can evaluate. The second issue is computational
complexity of evaluating properties written in process algebraic language.
Third, we discuss sound abstraction of the target program's execution, which
does not change the result of property evaluation. This abstraction can be
used as a technique to reduce monitoring overhead. \par

Theoretical understanding obtained from these issues affects the
implementation of Java-MaC, a toolset for the run-time monitoring and checking
of Java programs. Finally, we demonstrate the abstraction-based overhead
reduction technique implemented in Java-MaC through a case study.
 ",
 note    = "",
}

------------------------------------------------------------------------------

525

@InProceedings{kim2002b,
 key          = "2002b",
 author       = "Moonjoo Kim and Insup Lee and Usa Sammapun and Jangwoo Shin
                 and Oleg Sokolsky",
 title        = "Monitoring, Checking, and Steering of Real-Time Systems",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
The MaC system has been developed to provide assurance that a target program
is running correctly with respect to formal requirements specification. This
is achieved by monitoring and checking the execution of the target program at
run-time. MaC bridges the gap between formal verification, which ensures the
correctness of a design rather than an implementation, and testing, which only
partially validates an implementation. One weakness of the MaC system is that
it can detect property violations but cannot provide any feedback to the
running system. To remedy this weakness, the MaC system has been extended with
a feedback capability. The resulting system is called MaCS (Monitoring and
Checking with Steering). The feedback component uses the information collected
during monitoring and checking to steer the application back to a safe state
after an error occurs. We present a case study where MaCS is used in a control
system that keeps an inverted pendulum upright. MaCS detects faults in
controllers and performs dynamic reconfiguration of the control system using
steering.
 ",
 note    = "",
}

------------------------------------------------------------------------------

511

@InProceedings{levy2002a,
 key          = "levy2002a",
 author       = "Joshua Levy and Hassen Saidi and Tomas E. Uribe",
 title        = "Combining Monitors for Runtime System Verification",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Runtime verification permits checking system properties that cannot be fully
verified off-line. This is particularly true when the system includes complex
third-party components, such as general-purpose operating systems and software
libraries, and when the properties of interest include security and
performance. The challenge is to find reliable ways to monitor these
properties in realistic systems. In particular, it is important to have
assurance that violations will be reported when they actually occur. For
instance, a monitor may not detect a security violation if the violation
results from a series of system events that are not in its model. We describe
how combining runtime monitors for diverse features such as memory management,
security-related events, performance data, and higher-level temporal
properties can result in more effective runtime verification. After discussing
some basic notions for combining and relating monitors, we illustrate their
application in an intrusion-tolerant Web server architecture under development
at SRI.
 ",
 note    = "",
}

------------------------------------------------------------------------------

510

@InProceedings{gunter2002a,
 key          = "gunter2002a",
 author       = "Elsa Gunter and Doron Peled",
 title        = "Tracing the executions of concurrent programs",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Checking the reliability of software is an ever growing challenge. Fully
automatic tools that attempt to cover the entire state space often fail
because of state explosion. We present instead a toolset that employs some
less-ambitious but useful methods to assist in software debugging. The toolset
provides an automatic translation of the code into visual flowcharts, allowing
the user to interactively select execution paths. It assists the user by
calculating path conditions and exploring the neighborhood of the paths. It
also allows the user to interactively step through the execution of the
program, directed by temporal formulas interpreted over finite sequences. We
will show several different ways of using these capabilities for debugging
sequential and concurrent programs.
 ",
 note    = "",
}

------------------------------------------------------------------------------

509

@InProceedings{stoller2002a,
 key          = "stoller2002a",
 author       = "Scott D. Stoller",
 title        = "Testing Concurrent {Java} Programs using Randomized Scheduling",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
The difficulty of finding errors caused by unexpected interleavings of threads
in concurrent programs is well known. Model checkers can pinpoint such errors
and verify correctness but are not easily scalable to large programs. The
approach discussed here is more scalable but less systematic. We transform a
given Java program by inserting calls to a scheduling function at selected
points. The scheduling function either does nothing or causes a context
switch. The simplest scheduling function makes the choice blindly using a
pseudo-random number generator; more sophisticated scheduling functions use
heuristics to weight the choices. We try to insert as few calls as possible
while still ensuring that for each reachable deadlock and assertion violation,
there is a sequence of choices by the scheduling function that leads to it;
thus, there is a non-zero probability of finding it by testing the transformed
program, regardless of the scheduling policies of the underlying Java Virtual
Machine.
 ",
 note    = "",
}

------------------------------------------------------------------------------

503

@InProceedings{yong2002a,
 key          = "yong2002a",
 author       = "Suan Hsi Yong and Susan Horwitz",
 title        = "Reducing the Overhead of Dynamic Analysis",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Dynamic analysis (instrumenting programs with code to detect and prevent
errors during program execution) can be an effective approach to debugging, as
well as an effective means to prevent harm being caused by malicious code. One
problem with this approach is the runtime overhead introduced by the
instrumentation. We define several techniques that involve using the results
of static analysis to identify some cases where instrumentation can safely be
removed. While we have designed the techniques with a specific dynamic
analysis in mind (that used by the Runtime Type-Checking tool), the ideas may
be of more general applicability.
 ",
 note    = "",
}

------------------------------------------------------------------------------

508

@InProceedings{zuck2002a,
 key          = "zuck2002a",
 author       = "Lenore Zuck and Amir Pnueli and Yi Fang and Benjamin
                 Goldberg and Ying Hu",
 title        = "Translation and Run-Time Validation of Optimized Code",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
The paper presents approaches to the validation of optimizing compilers. The
emphasis is on aggressive and architecture-targeted optimizations which try to
obtain the highest performance from modern architectures, in particular
EPIC-like micro-processors. Rather than verify the compiler, the approach of
translation validation performs a validation check after every run of the
compiler, producing a formal proof that the produced target code is a correct
implementation of the source code.  \par

First we survey the standard approach to validation of optimizations which
preserve the loop structure of the code (though they may move code in and out
of loops and radically modify individual statements), present a
simulation-based general technique for validating such optimizations, and
describe a tool, VOC-64, which implements these technique. For more aggressive
optimizations which, typically, alter the loop structure of the code, such as
loop distribution and fusion, loop tiling, and loop interchanges, we present a
set of permutation rules which establish that the transformed code satisfies
all the implied data dependencies necessary for the validity of the considered
transformation. We describe the necessary extensions to the VOC-64 in order to
validate these structure-modifying optimizations.  \par

Finally, the paper discusses preliminary work on run-time validation of
speculative loop optimizations, that involves using run-time tests to ensure
the correctness of loop optimizations which neither the compiler nor
compiler-validation techniques can guarantee the correctness of. Unlike
compiler validation, run-time validation has not only the task of determining
when an optimization has generated incorrect code, but also has the task of
recovering from the optimization without aborting the program or producing an
incorrect result. This technique has been applied to several loop
optimizations, including loop interchange, loop tiling, and software
pipelining and appears to be quite promising.
 ",
 note    = "",
}

------------------------------------------------------------------------------

@Book{meyer97a,
 key       = "meyer97a",
 author    = "Bertrand Meyer",
 title     = "Object-Oriented Software Construction",
 publisher = "Prentice Hall",
 edition   = "2nd",
 month     = "",
 year      = "1997",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

496

@InProceedings{ergun1998a,
 key          = "ergun1998a",
 author       = "Funda {Erg\"un} and Sampath Kannan and S. Ravi Kumar
                 and Ronitt Rubinfeld and Mahesh Viswanathan",
 title        = "Spot-Checkers",
 booktitle    = "Proceedings of the thirtieth annual {ACM} Symposium on
                 Theory of Computing",
 address      = "Dallas, Texas",
 month        = "23--26" # may,
 year         = "1998",
 pages        = "259--68",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM Press",
 abstract     = "
On Labor Day weekend, the highway patrol sets up spot-checks at random points
on the freeways with the intention of deterring a large fraction of motorists
from driving incorrectly. We explore a very similar idea in the context of
program checking to ascertain with minimal overhead that a program output is
reasonably correct. Our model of spot-checking requires that the spot-checker
must run asymptotically much faster than the combined length of the input and
output. We then show that the spot-checking model can be applied to problems
in a wide range of areas, including problems regarding graphs, sets, and
algebra. In particular, we present spot-checkers for sorting, convex hull,
element distinctness, set containment, set equality, total orders, and
correctness of group and field operations. All of our spot-checkers are very
simple to state and rely on testing that the input and/or output have certain
simple properties that depend on very few bits. Our results also give property
tests as defined by [RS96, Rub94, GGR98].
 ",
 note    = "URL: {\urlBiBTeX{http://external.nj.nec.com/homepages/ronitt/papers/spot.ps}}",
}

------------------------------------------------------------------------------

504

@InProceedings{mcdonald1998a,
 key          = "mcdonald1998a",
 author       = "Jason McDonald and Paul Strooper",
 title        = "Translating {Object-Z} Specifications to Passive Test Oracles",
 booktitle    = "2nd International Conference on Formal Engineering Methods",
 address      = "Brisbane, Queensland, Australia",
 month        = "9--11" # dec,
 year         = "1998",
 pages        = "165--74",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
A test oracle provides a means for determining whether an implementation
functions according to its specification. A passive test oracle checks the
behaviour of the implementation, but does not attempt to reproduce this
behaviour. This paper describes the translation of formal specifications of
container classes to passive test oracles. Specifically, we use Object-Z for
specifications and C++ for oracles. We discuss several practical issues for
the use of formal specifications in test oracle generation. We then present
the translation process and illustrate it with an example based on an integer
set class. We then present the translation process and evaluate how the
derived oracles satisfy our goals of adequacy for testing, portability
between different testing methods, and potential for automated generation.
Our approach is illustrated with an example based on an integer set class.
 ",
 note    = "URL: {\urlBiBTeX{http://www.computer.org/proceedings/icfem/9198/91980165abs.htm}}",
}

------------------------------------------------------------------------------

505

@InProceedings{crane2003a,
 key          = "crane2003a",
 author       = "Michelle L. Crane and Juergen Dingel",
 title        = "Runtime Conformance Checking of Objects Using {Alloy}",
 booktitle    = "Runtime Verification",
 address      = "Boulder, Colorado",
 month        = "13" # jul,
 year         = "2003",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "89",
 issue        = "2",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
Object models are an important part of most object-oriented software
development methodologies, where they play a central role during the
specification and design phases. However, their usefulness is much more
limited during the implementation phase. In this paper, we demonstrate how
confidence in source code can be increased by using runtime conformance
checking to analyze the code with respect to an object model. More precisely,
we use the Alloy Analyzer, developed at MIT, to determine automatically
whether the runtime state of a program at certain user-specified locations
conforms to a given object model. The design, implementation and evaluation of
a prototype runtime conformance checker for Java programs with respect to
Alloy object models is described. 
 ",
 note    = "URL: {\urlBiBTeX{http://www.cs.queensu.ca/home/stl/papers/CraneDingel-RV03.ps}}",
}

------------------------------------------------------------------------------

506

@InProceedings{voas94a,
 key          = "voas94a",
 author       = "Jeffrey M. Voas and Keith W. Miller",
 title        = "Putting Assertions in Their Place",
 booktitle    = "Proceedings of the International Symposium on Software
                 Reliability Engineering",
 address      = "Monterey, CA",
 month        = "6--9 " # nov,
 year         = "1994",
 pages        = "",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Assertions that are placed at each statement in a program can automatically
monitor the internal computations of a program execution. However, the
advantages of universal assertions come at a cost. A program with such
extensive internal intstrumentation will be slower than the same program
without the instrumentation. Some of the assertions may be redundant. The task
of instrumenting the code with correct assertions at each location is
burdensome, and there is no guarantee that the assertions themselves will be
correct. \par

In this paper we advocate a middle ground between no assertions at all (the
most common practice) and the theoretical ideal of assertions at every
location. Our compromise is to place assertions only at locations where
traditional testing is unlikely to uncover software faults. One type of
testability measurement, sensitivity anlysis, identifies locations where
testing is unlikely to be effective.
",
 note         = "",
 url          = "",
}

------------------------------------------------------------------------------

507

@InProceedings{bhargavan2002a,
 key          = "bhargavan2002a",
 author       = "Karthikeyan Bhargavan and Carl A. Gunter",
 title        = "Requirements for a Practical Network Event Recognition
                 Language",
 booktitle    = "Runtime Verification",
 address      = "",
 month        = "26" # jul,
 year         = "2002",
 pages        = "",
 editor       = "Klaus Havelund and Grigore Ro{\c{s}}u",
 organization = "",
 volume       = "70",
 issue        = "4",
 series       = "Electronic Notes in Theoretical Computer Science",
 publisher    = "Elsevier",
 abstract     = "
We propose a run-time monitoring and checking architecture for network
protocols called Network Event Recognition. Our framework is based on
passively monitoring the packet trace produced by a protocol implementation
and checking it for properties written in a formal specification language,
NERL. In this paper, we describe the design requirements for NERL. We show how
the unique requirements of network protocol monitoring impact design and
implementation options. Finally we outline our prototype implementation of
NERL and discuss two case studies: checking the correctness of network
protocol simulations and privacy issues in packet-mode surveillance.
 ",
 note    = "",
}

------------------------------------------------------------------------------

444

@InProceedings{tang97a,
 key          = "tang97a",
 author       = "Dong Tang and Herbert Hecht",
 title        = "An Approach to Measuring and Assessing Dependability for
                 Critical Software Systems",
 booktitle    = "Proceedings of the International Symposium on Software
                 Reliability Engineering",
 address      = "Albuquerque, New Mexico",
 month        = "2--5 " # nov,
 year         = "1997",
 pages        = "192--202",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract     = "
Traditional software testing methods combined with probabilistic models cannot
measure and assess dependability for software that requires very high
reliability (failure rate < 0.000001/hour) and availability (>0.999999). This
paper proposes a novel approach, drawing on findings and methods that have
been described individually but have never been combined, applied in the late
testing phase or early operational phase, to quantify dependability for a
category of critical software with such high requirements. The concepts that
are integrated are: operational profile, rare conditions, importance sampling,
stress testing, and measurement-based dependability evaluation. In the
approach, importance sampling is applied on the operational profile to guide
the testing of critical operations of the software, thereby accelerating the
occurrence of rare conditions which have been shown to be a leading cause of
failure in critical systems. The failure rates measured in the testing are
then transformed to those that would occur in the normal operation by the
likelihood ratio function of the importance sampling theory, and finally
dependability for the tested software system is evaluated by using
measurement-based dependability modeling techniques. When the acceleration
factor is large (over 100), which is typical for a category of software of
interest, it is possible to quantify a very high reliability or availability
in a reasonable test duration. Some feasible methods to implement the approach
are discussed based on real data.
 ",
 note         = "",
}

------------------------------------------------------------------------------

@Misc{digitalmars2003a,
 key    = "digitalmars2003a",
 author = "Digital Mars",
 title  = "{Digital Mars} {C/C++} Compiler for {Win32}",
 url    = "http://www.digitalmars.com/",
 abstract = "
Digital Mars C and C++ Compilers for Win32, Win16, DOS32 and DOS. Fastest
compile/link times, powerful optimization technology, Design by Contract,
complete library source, HTML browsable documentation, disassembler,
librarian, resource compiler, make, etc., command line and GUI versions,
tutorials, sample code, online updates, Standard Template Library, and much
more!
",
 note   = "URL: {\urlBiBTeX{http://www.digitalmars.com/}}",
}

------------------------------------------------------------------------------

@Book{stroustrup2000a,
 key       = "stroustrup2000a",
 author    = "Bjarne Stroustrup",
 title     = "The C++ Programming Language",
 publisher = "Addison-Wesley",
 edition   = "3rd",
 month     = "",
 year      = "2000",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

@Book{smith1999a,
 key       = "smith1999a",
 author    = "Graeme Smith",
 title     = "The Object Z Specification Language",
 publisher = "Kluwer Academic Publishers",
 edition   = "",
 month     = "",
 year      = "1999",
 volume    = "",
 series    = "",
 address   = "",
 note      = "",
}

------------------------------------------------------------------------------

526

@Incollection{kiczales2001a,
 key       = "kiczales2001a",
 author    = "Gregor Kiczales and Erik Hilsdale and Jim Hugunin and
              Mik Kersten and Jeffrey Palm and William G. Griswold",
 title     = "An Overview of {AspectJ}",
 booktitle = "ECOOP 2001 --- Object-Oriented Programming 15th
              European Conference",
 pages     = "327--353",
 publisher = "Springer-Verlag",
 editor    = "J. Lindskov Knudsen",
 month     = jun,
 year      = "2001",
 volume    = "2072",
 chapter   = "",
 series    = "Lecture Notes in Computer Science",
 address   = "Budapest, Hungary",
 edition   = "",
 abstract = "
AspectJ is a simple and practical aspect-oriented extension to Java. With just
a few new constructs, AspectJ provides support for modular implementation of a
range of crosscutting concerns. In AspectJ's dynamic join point model, join
points are well-defined points in the execution of the program; pointcuts are
collections of join points; advice are special method-like constructs that can
be attached to pointcuts; and aspects are modular units of crosscutting
implementation, comprising pointcuts, advice, and ordinary Java member
declarations. AspectJ code is compiled into standard Java bytecode.  Simple
extensions to existing Java development environments make it possible to
browse the crosscutting structure of aspects in the same kind of way as one
browses the inheritance structure of classes. Several examples show that
AspectJ is powerful, and that programs written using it are easy to
understand.
",
 note      = "",
}

------------------------------------------------------------------------------

@Article{hjboehm88a,
 key     = "hjboehm88a",
 author  = "Hans-J. Boehm and Mark Weiser",
 title   = "Garbage Collection in an Uncooperative Environment",
 journal = j-spe,
 month   = "",
 year    = "2000",
 volume  = "18",
 number  = "9",
 pages   = "807--20",
 abstract = "
We describe a technique for storage allocation and garbage collection in the
absence of significant co-operation from the code using the allocator. This
limits garbage collection overhead to the time actually required for garbage
collection. In particular, application programs that rarely or never make use
of the collector no longer encounter a substantial performance penalty. This
approach greatly simplifies the implementation of languages supporting garbage
collection. It further allows conventional compilers to be used with a garbage
collector, either as the primary means of storage reclamation, or as a
debugging tool.
",
 note    = "",
}

------------------------------------------------------------------------------

@InProceedings{jackson2000a,
 key          = "jackson2000a",
 author       = "Daniel Jackson and Kevin Sullivan",
 title        = "{COM} Revisited: Tool Assisted Modelling and Analysis of
                 Software Structures",
 booktitle    = "Proceedings of the Eighth ACM SIGSOFT Symposium on the
                 Foundations of Software Engineering",
 month        = "6--10 " # nov,
 year         = "2000",
 pages        = "149--58",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "San Diego, CA",
 abstract     = "
Designing architectural frameworks without the aid of formal modeling is error
prone. But, unless supported by analysis, formal modeling is prone to its own
class of errors, in which formal statements fail to match the designer.s
intent. A fully automatic analysis tool can rapidly expose such errors, and
can make the process of constructing and refining a formal model more
effective. This paper describes a case study in which we recast a model of
Microsoft COM.s query interface and aggregation mechanism into Alloy, a
lightweight notation for describing structures. We used Alloy.s analyzer to
simulate the specification, to check properties and to evaluate changes. This
allowed us to manipulate our model more quickly and with far greater
confidence than would otherwise have been possible, resulting in a much
simpler model and a better understanding of its key properties.
",
 note         = "",
}

------------------------------------------------------------------------------

523

@InProceedings{evans94a,
 key          = "evans94a",
 author       = "David Evans and John Guttag and Jim Horning and
                 Yang Meng Tan",
 title        = "{LCLint}: A Tool for Using Specifications to Check Code",
 booktitle    = "Proceedings of the Second ACM SIGSOFT Symposium on the
                 Foundations of Software Engineering",
 month        = "6--9 " # dec,
 year         = "1994",
 pages        = "87--96",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "New Orleans, USA",
 abstract     = "
This paper describes LCLint, an efficient and flexible tool that accepts as
input programs (written in ANSI C) and various levels of formal specification.
Using this information, LCLint reports inconsistencies between a program and
its specification. We also describe our experience using LCLint to help
understand, document, and re-engineer legacy code.
",
 note         = "",
}

------------------------------------------------------------------------------

508

@Article{rosenblum1995a,
 key     = "rosenblum1995a",
 author  = "David S. Rosenblum",
 title   = "A Practical Approach to Programming With Assertions",
 journal = j-tse,
 month   = jan,
 year    = "1995",
 volume  = "21",
 number  = "1",
 pages   = "19--31",
 abstract = "
Embedded assertions have been recognized as a potentially powerful tool for
automatic runtime detection of software faults during debugging, testing,
maintenance and even production versions of software systems. Yet despite the
richness of the notations and the maturity of the techniques and tools that
have been developed for programming with assertions, assertions are a
development tool that has seen little widespread use in practice. The main
reasons seem to be that (1) previous assertion processing tools did not
integrate easily with existing programming environments, and (2) it is not
well understood what kinds of assertions are most effective at detecting
software faults. This paper describes experience using an assertion processing
tool that was built to address the concerns of ease-of-use and effectiveness.
The tool is called APP , an Annotation PreProcessor for C programs developed
in UNIX-based development environments. APP has been used in the development
of a variety of software systems over the past five years. Based on this
experience, the paper presents a classification of the assertions that were
most effective at detecting faults. While the assertions that are described
guard against many common kinds of faults and errors, the very commonness of
such faults demonstrates the need for an explicit, high-level, automatically
checkable specification of required behavior. It is hoped that the
classification presented in this paper will prove to be a useful first step in
developing a method of programming with assertions. Index Terms.Anna, APP,
assertions, C, consistency checking, formal specifications, formal methods,
programming environments, runtime checking, software faults.
",
}

-----------------------------------------------------------------------------

509

@Article{ammann94a,
 key     = "ammann94a",
 author  = "Paul E. Ammann and Susan S. Brilliant and John C. Knight",
 title   = "The Effect of Imperfect Error Detection on Reliability Assessment
            via Life Testing",
 journal = j-tse,
 month   = feb,
 year    = "1994",
 volume  = "20",
 number  = "2",
 pages   = "142--8",
 abstract = "
Measurement of software reliability by life testing involves executing the
software on large numbers of test cases and recording the results. The number
of failures observed is used to bound the failure probability even if the
number of failures observed is zero. Typical analyses assume that all failures
that occur are observed, but, in practice, failures occur without being
observed. In this paper, we examine the effect of imperfect error detection,
i.e. the situation in which a failure of the software may not be observed. If
a conventional analysis associated with life testing is used, the confidence
in the bound on the failure probability is optimistic. Our results show that
imperfect error detection does not necessarily limit the ability of life
testing to bound the probability of failure to the very low values required in
critical systems. However, we show that the confidence level associated with a
bound on failure probability cannot necessarily be made as high as desired,
unless very strong assumptions are made about the error detection mechanism.
Such assumptions are unlikely to be met in practice, and so life testing is
likely to be useful only for situations in which very high confidence levels
are not required. 
",
}

------------------------------------------------------------------------------

521

@InProceedings{jackson2000b,
 key          = "jackson2000b",
 author       = "Daniel Jackson",
 title        = "Automating First-Order Relational Logic",
 booktitle    = "Proceedings of the Eighth ACM SIGSOFT Symposium on the
                 Foundations of Software Engineering",
 month        = "6--10 " # nov,
 year         = "2000",
 pages        = "130--9",
 editor       = "David S. Rosenblum",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "San Diego, CA",
 abstract     = "
An automatic analysis method for first-order logic with sets and relations is
described. A first-order formula is translated to a quantifier-free boolean
formula, which has a model when the original formula has a model within a
given scope (that is, involving no more than some finite number of atoms).
Because the satisfiable formulas that occur in practice tend to have small
models, a small scope usually suffices and the analysis is efficient. The
paper presents a simple logic and gives a compositional translation scheme. It
also reports briefly on experience using the Alloy Analyzer, a tool that
implements the scheme. 
",
 note         = "",
}

------------------------------------------------------------------------------

520

@TechReport{detlefs98a,
 key         = "detlefs98a",
 author      = "David L. Detlefs and K. Rustan M. Leino and Greg
                Nelson and James B. Saxe",
 title       = "Extended Static Checking",
 institution = "Compaq Systems Research Center",
 month       = dec,
 year        = "1998",
 number      = "\#159",
 address     = "Palo Alto, CA",
 type        = "",
 abstract    = "
The paper describes a mechanical checker for software that catches many common
programming errors, in particular array index bounds errors, nil dereference
errors, and synchronization errors in multi-threaded programs. The checking is
performed at compile-time. The checker uses an automatic theorem-prover to
reason about the semantics of conditional statements, loops, procedure and
method calls, and exceptions. The checker has been implemented for Modula-3.
It has been applied to thousands of lines of code, including mature systems
code as well as fresh untested code, and it has found a number of errors. 
",
 note        = "URL: {\urlBiBTeX{ftp://ftp.digital.com/pub/DEC/SRC/research-reports/SRC-159.ps.gz}}",
 url         = "ftp://ftp.digital.com/pub/DEC/SRC/research-reports/SRC-159.ps.gz",
}

------------------------------------------------------------------------------

522

@InProceedings{evans96a,
 key          = "evans96a",
 author       = "David Evans",
 title        = "Static Detection of Dynamic Memory Errors",
 booktitle    = "Proceedings of the {ACM} {SIGPLAN} Conference on
                 Programming Language Design and Implementation",
 month        = "21--24 " # may,
 year         = "1996",
 pages        = "44--53",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 address      = "San Diego, CA",
 abstract     = "
Many important classes of bugs result from invalid assumptions
about the results of functions and the values of parameters and
global variables. Using traditional methods, these bugs cannot be
detected efficiently at compile-time, since detailed cross-procedural
analyses would be required to determine the relevant assumptions.
In this work, we introduce annotations to make certain assumptions
explicit at interface points. An efficient static checking tool that exploits
these annotations can detect a broad class of errors including
misuses of null pointers, uses of dead storage, memory leaks, and
dangerous aliasing. This technique has been used successfully to
fix memory management problems in a large program.
",
 note         = "",
}

------------------------------------------------------------------------------

524

@Article{ball2001a,
 key     = "ball2001a",
 author  = "Thomas Ball and Sriram K. Rajamani",
 title   = "Automatically Validating Temporal Safety Properties of
            Interfaces",
 journal = "Lecture Notes in Computer Science",
 month   = "",
 year    = "2001",
 volume  = "2057",
 number  = "",
 pages   = "103--22",
 abstract = "
We present a process for validating temporal safety properties of software
that uses a well-defined interface. The process requires only that the user
state the property of interest. It then automatically creates abstractions of
C code using iterative refinement, based on the given property. The process is
realized in the SLAM toolkit, which consists of a model checker, predicate
abstraction tool and predicate discovery tool. We have applied the SLAM
toolkit to a number of Windows NT device drivers to validate critical safety
properties such as correct locking behavior. We have found that the process
converges on a set of predicates powerful enough to validate properties in
just a few iterations.
",
}

------------------------------------------------------------------------------

527

Fischer, "Pragmatic"

------------------------------------------------------------------------------

528

@InProceedings{haddox2002a,
 key          = "haddox2002a",
 author       = "Jennifer M. Haddox and Gregory M. Kapfhammer and
                 Christoph C. Michael",
 title        = "An Approach for Understanding and Testing Third Party
                 Software Components",
 booktitle    = "Annual Reliability and Maintainability Symposium 2002
                 Proceedings",
 address      = "Seattle, WA",
 month        = "28--31 " # jan,
 year         = "2002",
 pages        = "293--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
In this paper we present an approach to mitigating software risk by
understanding and testing third party, or commercial-off-the-shelf (COTS),
software components. Our approach, based on the notion of software wrapping,
gives system integrators an improved understanding of how a COTS component
behaves within a particular system. Our approach to wrapping allows the data
flowing into and out of the component at the public interface level to be
intercepted. Using our wrapping approach, developers can apply testing
techniques such as fault injection, data collection and, assertion checking to
components whose source code is unavailable. We have created a methodology for
using software wrapping in conjunction with data collection, fault injection,
and assertion checking to test the interaction between a component and the
rest of the application. The methodology seeks to identify locations in the
program where the system's interaction with COTS components could be
problematic. Furthermore, we have developed a prototype that implements,our
methodology for Java applications. The goal of this process is to allow the
developers to identify scenarios where the interaction between COTS software
and the system could result in system failure. We believe that the technology
we have developed is an important step towards easing the process of using
COTS components in the building and maintenance of software systems.
",
 note         = "",
}

------------------------------------------------------------------------------

529

@Article{sankar93a,
 key     = "sankar93a",
 author  = "Sriram Sankar and Manas Mandal",
 title   = "Concurrent Runtime Monitoring Of Formally Specified Programs",
 journal = j-comp,
 month   = mar,
 year    = "1993",
 volume  = "26",
 number  = "3",
 pages   = "32--41",
 abstract = "
A methodology for continuously monitoring a program for specification
consistency during program execution is described. Prior development of the
formal specification and program is assumed. The program is annotated with
constructs from a formal specification language, and the formal specification
constructs are transformed into checking code, which is then inserted into the
underlying program. Calls to this checking code are inserted into underlying
program wherever it can potentially become inconsistent with its
specification. If an inconsistency does in fact occur, diagnostic information
is provided. The implementation of such a system for Anna (annotated Ada)
subtype annotations is presented.
",
 note    = "",
}

------------------------------------------------------------------------------

530

@InProceedings{kim99a,
 key          = "kim99a",
 author       = "Moonjoo Kim and Mahesh Viswanathan and 
                 {Han\^ene} Ben-Abdallah and Sampath Kannan and
                 Insup Lee and Oleg Sokolsky",
 title        = "Formally Specified Monitoring Of Temporal Properties",
 booktitle    = "Proceedings of 11th Euromicro Conference on Real-Time Systems",
 address      = "York, UK",
 month        = "9--11 " # jun,
 year         = "1999",
 pages        = "114--22",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "",
 abstract = "
We describe the Monitoring and Checking (MaC) framework which provides
assurance on the correctness of an execution of a real-time system at runtime.
Monitoring is performed based on a formal specification of system
requirements. MaC bridges the gap between formal specification, which analyzes
designs rather than implementations, and testing, which validates
implementations but lacks formality. An important aspect of the framework is a
clear separation between implementation-dependent description of monitored
objects and high-level requirements specification. Another salient feature is
automatic instrumentation of executable code. The paper presents an overview
of the framework, languages to express monitoring scripts and requirements,
and a prototype implementation of MaC targeted at systems implemented in Java.
",
 note         = "",
}

------------------------------------------------------------------------------

531

@Incollection{muller2000a,
 key       = "muller2000a",
 author    = "Peter {M\"uller} and Arnd Poetzsch-Heffter",
 title     = "Modular Specification and Verification Techniques for
              Object-Oriented Software Components",
 booktitle = "Foundations of Component-Based Systems",
 pages     = "",
 publisher = "Cambridge University Press",
 editor    = "G. T. Leavens and M. Sitaraman",
 month     = "",
 year      = "2000",
 volume    = "",
 chapter   = "",
 series    = "",
 address   = "",
 edition   = "",
 abstract = "
Component-based software development requires that the specification and
verification techniques can handle modularity and adaptability. Modularity
means that specifications need to support abstraction from encapsulated
implementation aspects, that they remain valid under composition, and that
they are sufficiently expressive for verifying properties of composed programs
from the specifications of their components. Adaptability means that the
programming and specification framework support techniques to adapt existing
components to the needs and interfaces of other components.  \par

In this article, we develop a formal modular specification and verification
framework for OO-components. OO-programming provides a good basis for
component technology. It supports adaption by subtyping, inheritance, and
dynamic method binding. Classes form an appropriate basic unit for
encapsulation and modularity on the level of types. To demonstrate our
techniques, we use a Java subset as programming language and assume that a
component is described by a package.  Syntactically, composition corresponds
to the import relation between packages. 
",
 note      = "",
}

------------------------------------------------------------------------------

532

@InProceedings{richardson92a,
 key          = "richardson92a",
 author       = "Debra J. Richardson and Stephanie Leif Aha and
                 T. Owen O'Malley",
 title        = "Specification-Based Test Oracles For Reactive Systems",
 booktitle    = "Proceedings of the 14th International Conference on Software
                 Engineering",
 address      = "Melbourne, Vic., Australia",
 month        = "11--15 " # may,
 year         = "1992",
 pages        = "105--18",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM New York, NY",
 abstract = "
The testing process is typically systematic in test data selection and test
execution. For the most part, however, the effective use of test oracles has
been neglected, even though they are a critical component of the testing
process. Test oracles prescribe acceptable behavior for test execution. In the
absence of judging test results with formal oracles, testing does not achieve
its goal of revealing failures or assuring correct behavior in a practical
manner; manual result checking is neither reliable nor cost-effective. The
authors argue that test oracles should be derived from specifications in
conjunction with testing criteria, represented in a common form, and their use
made integral to the testing process. For complex, reactive systems, oracles
must reflect the multiparadigm nature of the required behavior. Such systems
are often specified using multiple languages, each selected for its utility
specifying in a particular computational paradigm. Thus, the authors are
developing an approach for deriving and using oracles based on multiparadigm
and multilingual specifications to enable the verification of test results for
reactive systems as well as less complex systems.
",
 note         = "",
}

------------------------------------------------------------------------------

533

@InProceedings{edwards98a,
 key       = "edwards98a",
 author    = "Stephen H. Edwards and Gulam Shakir and Murali Sitaraman and
              Bruce W. Weide and Joseph Hollingsworth",
 title     = "A Framework For Detecting Interface Violations In
              Component-Based Software",
 booktitle = "Proceedings Fourth International Conference on Software
              Reuse",
 address   = "Los Alamitos, CA",
 month     = "2--5 " # jun,
 year      = "1998",
 pages     = "46--55",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "IEEE",
 abstract = "
Two kinds of interface contract violations can occur in component based
software: a client component may fail to satisfy a requirement of a component
it is using, or a component implementation may fail to fulfil its obligations
to the client. The paper proposes a systematic approach for detecting both
kinds of violations, so that violation detection is not hard coded into base
level components, but is ``layered'' on top of them, and so that it can be
turned ``on'' or ``off'' selectively for one or more components, with
practically no change to executable code (limiting changes to a few
declarations). Among the salient features of this approach are its use of
formal specifications, the ability to handle parameterized (i.e., generic, or
template) components, and the automatic generation of routine aspects of
violation detection. We have designed, built, and experimented with a
generator of checking components for C++ templates. (19 References).
",
 note      = "",
}

------------------------------------------------------------------------------

534

@InProceedings{baudry2001a,
 key       = "baudry2001a",
 author    = "Benoit Baudry and Yves Le Traon and Jean-Marc {J\'ez\'equel}",
 title     = "Robustness And Diagnosability Of OO Systems Designed By
              Contracts",
 booktitle = "Proceedings Seventh International Software Metrics Symposium",
 address   = "Los Alamitos, CA",
 month     = "4--6 " # apr,
 year      = "2001",
 pages     = "272--84",
 editor    = "",
 volume    = "",
 series    = "",
 publisher = "IEEE",
 abstract = "
While there is a growing interest in component-based systems for industrial
applications, little effort has so far been devoted to quality evaluation of
these systems. This paper presents the definition of measures for two quality
factors, namely robustness and ``diagnosability'' for the special case of
object-oriented (OO) systems, for which thee approach known as
design-by-contract has been used. The main steps in constructing these
measures are given, from informal definitions of the factors to be measured to
the mathematical model of the measures. To fix the parameters, experimental
studies have been conducted, essentially based on applying mutation analysis
in the OO context. Several measures are presented that reveal and estimate the
contribution of the contracts' quality and density to the overall quality of a
system in terms of robustness and diagnosability. (22 References)
",
 note      = "",
}

------------------------------------------------------------------------------

535

@InProceedings{andrews81a,
 key          = "andrews81a",
 author       = "Dorothy M. Andrews and Jeoffrey P. Benson",
 title        = "An Automated Program Testing Methodology And Its
                 Implementation",
 booktitle    = "Proceedings of the 5th International Conference on Software
                 Engineering",
 address      = "New York, NY",
 month        = "9--12 " # mar,
 year         = "1981",
 pages        = "254--61",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Describes an automated testing methodology and an experiment performed to
determine its effectiveness. The method is to insert in the program to be
tested a number of 'executable assertions', statements about the program that
trigger error signals whenever they are evaluated to be false (violated). A
test-case is then developed for the program using actual values of the input
variables. When the program is run, a plot is generated of the number of
assertions violated versus the input variable values used. The resulting
function is called the 'error function'. Heuristic search algorithms can then
be used to maximize this function and thereby automatically locate input
values which cause the most errors to occur. The experiment included
developing assertions for the program to be tested, choosing and inserting
representative errors into the program, and implementing search and data
collection algorithms for testing. The results indicate that combining
executable assertions with heuristic search algorithms is an effective method
for automating the testing of computer programs. (19 References).
",
 note         = "",
}

------------------------------------------------------------------------------

536

@Article{liao92a,
 key     = "liao92a",
 author  = "Yingsha Liao and Donald Cohen",
 title   = "A Specificational Approach To High Level Program Monitoring And
            Measuring",
 journal = j-tse,
 month   = nov,
 year    = "1992",
 volume  = "18",
 number  = "11",
 pages   = "969--78",
 abstract = "
Program monitoring and measuring is the activity of collecting information
about the execution characteristics of a program. Although this activity is
occasionally supported by special-purpose hardware, it is normally done by
adding instrumentation code to the program so that it collects interesting
data as it runs. Unfortunately, this alteration is itself a difficult task
involving all the complexities of programming. Given some questions to be
answered, the programmer must determine what data must be collected, determine
where in the program those data can be collected, and add code to the program
to collect that data and to process it to produce the desired results. The
goal of the work described is to automate the process. A high-level program
monitoring and measuring system is presented. The system provides a high-level
specification language to let programmers specify what they want to know about
their program's execution. It automatically generates an augmented program
whose execution produces both the results of the original program and answers
to the specified questions. (21 References).
",
}

------------------------------------------------------------------------------

537

@InProceedings{korel96a,
 key          = "korel96a",
 author       = "Bogdan Korel and Ali M. Al-Yami",
 title        = "Assertion-Oriented Automated Test Data Generation",
 booktitle    = "Proceedings of the 18th International Conference on Software
                 Engineering",
 address      = "Berlin, Germany ",
 month        = "25--30  " # mar,
 year         = "1996",
 pages        = "71--80",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
Assertions are recognized as a powerful tool for automatic run time detection
of software errors. However, existing testing methods do not use assertions to
generate test cases. We present a novel approach of automated test data
generation in which assertions are used to generate test cases. In this
approach the goal is to identify test cases on which an assertion is violated.
If such a test is found then this test uncovers an error in the program. The
problem of finding program input on which an assertion is violated may be
reduced to the problem of finding program input on which a selected statement
is executed. As a result, the existing methods of automated test data
generation for white box testing may be used to generate tests to violate
assertions. The experiments have shown that this approach may significantly
improve the chances of finding software errors as compared to the existing
methods of test generation. (23 References).
",
 note         = "",
}

------------------------------------------------------------------------------

538

@InProceedings{sosic92a,
 key          = "sosic92a",
 author       = "Rok {Sosi\vc}",
 title        = "Dynascope: A Tool For Program Directing.",
 booktitle    = "ACM SIGPLAN '92 Conference on Programming Language Design and
                 Implementation (PDLI)",
 address      = "San Francisco, CA, USA",
 month        = "17--19 " # jun,
 year         = "1992",
 pages        = "12--21",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "ACM SIGPLAN",
 abstract = "
The paper introduces program directing, a new way of program interaction.
Directing enables one program, the director, to monitor and to control another
program, the executor. One important application of program directing is human
interaction with complex computer simulations. The Dynascope programming
environment is designed specifically to support directing in traditional,
compiled languages. It provides a framework and building blocks for easy
construction of sophisticated directors. Directors are regular programs that
perform the directing of executors through Dynascope primitives. Dynascope is
built around the concept of the execution stream which provides a complete
description of the executor's computational behavior. The source code of
executors requires no changes in order to be subjected to directing. The paper
gives an overview of the Dynascope system. Sample applications are presented:
debugging register allocation, animation of procedure calls, and a complex
artificial life simulation. The performance of Dynascope is illustrated by
real time measurements. (18 References).
",
 note         = "",
}

------------------------------------------------------------------------------

539

@Article{morosvalle2000a,
 key     = "morosvalle2000a",
 author  = "{Bego\~na} Moros Valle and {Joaqu\'{\i}n} {Nicol\'as} Ros and
            {Jes\'us} {Garc\'{\i}a} Molina and {Jos\'e} Toval {\'Alvarez}",
 title   = "Combining Formal Specifications With Design By Contract",
 journal = "Journal of Object-Oriented Programming",
 month   = feb,
 year    = "2000",
 volume  = "12",
 number  = "9",
 pages   = "16--21",
 abstract = "
Presents an approach to object-oriented (OO) software development based on:
(a) automatic generation of a throwaway prototype from the initial
specification in a formal, declarative OO specification language, (b)
validation of user requirements and refinement of the specification using this
prototype, and (c) automatic translation from the validated specification
types to programming classes that include the semantics of the formal
specification by means of assertions. The last step is achieved by using an OO
implementation language supporting Eiffel-like assertions and the
design-by-contract technique; therefore, these classes force the first
evolutionary prototype (that will evolve into the final software) to be
formally consistent with the validated specification. This approach is
supported by a high-level computer-aided requirements engineering (CARE) tool.
(22 References).
",
 note    = "",
}

------------------------------------------------------------------------------

540

@InProceedings{lederer98a,
 key          = "lederer98a",
 author       = "Edgar F. A. Lederer and Romeo A. Dumitrescu",
 title        = "Specification-Consistent Coordination Model for Computations",
 booktitle    = "Proceedings ACM/SIGAPP Symposium on Applied Computing",
 address      = "",
 month        = "27--28 " # feb,
 year         = "1998",
 pages        = "122--9",
 editor       = "",
 organization = "",
 volume       = "",
 series       = "",
 publisher    = "IEEE",
 abstract = "
A new coordination model for computations is presented. It offers increased
confidence in the correctness of imperative programs and considerable
simplification of imperative programming and debugging. In this model,
programs consist of formal specifications of computations by recursive
function definitions and explicit mappings (coordinations) of these
specifications to imperative programs. The consistency between coordination
and specification is guaranteed by a special mechanism, called the consistency
checker, during the program's execution. It automatically detects any
inconsistency by comparisons against symbolic names associated with values.
",
 note         = "",
}

------------------------------------------------------------------------------

541

@Article{voas99a,
 key     = "voas99a",
 author  = "Jeffrey Voas and Lora Kassab",
 title   = "Using Assertions to Make Untestable Software More Testable",
 journal = "Software Quality Professional",
 month   = sep,
 year    = "1999",
 volume  = "1",
 number  = "4",
 pages   = "",
 abstract = "
Testing software that has been augmented with assertions increases defect
observability provided that the assertions are reached during testing. This
article presents an approach to assertion localization that is based on
finding regions of the code that appear to be untestable and making them more
testable. This article also explores the phenomenon where assertions designed
to boost the fault observability for a given test scheme cannot lower the
fault observability afforded by a different testing scheme, and in fact, may
actually increase it. If true, this demonstrates a unique and cost-effective
benefit of assertions not before exploited and lays forth a new avenue for
finding higher return-on-investment testing techniques.
",
 note    = "",
}

------------------------------------------------------------------------------

------------------------------------------------------------------------------

------------------------------------------------------------------------------

------------------------------------------------------------------------------

------------------------------------------------------------------------------

------------------------------------------------------------------------------

------------------------------------------------------------------------------

